{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-14 15:16:21.555269: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-14 15:16:22.410441: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-07-14 15:16:24.052333: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-14 15:16:24.052782: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-14 15:16:24.052801: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from sklearn.utils import shuffle\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Vocabulary\n",
    "\n",
    "First, we build the vocabulary dictionaries for the source and target. \n",
    "The vocabulary is the the file `vocab.txt` (generated in the other script)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary: [('<unk>', 0), ('<s>', 1), ('</s>', 2), ('.', 3), ('the', 4), (',', 5), ('a', 6), ('?', 7), ('to', 8), ('you', 9)]\n",
      "Reverse dictionary: [(0, '<unk>'), (1, '<s>'), (2, '</s>'), (3, '.'), (4, 'the'), (5, ','), (6, 'a'), (7, '?'), (8, 'to'), (9, 'you')]\n",
      "Vocabulary size:  30000\n"
     ]
    }
   ],
   "source": [
    "# Word string -> ID mapping\n",
    "dictionary = dict()\n",
    "\n",
    "with open('data/vocab.30K.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        # disregard the new line aka `\\n`\n",
    "        dictionary[line[:-1]] = len(dictionary)\n",
    "        \n",
    "reverse_dictionary = dict(zip(dictionary.values(),dictionary.keys()))\n",
    "\n",
    "print('Dictionary:', list(dictionary.items())[:10], end = '\\n')\n",
    "print('Reverse dictionary:', list(reverse_dictionary.items())[:10], end = '\\n')\n",
    "print('Vocabulary size: ', len(dictionary), end = '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset\n",
    "Here we load the data from the `dataset.csv` file (generated in the other script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('data/dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing\n",
    "Transform to lower, remove the new line and the punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowerDataset(data):\n",
    "    return data.str.lower() \n",
    "    \n",
    "def cleanDataset(data):\n",
    "    return data.str.replace(r\"[^a-zA-Z0-9?'.,]+\",' ',regex=True)\n",
    "    \n",
    "def paddDataset(data):\n",
    "    return data.str.replace(',', ' ,')                 \\\n",
    "                .str.replace('.',' . ', regex=False)    \\\n",
    "                .str.replace('?',' ?', regex=False)    \\\n",
    "                .str.replace('\\n',' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt = nltk.tokenize.WhitespaceTokenizer()\n",
    "for column in dataset.columns:    \n",
    "    dataset[column] = lowerDataset(dataset[column]) \n",
    "    dataset[column] = cleanDataset(dataset[column])\n",
    "    dataset[column] = paddDataset(dataset[column])                                    \n",
    "    dataset[column] = dataset[column].apply(wt.tokenize)\n",
    "dataset = shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>232316</th>\n",
       "      <td>what is something that frustrates you , but to...</td>\n",
       "      <td>okay this is my time to shine .  been thinking...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194487</th>\n",
       "      <td>i have cancer and will start chemo next month ...</td>\n",
       "      <td>hi good luck with everything .  i'm a cancer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222270</th>\n",
       "      <td>what first world problem are you currently dea...</td>\n",
       "      <td>reddit is distracting me from my tv show</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446293</th>\n",
       "      <td>my grandfather shot and killed himself yesterd...</td>\n",
       "      <td>first off , i'm terribly sorry for your loss ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890004</th>\n",
       "      <td>reddit people , what made you smile laugh today ?</td>\n",
       "      <td>my girlfriend farted in bed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 question  \\\n",
       "232316  what is something that frustrates you , but to...   \n",
       "194487  i have cancer and will start chemo next month ...   \n",
       "222270  what first world problem are you currently dea...   \n",
       "446293  my grandfather shot and killed himself yesterd...   \n",
       "890004  reddit people , what made you smile laugh today ?   \n",
       "\n",
       "                                                   answer  \n",
       "232316  okay this is my time to shine .  been thinking...  \n",
       "194487   hi good luck with everything .  i'm a cancer ...  \n",
       "222270           reddit is distracting me from my tv show  \n",
       "446293   first off , i'm terribly sorry for your loss ...  \n",
       "890004                        my girlfriend farted in bed  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data analysis\n",
    "Mean sentence length and standard deviation of sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Central tendency, dispersion and shape of questions’s distribution\n",
      "count    1149819.000000\n",
      "mean          17.113906\n",
      "std            9.139078\n",
      "min            1.000000\n",
      "25%           11.000000\n",
      "50%           15.000000\n",
      "75%           21.000000\n",
      "max           82.000000\n",
      "Name: question, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print('Central tendency, dispersion and shape of questions’s distribution')\n",
    "print(dataset['question'].str.len().describe().apply(lambda x: format(x, 'f')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Central tendency, dispersion and shape of answers’s distribution\n",
      "count    1149819.000000\n",
      "mean          54.452828\n",
      "std          844.371854\n",
      "min            0.000000\n",
      "25%           10.000000\n",
      "50%           22.000000\n",
      "75%           53.000000\n",
      "max       563680.000000\n",
      "Name: answer, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print('Central tendency, dispersion and shape of answers’s distribution')\n",
    "print(dataset['answer'].str.len().describe().apply(lambda x: format(x, 'f')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the sentences to fixed length\n",
    "Update all sentences with a fixed size, to process the sentences as batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sent_length = {'question' : 30, 'answer': 60}\n",
    "\n",
    "def padding_sent(source):\n",
    "    padded = []\n",
    "    for tokens in dataset[source]: \n",
    "        # adding the start token\n",
    "        tokens.insert(0, '<s>')  \n",
    "\n",
    "        if len(tokens) >= max_sent_length[source]:\n",
    "            tokens = tokens[:(max_sent_length[source] - 1)]\n",
    "            tokens.append('</s>')\n",
    "\n",
    "        if len(tokens) < max_sent_length[source]:\n",
    "            tokens.extend(['</s>' for _ in range(max_sent_length[source] - len(tokens))])  \n",
    "\n",
    "        padded.append(tokens)\n",
    "    return padded\n",
    "\n",
    "questions = padding_sent('question')\n",
    "answers = padding_sent('answer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the reverse dataset\n",
    "The reverse dataset are going to be used to retrieve the decoder output to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reverse_dataset(source):\n",
    "    reverse_tokens = []\n",
    "    reverse_dataset = []\n",
    "    for tokens in source: \n",
    "        for token in tokens: \n",
    "            if token not in dictionary.keys():\n",
    "                reverse_tokens.append(dictionary['<unk>'])\n",
    "            else:\n",
    "                reverse_tokens.append(dictionary[token])\n",
    "        reverse_dataset.append(reverse_tokens)\n",
    "        reverse_tokens = []\n",
    "    return reverse_dataset\n",
    "\n",
    "inputs_indexes =  np.array(create_reverse_dataset(questions), dtype=np.int32)\n",
    "outputs_indexes =  np.array(create_reverse_dataset(answers), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding\n",
    "Use the Word2Vec to embed the input to a hight demention vector, that will keep the word relationships "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLearns all the Word2Vec relationships fo the questions, answers and unkown words\\nThis code needs to be run just once\\nUncomment the lines to create the Word2Vec model\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Learns all the Word2Vec relationships fo the questions, answers and unkown words\n",
    "This code needs to be run just once\n",
    "Uncomment the lines to create the Word2Vec model\n",
    "\"\"\"\n",
    "\n",
    "#model = Word2Vec(questions + answers + [['<unk>']], vector_size=100, window=5, min_count=1, workers=4)\n",
    "#model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formating the inputs and outputs\n",
    "\n",
    "The dataset original format for inputs is 30x100 and for the outputs is 60x30000 for 1.149 million records, making the memory usage impracticable.\n",
    "\n",
    "The inputs and outputs are goint to be refactor to 15x100 for the inputs and 30x30000 for the outputs to a total of 2.299 million records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "input_window_size = 15\n",
    "output_window_size = 30\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#Factor to reshape the dataset\n",
    "reshape_factor = 2 \n",
    "\n",
    "input_window_size = int(max_sent_length['question'] / reshape_factor)\n",
    "output_window_size = int(max_sent_length['answer'] / reshape_factor)\n",
    "\n",
    "def array_numpy(array):\n",
    "    return np.array(array, dtype=np.float32)\n",
    "\n",
    "def get_batch_inputs(batch, batch_size = 10):\n",
    "    train_inputs = list()\n",
    "    \n",
    "    batch_start = batch * batch_size\n",
    "    batch_end = batch_start + batch_size\n",
    "    \n",
    "    for input_index in inputs_indexes[batch_start:batch_end]:\n",
    "        train_input = list()   \n",
    "        \n",
    "        for index in input_index:\n",
    "            # Formates the input to the word2vec encoded format\n",
    "            train_input.append(model.wv[reverse_dictionary[index]])\n",
    "            \n",
    "        train_inputs.append(array_numpy(train_input[:input_window_size]))\n",
    "        train_inputs.append(array_numpy(train_input[input_window_size:]))\n",
    "    return array_numpy(train_inputs)\n",
    "\n",
    "def get_batch_outputs(batch, batch_size = 10):\n",
    "    train_outputs = list()\n",
    "    train_targets = list()\n",
    "    \n",
    "    batch_start = batch * batch_size\n",
    "    batch_end = batch_start + batch_size\n",
    "    \n",
    "    for output_index in outputs_indexes[batch_start:batch_end]:\n",
    "        train_output = list()\n",
    "        train_target = list()\n",
    "                \n",
    "        for timestep, index in enumerate(output_index):\n",
    "            # Formates the output to the one-hot-encode format\n",
    "            output_encoded = np.zeros(len(dictionary), dtype=np.float32)\n",
    "            output_encoded[index] = 1\n",
    "            train_output.append(output_encoded)\n",
    "            \n",
    "            # Formates the target to the one-hot-encode format\n",
    "            # Setted as index - 1 because it ignores the first <s>\n",
    "            if timestep > 0:\n",
    "                target_encoded = np.zeros(len(dictionary), dtype=np.float32)\n",
    "                target_encoded[output_index[timestep]] = 1.0\n",
    "                train_target.append(target_encoded)\n",
    "        \n",
    "        train_outputs.append(array_numpy(train_output[:output_window_size]))\n",
    "        train_outputs.append(array_numpy(train_output[output_window_size:]))\n",
    "        \n",
    "        #Add a </s> in the end of the target so len(output) == len(taget) \n",
    "        target_encoded = np.zeros(len(dictionary), dtype=np.float32)\n",
    "        target_encoded[output_index[-1]] = 1.0\n",
    "        train_target.append(target_encoded)\n",
    "                \n",
    "        train_targets.append(array_numpy(train_target[:output_window_size]))\n",
    "        train_targets.append(array_numpy(train_target[output_window_size:]))\n",
    "        \n",
    "    return array_numpy(train_outputs), array_numpy(train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_example = get_batch_inputs(0, 1)\n",
    "output_example, target_example = get_batch_outputs(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (2, 15, 100)\n",
      "Output shape: (2, 30, 20000)\n",
      "Target shape: (2, 30, 20000)\n"
     ]
    }
   ],
   "source": [
    "print(\"Input shape:\", input_example.shape)\n",
    "print(\"Output shape:\", output_example.shape)\n",
    "print(\"Target shape:\", target_example.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-12 19:33:03.102530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-12 19:33:03.129503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-12 19:33:03.129796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-12 19:33:03.130326: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-12 19:33:03.130596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-12 19:33:03.130860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-12 19:33:03.131018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-12 19:33:03.427387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-12 19:33:03.427618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-12 19:33:03.427761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-12 19:33:03.427869: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3176 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, None, 100)]  0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, None, 20000  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 256),        365568      ['input_1[0][0]']                \n",
      "                                 (None, 256),                                                     \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, None, 256),  20743168    ['input_2[0][0]',                \n",
      "                                 (None, 256),                     'lstm[0][1]',                   \n",
      "                                 (None, 256)]                     'lstm[0][2]']                   \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, None, 20000)  5140000     ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 26,248,736\n",
      "Trainable params: 26,248,736\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Input shape = 100\n",
    "input_shape = input_example.shape[2]\n",
    "\n",
    "# Output shape = 30000\n",
    "output_shape = output_example.shape[2]\n",
    "\n",
    "#Dimensionality\n",
    "dimensionality = 256\n",
    "\n",
    "#The batch size and number of epochs\n",
    "batch_size = 10\n",
    "epochs = 150\n",
    "\n",
    "#Encoder\n",
    "encoder_inputs = Input(shape=(None, input_shape))\n",
    "encoder_lstm = LSTM(dimensionality, return_state=True)\n",
    "encoder_outputs, state_hidden, state_cell = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [state_hidden, state_cell]\n",
    "\n",
    "#Decoder\n",
    "decoder_inputs = Input(shape=(None, output_shape))\n",
    "decoder_lstm = LSTM(dimensionality, return_sequences=True, return_state=True)\n",
    "decoder_outputs, decoder_state_hidden, decoder_state_cell = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(output_shape, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "#Model\n",
    "training_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "#Compiling\n",
    "training_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'], sample_weight_mode='temporal')\n",
    "\n",
    "training_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training in batches\n",
    "\n",
    "Due to memory usage, the training will be divided into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_records = len(dataset) * reshape_factor\n",
    "data_batch_size = 500\n",
    "supported_batch_records = num_records / data_batch_size\n",
    "batch_records = math.ceil(supported_batch_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using  91986  batches\n",
      "With  25  (inputs, outputs) each\n",
      "With the totall of  2299638  pairs to process\n"
     ]
    }
   ],
   "source": [
    "print(\"Using \", batch_records, \" batches\")\n",
    "print(\"With \", data_batch_size, \" (inputs, outputs) each\")\n",
    "print(\"With the totall of \", num_records, \" pairs to process\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning memory\n",
    "\n",
    "Removing variables that are no longer needed before start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset\n",
    "del questions\n",
    "del answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run for the batches \n",
    "for batch_record in range(batch_records + 1):\n",
    "    \n",
    "    # Validate for the limit size of the dataset\n",
    "    # Limit the last step not to exceed the data size\n",
    "    if batch_record > (num_records / data_batch_size):\n",
    "        input_batch_size = math.floor((supported_batch_records % 1) * data_batch_size)\n",
    "    \n",
    "    # Get the inputs\n",
    "    inputs_data = get_batch_inputs(batch_record, data_batch_size)\n",
    "    \n",
    "    # Get the outputs\n",
    "    outputs_data, target_data = get_batch_outputs(batch_record, data_batch_size)\n",
    "    \n",
    "    #Training\n",
    "    training_model.fit([inputs_data, outputs_data], outputs_data, batch_size = batch_size, epochs = epochs, validation_split = 0.2)\n",
    "    training_model.save('training_model.h5')\n",
    "   \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "chatbot.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
