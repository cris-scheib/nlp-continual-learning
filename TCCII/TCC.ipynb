{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 17:09:08.930718: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-18 17:09:09.727187: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-07-18 17:09:11.145058: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-18 17:09:11.145239: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-18 17:09:11.145249: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from sklearn.utils import shuffle\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset\n",
    "Here we load the data from the `dataset.csv` file (generated in the other script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    return pd.read_csv('data/fraction/dataset-50k-1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing\n",
    "Transform to lower, remove the new line and the punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_data(data):\n",
    "    return data.str.lower() \n",
    "    \n",
    "def clean_data(data):\n",
    "    return data.str.replace(',', ' ,')                \\\n",
    "                .str.replace('.',' . ', regex=False)  \\\n",
    "                .str.replace('?',' ?', regex=False)   \\\n",
    "                .str.replace(r\"[^a-zA-Z0-9?'.,]+\",' ',regex=True)\n",
    "\n",
    "def get_data():\n",
    "    data = load_data()\n",
    "    for column in data.columns:    \n",
    "        data[column] = lower_data(data[column])\n",
    "        data[column] = clean_data(data[column])\n",
    "    return shuffle(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data analysis\n",
    "Mean sentence length and standard deviation of sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_data_analysis(data):\n",
    "    print('Central tendency, dispersion and shape of questions’s distribution')\n",
    "    print(data['question'].str.len().describe().apply(lambda x: format(x, 'f')))\n",
    "    print('-'*100)\n",
    "    print('Central tendency, dispersion and shape of answers’s distribution')\n",
    "    print(data['answer'].str.len().describe().apply(lambda x: format(x, 'f')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print_data_analysis(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(data):\n",
    "    return data[(data['question'].str.len() < 100) & (data['answer'].str.len() < 200)]\n",
    "\n",
    "def padd_data(data):\n",
    "    data = data.assign(question = '<start> ' + data.question  + ' <end>')\n",
    "    data = data.assign(answer = '<start> ' + data.answer  + ' <end>')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "# Convert the unicode sequence to ascii\n",
    "def unicode_to_ascii(s):\n",
    "\n",
    "    # Normalize the unicode string and remove the non-spacking mark\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "# Preprocess the sequence\n",
    "def preprocess_sentence(w):\n",
    "\n",
    "    # Clean the sequence\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "    # Create a space between word and the punctuation following it\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    # Replace everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "    w = w.strip()\n",
    "\n",
    "    # Add a start and stop token to detect the start and end of the sequence\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the dataset\n",
    "Removing the outliers and adding <start> and <end> for each question, awnser pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(num_examples):\n",
    "    dataset = remove_outliers(get_data())\n",
    "    dataset = padd_data(dataset)\n",
    "    return dataset['question'].tolist(), dataset['answer'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing \n",
    "Tokenize the data, padd the sequence and create the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='!\"#$%&()*+-:;=@[\\\\]^_{|}~\\t')\n",
    "  \n",
    "    # Convert sequences into internal vocab\n",
    "    tokenizer.fit_on_texts(text)\n",
    "\n",
    "    # Convert internal vocab to numbers\n",
    "    tensor = tokenizer.texts_to_sequences(text)\n",
    "\n",
    "    # Pad the tensors to assign equal length to all the sequences\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', truncating='post',maxlen=None)\n",
    "\n",
    "    return tensor, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the clean and formated data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(num_examples=None):\n",
    " \n",
    "    questions, answers = create_dataset(num_examples=None)\n",
    "\n",
    "    questions_tensor, questions_tokenizer = tokenize(questions)\n",
    "    answers_tensor, answers_tokenizer = tokenize(answers)\n",
    "\n",
    "    return questions_tensor, answers_tensor, questions_tokenizer, answers_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_tensor, answers_tensor, questions_tokenizer, answers_tokenizer = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split in train and test\n",
    "Split 80% of the data to train and 20% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test count: 4034\n",
      "Train count: 1009\n"
     ]
    }
   ],
   "source": [
    "max_length_input, max_length_target = questions_tensor.shape[1], answers_tensor.shape[1]\n",
    "input_train, input_test, target_train, target_test = train_test_split(questions_tensor, answers_tensor, test_size=0.2)\n",
    "\n",
    "print(\"Test count:\", len(input_train))\n",
    "print(\"Train count:\", len(input_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input question; index to word mapping\n",
      "2 ----> <start>\n",
      "35 ----> who\n",
      "8 ----> is\n",
      "9 ----> a\n",
      "202 ----> fictional\n",
      "137 ----> character\n",
      "5 ----> you\n",
      "519 ----> shouldn't\n",
      "1870 ----> relate\n",
      "11 ----> to\n",
      "1 ----> ?\n",
      "3 ----> <end>\n",
      "\n",
      "Target awnser; index to word mapping\n",
      "2 ----> <start>\n",
      "6108 ----> sheldon\n",
      "6109 ----> cooper\n",
      "1 ----> .\n",
      "7 ----> i\n",
      "6110 ----> behave\n",
      "326 ----> almost\n",
      "658 ----> exactly\n",
      "29 ----> like\n",
      "103 ----> him\n",
      "6 ----> a\n",
      "6111 ----> pedantic\n",
      "294 ----> super\n",
      "2623 ----> nerd\n",
      "1637 ----> who's\n",
      "6 ----> a\n",
      "696 ----> stick\n",
      "12 ----> in\n",
      "4 ----> the\n",
      "2624 ----> mud\n",
      "8 ----> and\n",
      "6 ----> a\n",
      "1972 ----> creature\n",
      "10 ----> of\n",
      "2625 ----> extreme\n",
      "6112 ----> routine\n",
      "1 ----> .\n",
      "13 ----> you\n",
      "161 ----> should\n",
      "26 ----> not\n",
      "770 ----> act\n",
      "29 ----> like\n",
      "103 ----> him\n",
      "1 ----> .\n",
      "3 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "def convert(text, tensor):\n",
    "    for t in tensor:\n",
    "        if t != 0:\n",
    "            print (\"%d ----> %s\" % (t, text.index_word[t]))\n",
    "      \n",
    "print(\"Input question; index to word mapping\")\n",
    "convert(questions_tokenizer, input_train[0])\n",
    "print()\n",
    "print(\"Target awnser; index to word mapping\")\n",
    "convert(answers_tokenizer, target_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_input_size = len(questions_tokenizer.word_index) + 1\n",
    "vocab_target_size = len(answers_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 17:09:13.490155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-18 17:09:13.787548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-18 17:09:13.788275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-18 17:09:13.794964: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-18 17:09:13.797721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-18 17:09:13.798419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-18 17:09:13.799066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-18 17:09:14.937516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-18 17:09:14.938492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-18 17:09:14.938648: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-18 17:09:14.938766: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3131 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((input_train, target_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 31]), TensorShape([64, 49]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, encoder_units, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.encoder_units = encoder_units\n",
    "\n",
    "        # Embed the vocab to a dense embedding \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # LSTM Layer\n",
    "        # glorot_uniform: Initializer for the recurrent_kernel weights matrix, \n",
    "        # used for the linear transformation of the recurrent state\n",
    "        self.lstm = tf.keras.layers.GRU(self.encoder_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    # Encoder network comprises an Embedding layer followed by a LSTM layer\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.lstm(x, initial_state=hidden)\n",
    "        return output, state\n",
    "\n",
    "    # To initialize the hidden state\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.encoder_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 17:09:17.095466: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 31, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_input_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Mechanism\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query hidden state shape == (batch_size, hidden size)\n",
    "        # values shape == (batch_size, max_len, hidden size)\n",
    "\n",
    "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (64, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (64, 31, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder class\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        # Used for attention\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # x shape == (batch_size, 1)\n",
    "        # hidden shape == (batch_size, max_length)\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "\n",
    "        # context_vector shape == (batch_size, hidden_size)\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (64, 10809)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_target_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer and loss functions\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "# Loss function\n",
    "def loss_function(real, pred):\n",
    "\n",
    "    # Take care of the padding. Not all sequences are of equal length.\n",
    "    # If there's a '0' in the sequence, the loss is being nullified\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    # tf.GradientTape() -- record operations for automatic differentiation\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        # dec_hidden is used by attention, hence is the same enc_hidden\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        # <start> token is the initial decoder input\n",
    "        dec_input = tf.expand_dims([answers_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "\n",
    "            # Pass enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # Use teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    # As this function is called per batch, compute the batch_loss\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    # Get the model's variables\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    # Compute the gradients\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    # Update the variables of the model/network\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 2.8019\n",
      "Epoch 1 Loss 2.3926\n",
      "Time taken for 1 epoch 289.49769258499146 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 2.5005\n",
      "Epoch 2 Loss 2.2083\n",
      "Time taken for 1 epoch 246.19020628929138 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 2.3963\n",
      "Epoch 3 Loss 2.1740\n",
      "Time taken for 1 epoch 244.75347352027893 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 2.3321\n",
      "Epoch 4 Loss 2.1433\n",
      "Time taken for 1 epoch 244.72218990325928 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.9705\n",
      "Epoch 5 Loss 2.1088\n",
      "Time taken for 1 epoch 243.50544691085815 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 2.2937\n",
      "Epoch 6 Loss 2.0599\n",
      "Time taken for 1 epoch 245.1483166217804 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 1.9863\n",
      "Epoch 7 Loss 2.0103\n",
      "Time taken for 1 epoch 243.70060467720032 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 1.6632\n",
      "Epoch 8 Loss 1.9596\n",
      "Time taken for 1 epoch 244.8394160270691 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 1.7621\n",
      "Epoch 9 Loss 1.9137\n",
      "Time taken for 1 epoch 243.98470735549927 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 1.8159\n",
      "Epoch 10 Loss 1.8625\n",
      "Time taken for 1 epoch 245.00808954238892 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 1.8362\n",
      "Epoch 11 Loss 1.8062\n",
      "Time taken for 1 epoch 243.77059078216553 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 1.8148\n",
      "Epoch 12 Loss 1.7462\n",
      "Time taken for 1 epoch 244.90069150924683 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 1.6868\n",
      "Epoch 13 Loss 1.6790\n",
      "Time taken for 1 epoch 243.76980233192444 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 1.5149\n",
      "Epoch 14 Loss 1.6079\n",
      "Time taken for 1 epoch 245.10948991775513 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 1.4193\n",
      "Epoch 15 Loss 1.5365\n",
      "Time taken for 1 epoch 243.77540564537048 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 1.3565\n",
      "Epoch 16 Loss 1.4617\n",
      "Time taken for 1 epoch 245.34919142723083 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 1.2673\n",
      "Epoch 17 Loss 1.3811\n",
      "Time taken for 1 epoch 244.576238155365 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 1.2186\n",
      "Epoch 18 Loss 1.2970\n",
      "Time taken for 1 epoch 245.79938340187073 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 1.1456\n",
      "Epoch 19 Loss 1.2090\n",
      "Time taken for 1 epoch 244.93802857398987 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 1.0628\n",
      "Epoch 20 Loss 1.1215\n",
      "Time taken for 1 epoch 245.979962348938 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 1.0723\n",
      "Epoch 21 Loss 1.0367\n",
      "Time taken for 1 epoch 245.33766531944275 sec\n",
      "\n",
      "Epoch 22 Batch 0 Loss 0.8015\n",
      "Epoch 22 Loss 0.9565\n",
      "Time taken for 1 epoch 246.68826866149902 sec\n",
      "\n",
      "Epoch 23 Batch 0 Loss 0.9688\n",
      "Epoch 23 Loss 0.8790\n",
      "Time taken for 1 epoch 244.74008083343506 sec\n",
      "\n",
      "Epoch 24 Batch 0 Loss 0.8010\n",
      "Epoch 24 Loss 0.8049\n",
      "Time taken for 1 epoch 245.6498863697052 sec\n",
      "\n",
      "Epoch 25 Batch 0 Loss 0.7434\n",
      "Epoch 25 Loss 0.7335\n",
      "Time taken for 1 epoch 245.84487962722778 sec\n",
      "\n",
      "Epoch 26 Batch 0 Loss 0.6257\n",
      "Epoch 26 Loss 0.6664\n",
      "Time taken for 1 epoch 246.167396068573 sec\n",
      "\n",
      "Epoch 27 Batch 0 Loss 0.5359\n",
      "Epoch 27 Loss 0.6020\n",
      "Time taken for 1 epoch 245.1981439590454 sec\n",
      "\n",
      "Epoch 28 Batch 0 Loss 0.4441\n",
      "Epoch 28 Loss 0.5387\n",
      "Time taken for 1 epoch 245.6396996974945 sec\n",
      "\n",
      "Epoch 29 Batch 0 Loss 0.3844\n",
      "Epoch 29 Loss 0.4787\n",
      "Time taken for 1 epoch 244.89510321617126 sec\n",
      "\n",
      "Epoch 30 Batch 0 Loss 0.4592\n",
      "Epoch 30 Loss 0.4225\n",
      "Time taken for 1 epoch 246.24497628211975 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "EPOCHS = 30\n",
    "\n",
    "# Training loop\n",
    "with tf.device('/cpu:0'):\n",
    "    for epoch in range(EPOCHS):\n",
    "        start = time.time()\n",
    "\n",
    "        # Initialize the hidden state\n",
    "        enc_hidden = encoder.initialize_hidden_state()\n",
    "        total_loss = 0\n",
    "\n",
    "        # Loop through the dataset\n",
    "        for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "\n",
    "            # Call the train method\n",
    "            batch_loss = train_step(inp, targ, enc_hidden)\n",
    "\n",
    "            # Compute the loss (per batch)\n",
    "            total_loss += batch_loss\n",
    "\n",
    "            if batch % 100 == 0:\n",
    "                print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n",
    "\n",
    "        # Save (checkpoint) the model every 2 epochs\n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "        # Output the loss observed until that epoch\n",
    "        print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch))\n",
    "\n",
    "        print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Evaluate function -- similar to the training loop\n",
    "def evaluate(sentence):\n",
    "\n",
    "    # Attention plot (to be plotted later on) -- initialized with max_lengths of both target and input\n",
    "    attention_plot = np.zeros((max_length_target, max_length_input))\n",
    "\n",
    "    # Preprocess the sentence given\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    # Fetch the indices concerning the words in the sentence and pad the sequence\n",
    "    inputs = [questions_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_input,\n",
    "                                                         padding='post')\n",
    "    # Convert the inputs to tensors\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([answers_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    # Loop until the max_length is reached for the target lang (ENGLISH)\n",
    "    for t in range(max_length_target):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "\n",
    "        \n",
    "        # Store the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        # Get the prediction with the maximum attention\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        \n",
    "        # Append the token to the result\n",
    "        result += answers_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        # If <end> token is reached, return the result, input, and attention plot\n",
    "        if answers_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "\n",
    "        # The predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# Function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate function (which internally calls the evaluate function)\n",
    "def translate(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted awnser: {}'.format(result))\n",
    "\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7fbeada12c50>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Restore the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "chatbot.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
