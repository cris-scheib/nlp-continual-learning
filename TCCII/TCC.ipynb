{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-21 16:57:20.289940: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-21 16:57:21.121346: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-06-21 16:57:22.437663: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-21 16:57:22.437853: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-21 16:57:22.437873: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from sklearn.utils import shuffle\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow import keras\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Vocabulary\n",
    "\n",
    "First, we build the vocabulary dictionaries for the source and target. \n",
    "The vocabulary is the the file `vocab.txt` (generated in the other script)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary: [('<unk>', 0), ('<s>', 1), ('</s>', 2), ('.', 3), ('the', 4), (',', 5), ('a', 6), ('to', 7), ('and', 8), ('i', 9)]\n",
      "Reverse dictionary: [(0, '<unk>'), (1, '<s>'), (2, '</s>'), (3, '.'), (4, 'the'), (5, ','), (6, 'a'), (7, 'to'), (8, 'and'), (9, 'i')]\n",
      "Vocabulary size:  50000\n"
     ]
    }
   ],
   "source": [
    "# Word string -> ID mapping\n",
    "dictionary = dict()\n",
    "\n",
    "with open('data/vocab.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        # disregard the new line aka `\\n`\n",
    "        dictionary[line[:-1]] = len(dictionary)\n",
    "        \n",
    "reverse_dictionary = dict(zip(dictionary.values(),dictionary.keys()))\n",
    "\n",
    "print('Dictionary:', list(dictionary.items())[:10], end = '\\n')\n",
    "print('Reverse dictionary:', list(reverse_dictionary.items())[:10], end = '\\n')\n",
    "print('Vocabulary size: ', len(dictionary), end = '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset\n",
    "Here we load the data from the `dataset.csv` file (generated in the other script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('data/dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing\n",
    "Transform to lower, remove the new line and the punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowerDataset(data):\n",
    "    return data.str.lower() \n",
    "    \n",
    "def cleanDataset(data):\n",
    "    return data.str.replace('/r/','')                  \\\n",
    "                .str.replace(')','', regex=False)      \\\n",
    "                .str.replace('(','', regex=False)      \\\n",
    "                .str.replace(']','', regex=False)      \\\n",
    "                .str.replace('[','', regex=False)      \\\n",
    "                .str.replace('!','')                   \\\n",
    "                .str.replace('\"','')                   \\\n",
    "    \n",
    "def paddDataset(data):\n",
    "    return data.str.replace(',', ' ,')                 \\\n",
    "                .str.replace('.',' . ', regex=False)    \\\n",
    "                .str.replace('?',' ?', regex=False)    \\\n",
    "                .str.replace('\\n',' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt = nltk.tokenize.WhitespaceTokenizer()\n",
    "for column in dataset.columns:    \n",
    "    dataset[column] = lowerDataset(dataset[column]) \n",
    "    dataset[column] = cleanDataset(dataset[column])\n",
    "    dataset[column] = paddDataset(dataset[column])                                    \n",
    "    dataset[column] = dataset[column].apply(wt.tokenize)\n",
    "dataset = shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>774878</th>\n",
       "      <td>[saw, the, 'what, makes, you, angry, post', so...</td>\n",
       "      <td>[right, now, ?, i, am, drinking, beer, ., that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168595</th>\n",
       "      <td>[in, your, opinion, ,, what's, the, best, plac...</td>\n",
       "      <td>[buffalo, ,, ny, because, no, one, judges, you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456346</th>\n",
       "      <td>[not, too, long, ago, ,, i, went, to, one, of,...</td>\n",
       "      <td>[lollapalooza, ,, 1994, ., the, weather, where...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430174</th>\n",
       "      <td>[i, was, dumped, by, my, gf, because, she, tho...</td>\n",
       "      <td>[he, said, we, slept, together, on, the, first...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67976</th>\n",
       "      <td>[what's, a, common, fact, that's, actually, in...</td>\n",
       "      <td>[people, actually, did, expect, the, spanish, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 question  \\\n",
       "774878  [saw, the, 'what, makes, you, angry, post', so...   \n",
       "168595  [in, your, opinion, ,, what's, the, best, plac...   \n",
       "456346  [not, too, long, ago, ,, i, went, to, one, of,...   \n",
       "430174  [i, was, dumped, by, my, gf, because, she, tho...   \n",
       "67976   [what's, a, common, fact, that's, actually, in...   \n",
       "\n",
       "                                                   answer  \n",
       "774878  [right, now, ?, i, am, drinking, beer, ., that...  \n",
       "168595  [buffalo, ,, ny, because, no, one, judges, you...  \n",
       "456346  [lollapalooza, ,, 1994, ., the, weather, where...  \n",
       "430174  [he, said, we, slept, together, on, the, first...  \n",
       "67976   [people, actually, did, expect, the, spanish, ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data analysis\n",
    "Mean sentence length and standard deviation of sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Central tendency, dispersion and shape of questions’s distribution\n",
      "count    1149819.000000\n",
      "mean          17.113906\n",
      "std            9.139078\n",
      "min            1.000000\n",
      "25%           11.000000\n",
      "50%           15.000000\n",
      "75%           21.000000\n",
      "max           82.000000\n",
      "Name: question, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print('Central tendency, dispersion and shape of questions’s distribution')\n",
    "print(dataset['question'].str.len().describe().apply(lambda x: format(x, 'f')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Central tendency, dispersion and shape of answers’s distribution\n",
      "count    1149819.000000\n",
      "mean          54.452828\n",
      "std          844.371854\n",
      "min            0.000000\n",
      "25%           10.000000\n",
      "50%           22.000000\n",
      "75%           53.000000\n",
      "max       563680.000000\n",
      "Name: answer, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print('Central tendency, dispersion and shape of answers’s distribution')\n",
    "print(dataset['answer'].str.len().describe().apply(lambda x: format(x, 'f')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the sentences to fixed length\n",
    "Update all sentences with a fixed size, to process the sentences as batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sent_length = {'question' : 30, 'answer': 60}\n",
    "\n",
    "def padding_sent(source):\n",
    "    padded = []\n",
    "    for tokens in dataset[source]: \n",
    "        # adding the start token\n",
    "        tokens.insert(0, '<s>')  \n",
    "\n",
    "        if len(tokens) >= max_sent_length[source]:\n",
    "            tokens = tokens[:(max_sent_length[source] - 1)]\n",
    "            tokens.append('</s>')\n",
    "\n",
    "        if len(tokens) < max_sent_length[source]:\n",
    "            tokens.extend(['</s>' for _ in range(max_sent_length[source] - len(tokens))])  \n",
    "\n",
    "        padded.append(tokens)\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = padding_sent('question')\n",
    "answers = padding_sent('answer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the reverse dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reverse_dataset(source):\n",
    "    reverse_tokens = []\n",
    "    reverse_dataset = []\n",
    "    for tokens in source: \n",
    "        for token in tokens: \n",
    "            if token not in dictionary.keys():\n",
    "                reverse_tokens.append(dictionary['<unk>'])\n",
    "            else:\n",
    "                reverse_tokens.append(dictionary[token])\n",
    "        reverse_dataset.append(reverse_tokens)\n",
    "        reverse_tokens = []\n",
    "    return reverse_dataset\n",
    "\n",
    "inputs_indexes =  np.array(create_reverse_dataset(questions), dtype=np.int32)\n",
    "outputs_indexes =  np.array(create_reverse_dataset(answers), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(questions + answers + [['<unk>']], vector_size=100, window=5, min_count=1, workers=4)\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = list()\n",
    "for input_index in inputs_indexes:\n",
    "    train_input = list()\n",
    "    for index in input_index:\n",
    "        train_input.append(model.wv[reverse_dictionary[index]])\n",
    "    train_inputs.append(train_input)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "chatbot.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
