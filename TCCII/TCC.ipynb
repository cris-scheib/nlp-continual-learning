{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-11 17:40:57.208363: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-11 17:40:57.332930: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-07-11 17:40:57.795900: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-11 17:40:57.795963: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-11 17:40:57.795970: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from sklearn.utils import shuffle\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow import keras\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Vocabulary\n",
    "\n",
    "First, we build the vocabulary dictionaries for the source and target. \n",
    "The vocabulary is the the file `vocab.txt` (generated in the other script)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary: [('<unk>', 0), ('<s>', 1), ('</s>', 2), ('.', 3), ('the', 4), (',', 5), ('a', 6), ('?', 7), ('to', 8), ('you', 9)]\n",
      "Reverse dictionary: [(0, '<unk>'), (1, '<s>'), (2, '</s>'), (3, '.'), (4, 'the'), (5, ','), (6, 'a'), (7, '?'), (8, 'to'), (9, 'you')]\n",
      "Vocabulary size:  30000\n"
     ]
    }
   ],
   "source": [
    "# Word string -> ID mapping\n",
    "dictionary = dict()\n",
    "\n",
    "with open('data/vocab.30K.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        # disregard the new line aka `\\n`\n",
    "        dictionary[line[:-1]] = len(dictionary)\n",
    "        \n",
    "reverse_dictionary = dict(zip(dictionary.values(),dictionary.keys()))\n",
    "\n",
    "print('Dictionary:', list(dictionary.items())[:10], end = '\\n')\n",
    "print('Reverse dictionary:', list(reverse_dictionary.items())[:10], end = '\\n')\n",
    "print('Vocabulary size: ', len(dictionary), end = '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset\n",
    "Here we load the data from the `dataset.csv` file (generated in the other script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('data/dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing\n",
    "Transform to lower, remove the new line and the punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowerDataset(data):\n",
    "    return data.str.lower() \n",
    "    \n",
    "def cleanDataset(data):\n",
    "    return data.str.replace('/r/','')                  \\\n",
    "                .str.replace(')','', regex=False)      \\\n",
    "                .str.replace('(','', regex=False)      \\\n",
    "                .str.replace(']','', regex=False)      \\\n",
    "                .str.replace('[','', regex=False)      \\\n",
    "                .str.replace('!','')                   \\\n",
    "                .str.replace('\"','')                   \\\n",
    "    \n",
    "def paddDataset(data):\n",
    "    return data.str.replace(',', ' ,')                 \\\n",
    "                .str.replace('.',' . ', regex=False)    \\\n",
    "                .str.replace('?',' ?', regex=False)    \\\n",
    "                .str.replace('\\n',' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt = nltk.tokenize.WhitespaceTokenizer()\n",
    "for column in dataset.columns:    \n",
    "    dataset[column] = lowerDataset(dataset[column]) \n",
    "    dataset[column] = cleanDataset(dataset[column])\n",
    "    dataset[column] = paddDataset(dataset[column])                                    \n",
    "    dataset[column] = dataset[column].apply(wt.tokenize)\n",
    "dataset = shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>863578</th>\n",
       "      <td>[reddit, ,, what, are, you, listening, to, rig...</td>\n",
       "      <td>[last, hope, -, paramore, ., some, good, soft,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472061</th>\n",
       "      <td>[serious, what, do, you, regret, the, most, in...</td>\n",
       "      <td>[doing, heroin, for, the, first, time, ., i, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17364</th>\n",
       "      <td>[what, is, the, worst, song, to, play, at, a, ...</td>\n",
       "      <td>[hooker, with, a, penis, -, tool]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734580</th>\n",
       "      <td>[night, shift, workers, of, reddit, ,, what, w...</td>\n",
       "      <td>[i, worked, the, first, shift, at, a, burger, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984743</th>\n",
       "      <td>[theme, park, employees, ,, what, are, some, c...</td>\n",
       "      <td>[so, i, work, at, california's, great, america...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 question  \\\n",
       "863578  [reddit, ,, what, are, you, listening, to, rig...   \n",
       "472061  [serious, what, do, you, regret, the, most, in...   \n",
       "17364   [what, is, the, worst, song, to, play, at, a, ...   \n",
       "734580  [night, shift, workers, of, reddit, ,, what, w...   \n",
       "984743  [theme, park, employees, ,, what, are, some, c...   \n",
       "\n",
       "                                                   answer  \n",
       "863578  [last, hope, -, paramore, ., some, good, soft,...  \n",
       "472061  [doing, heroin, for, the, first, time, ., i, w...  \n",
       "17364                   [hooker, with, a, penis, -, tool]  \n",
       "734580  [i, worked, the, first, shift, at, a, burger, ...  \n",
       "984743  [so, i, work, at, california's, great, america...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data analysis\n",
    "Mean sentence length and standard deviation of sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Central tendency, dispersion and shape of questions’s distribution\n",
      "count    1149819.000000\n",
      "mean          17.113906\n",
      "std            9.139078\n",
      "min            1.000000\n",
      "25%           11.000000\n",
      "50%           15.000000\n",
      "75%           21.000000\n",
      "max           82.000000\n",
      "Name: question, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print('Central tendency, dispersion and shape of questions’s distribution')\n",
    "print(dataset['question'].str.len().describe().apply(lambda x: format(x, 'f')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Central tendency, dispersion and shape of answers’s distribution\n",
      "count    1149819.000000\n",
      "mean          54.452828\n",
      "std          844.371854\n",
      "min            0.000000\n",
      "25%           10.000000\n",
      "50%           22.000000\n",
      "75%           53.000000\n",
      "max       563680.000000\n",
      "Name: answer, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print('Central tendency, dispersion and shape of answers’s distribution')\n",
    "print(dataset['answer'].str.len().describe().apply(lambda x: format(x, 'f')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the sentences to fixed length\n",
    "Update all sentences with a fixed size, to process the sentences as batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sent_length = {'question' : 30, 'answer': 60}\n",
    "\n",
    "def padding_sent(source):\n",
    "    padded = []\n",
    "    for tokens in dataset[source]: \n",
    "        # adding the start token\n",
    "        tokens.insert(0, '<s>')  \n",
    "\n",
    "        if len(tokens) >= max_sent_length[source]:\n",
    "            tokens = tokens[:(max_sent_length[source] - 1)]\n",
    "            tokens.append('</s>')\n",
    "\n",
    "        if len(tokens) < max_sent_length[source]:\n",
    "            tokens.extend(['</s>' for _ in range(max_sent_length[source] - len(tokens))])  \n",
    "\n",
    "        padded.append(tokens)\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = padding_sent('question')\n",
    "answers = padding_sent('answer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the reverse dataset\n",
    "The reverse dataset are going to be used to retrieve the decoder output to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reverse_dataset(source):\n",
    "    reverse_tokens = []\n",
    "    reverse_dataset = []\n",
    "    for tokens in source: \n",
    "        for token in tokens: \n",
    "            if token not in dictionary.keys():\n",
    "                reverse_tokens.append(dictionary['<unk>'])\n",
    "            else:\n",
    "                reverse_tokens.append(dictionary[token])\n",
    "        reverse_dataset.append(reverse_tokens)\n",
    "        reverse_tokens = []\n",
    "    return reverse_dataset\n",
    "\n",
    "inputs_indexes =  np.array(create_reverse_dataset(questions), dtype=np.int32)\n",
    "outputs_indexes =  np.array(create_reverse_dataset(answers), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding\n",
    "Use the Word2Vec to embed the input to a hight demention vector, that will keep the word relationships "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Learns all the Word2Vec relationships fo the questions, answers and unkown words\n",
    "This code needs to be run just once\n",
    "\"\"\"\n",
    "\n",
    "model = Word2Vec(questions + answers + [['<unk>']], vector_size=100, window=5, min_count=1, workers=4)\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formating the inputs and outputs\n",
    "\n",
    "The dataset original format for inputs is 30x100 and for the outputs is 60x30000 for 1.149 million records, making the memory usage impracticable.\n",
    "\n",
    "The inputs and outputs are goint to be refactor to 15x100 for the inputs and 30x30000 for the outputs to a total of 2.299 million records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "input_window_size = 15\n",
    "output_window_size = 30\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#Factor to reshape the dataset\n",
    "reshape_factor = 2 \n",
    "\n",
    "input_window_size = int(max_sent_length['question'] / reshape_factor)\n",
    "output_window_size = int(max_sent_length['answer'] / reshape_factor)\n",
    "\n",
    "def array_numpy(array):\n",
    "    return np.array(array, dtype=np.float32)\n",
    "\n",
    "def get_batch_inputs(batch, batch_size = 10):\n",
    "    train_inputs = list()\n",
    "    \n",
    "    for input_index in inputs_indexes[batch:batch + batch_size]:\n",
    "        train_input = list()   \n",
    "        \n",
    "        for index in input_index:\n",
    "            # Formates the input to the word2vec encoded format\n",
    "            train_input.append(model.wv[reverse_dictionary[index]])\n",
    "            \n",
    "        train_inputs.append(array_numpy(train_input[:input_window_size]))\n",
    "        train_inputs.append(array_numpy(train_input[input_window_size:]))\n",
    "    return array_numpy(train_inputs)\n",
    "\n",
    "def get_batch_outputs(batch, batch_size = 10):\n",
    "    train_outputs = list()\n",
    "    train_targets = list()\n",
    "    \n",
    "    for output_index in outputs_indexes[batch:batch + batch_size]:\n",
    "        train_output = list()\n",
    "        train_target = list()\n",
    "                \n",
    "        for timestep, index in enumerate(output_index):\n",
    "            # Formates the output to the one-hot-encode format\n",
    "            output_encoded = np.zeros(len(dictionary), dtype=np.float32)\n",
    "            output_encoded[index] = 1\n",
    "            train_output.append(output_encoded)\n",
    "            \n",
    "            # Formates the target to the one-hot-encode format\n",
    "            # Setted as index - 1 because it ignores the first <s>\n",
    "            if timestep > 0:\n",
    "                target_encoded = np.zeros(len(dictionary), dtype=np.float32)\n",
    "                target_encoded[output_index[timestep]] = 1.0\n",
    "                train_target.append(target_encoded)\n",
    "        \n",
    "        train_outputs.append(array_numpy(train_output[:output_window_size]))\n",
    "        train_outputs.append(array_numpy(train_output[output_window_size:]))\n",
    "        \n",
    "        #Add a </s> in the end of the target so len(output) == len(taget) \n",
    "        target_encoded = np.zeros(len(dictionary), dtype=np.float32)\n",
    "        target_encoded[output_index[-1]] = 1.0\n",
    "        train_target.append(target_encoded)\n",
    "                \n",
    "        train_targets.append(array_numpy(train_target[:output_window_size]))\n",
    "        train_targets.append(array_numpy(train_target[output_window_size:]))\n",
    "        \n",
    "    return array_numpy(train_outputs), array_numpy(train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_example = get_batch_inputs(0, 1)\n",
    "output_example, target_example = get_batch_outputs(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (2, 15, 100)\n",
      "Output shape: (2, 30, 30000)\n",
      "Target shape: (2, 30, 30000)\n"
     ]
    }
   ],
   "source": [
    "print(\"Input shape:\", input_example.shape)\n",
    "print(\"Output shape:\", output_example.shape)\n",
    "print(\"Target shape:\", target_example.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-11 17:43:35.446199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-11 17:43:35.478370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-11 17:43:35.478551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-11 17:43:35.478949: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-11 17:43:35.479197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-11 17:43:35.479346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-11 17:43:35.479478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-11 17:43:35.766036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-11 17:43:35.766220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-11 17:43:35.766359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-11 17:43:35.766466: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3181 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "# Input shape = 100\n",
    "input_shape = input_example.shape[2]\n",
    "\n",
    "# Output shape = 30000\n",
    "output_shape = output_example.shape[2]\n",
    "\n",
    "#Dimensionality\n",
    "dimensionality = 256\n",
    "\n",
    "#The batch size and number of epochs\n",
    "batch_size = 10\n",
    "epochs = 150\n",
    "\n",
    "#Encoder\n",
    "encoder_inputs = Input(shape=(None, input_shape))\n",
    "encoder_lstm = LSTM(dimensionality, return_state=True)\n",
    "encoder_outputs, state_hidden, state_cell = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [state_hidden, state_cell]\n",
    "\n",
    "#Decoder\n",
    "decoder_inputs = Input(shape=(None, output_shape))\n",
    "decoder_lstm = LSTM(dimensionality, return_sequences=True, return_state=True)\n",
    "decoder_outputs, decoder_state_hidden, decoder_state_cell = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(output_shape, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "#Model\n",
    "training_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "#Compiling\n",
    "training_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'], sample_weight_mode='temporal')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training in batches\n",
    "\n",
    "Due to memory usage, the training will be divided into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_records = len(dataset) * reshape_factor\n",
    "data_batch_size = 50\n",
    "supported_batch_records = num_records / data_batch_size\n",
    "batch_records = math.ceil(supported_batch_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using  45993  batches\n",
      "With  50  (inputs, outputs) each\n",
      "With the totall of  2299638  pairs to process\n"
     ]
    }
   ],
   "source": [
    "print(\"Using \", batch_records, \" batches\")\n",
    "print(\"With \", data_batch_size, \" (inputs, outputs) each\")\n",
    "print(\"With the totall of \", num_records, \" pairs to process\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning memory\n",
    "\n",
    "Removing variables that are no longer needed before start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset\n",
    "del questions\n",
    "del answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-11 17:43:52.239846: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 4s 205ms/step - loss: 10.0199 - accuracy: 0.4275 - val_loss: 9.0680 - val_accuracy: 0.4533\n",
      "Epoch 2/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 7.4042 - accuracy: 0.5183 - val_loss: 6.1945 - val_accuracy: 0.4533\n",
      "Epoch 3/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 4.4377 - accuracy: 0.5183 - val_loss: 4.9472 - val_accuracy: 0.4533\n",
      "Epoch 4/150\n",
      "8/8 [==============================] - 1s 126ms/step - loss: 3.3941 - accuracy: 0.5183 - val_loss: 5.0993 - val_accuracy: 0.4533\n",
      "Epoch 5/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 3.4368 - accuracy: 0.5183 - val_loss: 5.2685 - val_accuracy: 0.4533\n",
      "Epoch 6/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 3.3446 - accuracy: 0.5183 - val_loss: 5.2588 - val_accuracy: 0.4533\n",
      "Epoch 7/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 3.3508 - accuracy: 0.5183 - val_loss: 5.3026 - val_accuracy: 0.4533\n",
      "Epoch 8/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 3.2967 - accuracy: 0.5183 - val_loss: 5.3114 - val_accuracy: 0.4533\n",
      "Epoch 9/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 3.2947 - accuracy: 0.5183 - val_loss: 5.3119 - val_accuracy: 0.4533\n",
      "Epoch 10/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 3.2730 - accuracy: 0.5183 - val_loss: 5.3102 - val_accuracy: 0.4533\n",
      "Epoch 11/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 3.2603 - accuracy: 0.5183 - val_loss: 5.3061 - val_accuracy: 0.4533\n",
      "Epoch 12/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 3.2572 - accuracy: 0.5183 - val_loss: 5.3020 - val_accuracy: 0.4533\n",
      "Epoch 13/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 3.2364 - accuracy: 0.5183 - val_loss: 5.3151 - val_accuracy: 0.4533\n",
      "Epoch 14/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 3.2189 - accuracy: 0.5183 - val_loss: 5.2601 - val_accuracy: 0.4533\n",
      "Epoch 15/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 3.2005 - accuracy: 0.5183 - val_loss: 5.2757 - val_accuracy: 0.4533\n",
      "Epoch 16/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 3.1921 - accuracy: 0.5183 - val_loss: 5.2796 - val_accuracy: 0.4533\n",
      "Epoch 17/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 3.1677 - accuracy: 0.5183 - val_loss: 5.2676 - val_accuracy: 0.4533\n",
      "Epoch 18/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 3.1590 - accuracy: 0.5183 - val_loss: 5.3033 - val_accuracy: 0.4533\n",
      "Epoch 19/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 3.1357 - accuracy: 0.5183 - val_loss: 5.2440 - val_accuracy: 0.4533\n",
      "Epoch 20/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 3.1275 - accuracy: 0.5183 - val_loss: 5.2926 - val_accuracy: 0.4533\n",
      "Epoch 21/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 3.0964 - accuracy: 0.5183 - val_loss: 5.2906 - val_accuracy: 0.4533\n",
      "Epoch 22/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 3.0800 - accuracy: 0.5275 - val_loss: 5.2644 - val_accuracy: 0.4700\n",
      "Epoch 23/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 3.0747 - accuracy: 0.5196 - val_loss: 5.3098 - val_accuracy: 0.4650\n",
      "Epoch 24/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 3.0453 - accuracy: 0.5304 - val_loss: 5.2729 - val_accuracy: 0.4633\n",
      "Epoch 25/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 3.0331 - accuracy: 0.5308 - val_loss: 5.2727 - val_accuracy: 0.4700\n",
      "Epoch 26/150\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 3.0448 - accuracy: 0.5329 - val_loss: 5.3084 - val_accuracy: 0.4667\n",
      "Epoch 27/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 3.0312 - accuracy: 0.5312 - val_loss: 5.3057 - val_accuracy: 0.4667\n",
      "Epoch 28/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 2.9808 - accuracy: 0.5346 - val_loss: 5.3118 - val_accuracy: 0.4700\n",
      "Epoch 29/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 2.9500 - accuracy: 0.5333 - val_loss: 5.3302 - val_accuracy: 0.4700\n",
      "Epoch 30/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 2.9283 - accuracy: 0.5342 - val_loss: 5.3383 - val_accuracy: 0.4683\n",
      "Epoch 31/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 2.8976 - accuracy: 0.5337 - val_loss: 5.3306 - val_accuracy: 0.4683\n",
      "Epoch 32/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 2.8768 - accuracy: 0.5342 - val_loss: 5.3176 - val_accuracy: 0.4683\n",
      "Epoch 33/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 2.8538 - accuracy: 0.5350 - val_loss: 5.3974 - val_accuracy: 0.4683\n",
      "Epoch 34/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 2.8362 - accuracy: 0.5350 - val_loss: 5.3787 - val_accuracy: 0.4683\n",
      "Epoch 35/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 2.8114 - accuracy: 0.5342 - val_loss: 5.2996 - val_accuracy: 0.4667\n",
      "Epoch 36/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 2.8059 - accuracy: 0.5350 - val_loss: 5.3776 - val_accuracy: 0.4683\n",
      "Epoch 37/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 2.7792 - accuracy: 0.5354 - val_loss: 5.4225 - val_accuracy: 0.4667\n",
      "Epoch 38/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 2.7588 - accuracy: 0.5350 - val_loss: 5.4367 - val_accuracy: 0.4667\n",
      "Epoch 39/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 2.7357 - accuracy: 0.5367 - val_loss: 5.3983 - val_accuracy: 0.4683\n",
      "Epoch 40/150\n",
      "8/8 [==============================] - 1s 126ms/step - loss: 2.7081 - accuracy: 0.5379 - val_loss: 5.4986 - val_accuracy: 0.4667\n",
      "Epoch 41/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 2.7133 - accuracy: 0.5379 - val_loss: 5.4728 - val_accuracy: 0.4700\n",
      "Epoch 42/150\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 2.6941 - accuracy: 0.5375 - val_loss: 5.4641 - val_accuracy: 0.4650\n",
      "Epoch 43/150\n",
      "8/8 [==============================] - 1s 128ms/step - loss: 2.6887 - accuracy: 0.5371 - val_loss: 5.4339 - val_accuracy: 0.4650\n",
      "Epoch 44/150\n",
      "8/8 [==============================] - 1s 126ms/step - loss: 2.6750 - accuracy: 0.5404 - val_loss: 5.4821 - val_accuracy: 0.4583\n",
      "Epoch 45/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 2.6515 - accuracy: 0.5408 - val_loss: 5.6170 - val_accuracy: 0.4650\n",
      "Epoch 46/150\n",
      "8/8 [==============================] - 1s 126ms/step - loss: 2.6366 - accuracy: 0.5408 - val_loss: 5.5001 - val_accuracy: 0.4550\n",
      "Epoch 47/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 2.6270 - accuracy: 0.5421 - val_loss: 5.4825 - val_accuracy: 0.4483\n",
      "Epoch 48/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 2.6079 - accuracy: 0.5421 - val_loss: 5.6287 - val_accuracy: 0.4633\n",
      "Epoch 49/150\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 2.5934 - accuracy: 0.5425 - val_loss: 5.6858 - val_accuracy: 0.4617\n",
      "Epoch 50/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 2.5801 - accuracy: 0.5450 - val_loss: 5.5841 - val_accuracy: 0.4400\n",
      "Epoch 51/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 2.5751 - accuracy: 0.5429 - val_loss: 5.5740 - val_accuracy: 0.4450\n",
      "Epoch 52/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 2.5600 - accuracy: 0.5471 - val_loss: 5.6954 - val_accuracy: 0.4650\n",
      "Epoch 53/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 2.5287 - accuracy: 0.5450 - val_loss: 5.6305 - val_accuracy: 0.4383\n",
      "Epoch 54/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 2.5160 - accuracy: 0.5446 - val_loss: 5.6511 - val_accuracy: 0.4433\n",
      "Epoch 55/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 2.4926 - accuracy: 0.5496 - val_loss: 5.7025 - val_accuracy: 0.4433\n",
      "Epoch 56/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 2.4719 - accuracy: 0.5492 - val_loss: 5.7071 - val_accuracy: 0.4500\n",
      "Epoch 57/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 2.4522 - accuracy: 0.5504 - val_loss: 5.6757 - val_accuracy: 0.4400\n",
      "Epoch 58/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 2.4483 - accuracy: 0.5492 - val_loss: 5.6286 - val_accuracy: 0.4283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 2.4423 - accuracy: 0.5525 - val_loss: 5.7379 - val_accuracy: 0.4483\n",
      "Epoch 60/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 2.4217 - accuracy: 0.5562 - val_loss: 5.6650 - val_accuracy: 0.4300\n",
      "Epoch 61/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 2.4016 - accuracy: 0.5533 - val_loss: 5.6714 - val_accuracy: 0.4333\n",
      "Epoch 62/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 2.3783 - accuracy: 0.5550 - val_loss: 5.7403 - val_accuracy: 0.4317\n",
      "Epoch 63/150\n",
      "8/8 [==============================] - 1s 126ms/step - loss: 2.3644 - accuracy: 0.5583 - val_loss: 5.7189 - val_accuracy: 0.4350\n",
      "Epoch 64/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 2.3430 - accuracy: 0.5554 - val_loss: 5.7232 - val_accuracy: 0.4300\n",
      "Epoch 65/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 2.3268 - accuracy: 0.5583 - val_loss: 5.7702 - val_accuracy: 0.4283\n",
      "Epoch 66/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 2.3256 - accuracy: 0.5554 - val_loss: 5.6309 - val_accuracy: 0.4233\n",
      "Epoch 67/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 2.3174 - accuracy: 0.5558 - val_loss: 5.7347 - val_accuracy: 0.4267\n",
      "Epoch 68/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 2.3055 - accuracy: 0.5596 - val_loss: 5.8346 - val_accuracy: 0.4467\n",
      "Epoch 69/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 2.2947 - accuracy: 0.5587 - val_loss: 5.7219 - val_accuracy: 0.4200\n",
      "Epoch 70/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 2.2740 - accuracy: 0.5604 - val_loss: 5.6984 - val_accuracy: 0.4217\n",
      "Epoch 71/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 2.2631 - accuracy: 0.5625 - val_loss: 5.8591 - val_accuracy: 0.4417\n",
      "Epoch 72/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 2.2337 - accuracy: 0.5617 - val_loss: 5.6848 - val_accuracy: 0.4150\n",
      "Epoch 73/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 2.2208 - accuracy: 0.5621 - val_loss: 5.7027 - val_accuracy: 0.4250\n",
      "Epoch 74/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 2.1951 - accuracy: 0.5625 - val_loss: 5.8929 - val_accuracy: 0.4333\n",
      "Epoch 75/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 2.1945 - accuracy: 0.5642 - val_loss: 5.7815 - val_accuracy: 0.4250\n",
      "Epoch 76/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 2.1619 - accuracy: 0.5650 - val_loss: 5.7219 - val_accuracy: 0.4217\n",
      "Epoch 77/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 2.1430 - accuracy: 0.5692 - val_loss: 5.8327 - val_accuracy: 0.4233\n",
      "Epoch 78/150\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 2.1303 - accuracy: 0.5683 - val_loss: 5.8317 - val_accuracy: 0.4250\n",
      "Epoch 79/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 2.1030 - accuracy: 0.5733 - val_loss: 5.7635 - val_accuracy: 0.4217\n",
      "Epoch 80/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 2.0804 - accuracy: 0.5738 - val_loss: 5.7441 - val_accuracy: 0.4250\n",
      "Epoch 81/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 2.0592 - accuracy: 0.5754 - val_loss: 5.7454 - val_accuracy: 0.4250\n",
      "Epoch 82/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 2.0395 - accuracy: 0.5763 - val_loss: 5.8268 - val_accuracy: 0.4250\n",
      "Epoch 83/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 2.0211 - accuracy: 0.5783 - val_loss: 5.8474 - val_accuracy: 0.4283\n",
      "Epoch 84/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 2.0046 - accuracy: 0.5800 - val_loss: 5.8520 - val_accuracy: 0.4267\n",
      "Epoch 85/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 1.9835 - accuracy: 0.5871 - val_loss: 5.7410 - val_accuracy: 0.4233\n",
      "Epoch 86/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 1.9671 - accuracy: 0.5854 - val_loss: 5.7399 - val_accuracy: 0.4150\n",
      "Epoch 87/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 1.9445 - accuracy: 0.5908 - val_loss: 5.8771 - val_accuracy: 0.4317\n",
      "Epoch 88/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 1.9264 - accuracy: 0.5942 - val_loss: 5.9102 - val_accuracy: 0.4317\n",
      "Epoch 89/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 1.9023 - accuracy: 0.5983 - val_loss: 5.8037 - val_accuracy: 0.4233\n",
      "Epoch 90/150\n",
      "8/8 [==============================] - 1s 127ms/step - loss: 1.8791 - accuracy: 0.6004 - val_loss: 5.7732 - val_accuracy: 0.4217\n",
      "Epoch 91/150\n",
      "8/8 [==============================] - 1s 127ms/step - loss: 1.8614 - accuracy: 0.6104 - val_loss: 5.7724 - val_accuracy: 0.4300\n",
      "Epoch 92/150\n",
      "8/8 [==============================] - 1s 126ms/step - loss: 1.8404 - accuracy: 0.6117 - val_loss: 5.7965 - val_accuracy: 0.4233\n",
      "Epoch 93/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 1.8141 - accuracy: 0.6196 - val_loss: 5.9305 - val_accuracy: 0.4367\n",
      "Epoch 94/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 1.7954 - accuracy: 0.6225 - val_loss: 5.9529 - val_accuracy: 0.4433\n",
      "Epoch 95/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 1.7844 - accuracy: 0.6263 - val_loss: 5.7747 - val_accuracy: 0.4317\n",
      "Epoch 96/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 1.7585 - accuracy: 0.6283 - val_loss: 5.7475 - val_accuracy: 0.4350\n",
      "Epoch 97/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 1.7280 - accuracy: 0.6358 - val_loss: 5.9055 - val_accuracy: 0.4450\n",
      "Epoch 98/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 1.7081 - accuracy: 0.6446 - val_loss: 5.9116 - val_accuracy: 0.4433\n",
      "Epoch 99/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 1.6783 - accuracy: 0.6475 - val_loss: 5.7766 - val_accuracy: 0.4400\n",
      "Epoch 100/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 1.6562 - accuracy: 0.6529 - val_loss: 5.8171 - val_accuracy: 0.4383\n",
      "Epoch 101/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 1.6299 - accuracy: 0.6596 - val_loss: 5.8500 - val_accuracy: 0.4483\n",
      "Epoch 102/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 1.6049 - accuracy: 0.6683 - val_loss: 5.8511 - val_accuracy: 0.4467\n",
      "Epoch 103/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 1.5791 - accuracy: 0.6787 - val_loss: 5.8837 - val_accuracy: 0.4483\n",
      "Epoch 104/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 1.5558 - accuracy: 0.6837 - val_loss: 5.8858 - val_accuracy: 0.4533\n",
      "Epoch 105/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 1.5300 - accuracy: 0.6925 - val_loss: 5.9300 - val_accuracy: 0.4550\n",
      "Epoch 106/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 1.5070 - accuracy: 0.6913 - val_loss: 5.8506 - val_accuracy: 0.4567\n",
      "Epoch 107/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 1.4814 - accuracy: 0.7050 - val_loss: 5.8961 - val_accuracy: 0.4567\n",
      "Epoch 108/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 1.4565 - accuracy: 0.7083 - val_loss: 5.7562 - val_accuracy: 0.4633\n",
      "Epoch 109/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 1.4357 - accuracy: 0.7158 - val_loss: 5.5357 - val_accuracy: 0.4767\n",
      "Epoch 110/150\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 1.4191 - accuracy: 0.7150 - val_loss: 5.5996 - val_accuracy: 0.4767\n",
      "Epoch 111/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 1.3956 - accuracy: 0.7221 - val_loss: 5.8023 - val_accuracy: 0.4733\n",
      "Epoch 112/150\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 1.3633 - accuracy: 0.7367 - val_loss: 5.8215 - val_accuracy: 0.4750\n",
      "Epoch 113/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 1.3400 - accuracy: 0.7362 - val_loss: 5.8264 - val_accuracy: 0.4733\n",
      "Epoch 114/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 1.3137 - accuracy: 0.7425 - val_loss: 5.6453 - val_accuracy: 0.4817\n",
      "Epoch 115/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 1.2891 - accuracy: 0.7492 - val_loss: 5.7057 - val_accuracy: 0.4733\n",
      "Epoch 116/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 122ms/step - loss: 1.2676 - accuracy: 0.7571 - val_loss: 5.7522 - val_accuracy: 0.4783\n",
      "Epoch 117/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 1.2422 - accuracy: 0.7688 - val_loss: 5.7197 - val_accuracy: 0.4817\n",
      "Epoch 118/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 1.2181 - accuracy: 0.7754 - val_loss: 5.7889 - val_accuracy: 0.4767\n",
      "Epoch 119/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 1.1952 - accuracy: 0.7808 - val_loss: 5.7796 - val_accuracy: 0.4817\n",
      "Epoch 120/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 1.1711 - accuracy: 0.7912 - val_loss: 5.7822 - val_accuracy: 0.4833\n",
      "Epoch 121/150\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 1.1496 - accuracy: 0.7950 - val_loss: 5.7151 - val_accuracy: 0.4900\n",
      "Epoch 122/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 1.1285 - accuracy: 0.8075 - val_loss: 5.7871 - val_accuracy: 0.4850\n",
      "Epoch 123/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 1.1066 - accuracy: 0.8117 - val_loss: 5.7632 - val_accuracy: 0.4917\n",
      "Epoch 124/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 1.0852 - accuracy: 0.8204 - val_loss: 5.7656 - val_accuracy: 0.4967\n",
      "Epoch 125/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 1.0643 - accuracy: 0.8233 - val_loss: 5.7916 - val_accuracy: 0.4900\n",
      "Epoch 126/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 1.0453 - accuracy: 0.8346 - val_loss: 5.7265 - val_accuracy: 0.5000\n",
      "Epoch 127/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 1.0227 - accuracy: 0.8358 - val_loss: 5.6808 - val_accuracy: 0.5050\n",
      "Epoch 128/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 1.0048 - accuracy: 0.8429 - val_loss: 5.7041 - val_accuracy: 0.5050\n",
      "Epoch 129/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.9855 - accuracy: 0.8512 - val_loss: 5.6106 - val_accuracy: 0.5100\n",
      "Epoch 130/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.9682 - accuracy: 0.8567 - val_loss: 5.6603 - val_accuracy: 0.5067\n",
      "Epoch 131/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.9481 - accuracy: 0.8646 - val_loss: 5.5818 - val_accuracy: 0.5217\n",
      "Epoch 132/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.9300 - accuracy: 0.8667 - val_loss: 5.7010 - val_accuracy: 0.5100\n",
      "Epoch 133/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.9124 - accuracy: 0.8737 - val_loss: 5.7938 - val_accuracy: 0.5100\n",
      "Epoch 134/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.8947 - accuracy: 0.8788 - val_loss: 5.5473 - val_accuracy: 0.5183\n",
      "Epoch 135/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.8802 - accuracy: 0.8825 - val_loss: 5.6681 - val_accuracy: 0.5150\n",
      "Epoch 136/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.8652 - accuracy: 0.8904 - val_loss: 5.4663 - val_accuracy: 0.5267\n",
      "Epoch 137/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.8489 - accuracy: 0.8967 - val_loss: 5.4769 - val_accuracy: 0.5283\n",
      "Epoch 138/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.8280 - accuracy: 0.9021 - val_loss: 5.6485 - val_accuracy: 0.5217\n",
      "Epoch 139/150\n",
      "8/8 [==============================] - 1s 126ms/step - loss: 0.8140 - accuracy: 0.9038 - val_loss: 5.6319 - val_accuracy: 0.5183\n",
      "Epoch 140/150\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 0.7948 - accuracy: 0.9158 - val_loss: 5.7917 - val_accuracy: 0.5200\n",
      "Epoch 141/150\n",
      "8/8 [==============================] - 1s 126ms/step - loss: 0.7804 - accuracy: 0.9150 - val_loss: 5.7105 - val_accuracy: 0.5217\n",
      "Epoch 142/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.7640 - accuracy: 0.9154 - val_loss: 5.5285 - val_accuracy: 0.5200\n",
      "Epoch 143/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.7492 - accuracy: 0.9250 - val_loss: 5.4430 - val_accuracy: 0.5367\n",
      "Epoch 144/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 0.7332 - accuracy: 0.9279 - val_loss: 5.5920 - val_accuracy: 0.5233\n",
      "Epoch 145/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.7172 - accuracy: 0.9308 - val_loss: 5.5590 - val_accuracy: 0.5367\n",
      "Epoch 146/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.7029 - accuracy: 0.9350 - val_loss: 5.5993 - val_accuracy: 0.5283\n",
      "Epoch 147/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.6882 - accuracy: 0.9342 - val_loss: 5.5206 - val_accuracy: 0.5367\n",
      "Epoch 148/150\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 0.6727 - accuracy: 0.9388 - val_loss: 5.5019 - val_accuracy: 0.5467\n",
      "Epoch 149/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.6596 - accuracy: 0.9421 - val_loss: 5.7213 - val_accuracy: 0.5333\n",
      "Epoch 150/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.6481 - accuracy: 0.9438 - val_loss: 5.5701 - val_accuracy: 0.5383\n",
      "Epoch 1/150\n",
      "8/8 [==============================] - 1s 154ms/step - loss: 0.8873 - accuracy: 0.9346 - val_loss: 4.6367 - val_accuracy: 0.5817\n",
      "Epoch 2/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.8107 - accuracy: 0.9308 - val_loss: 4.4849 - val_accuracy: 0.5717\n",
      "Epoch 3/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.7846 - accuracy: 0.9267 - val_loss: 4.3437 - val_accuracy: 0.5917\n",
      "Epoch 4/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 0.7407 - accuracy: 0.9358 - val_loss: 4.5613 - val_accuracy: 0.5800\n",
      "Epoch 5/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.6972 - accuracy: 0.9392 - val_loss: 4.4051 - val_accuracy: 0.5917\n",
      "Epoch 6/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.6684 - accuracy: 0.9404 - val_loss: 4.3963 - val_accuracy: 0.5900\n",
      "Epoch 7/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.6410 - accuracy: 0.9442 - val_loss: 4.4987 - val_accuracy: 0.5833\n",
      "Epoch 8/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.6180 - accuracy: 0.9454 - val_loss: 4.4326 - val_accuracy: 0.5867\n",
      "Epoch 9/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.5976 - accuracy: 0.9471 - val_loss: 4.5064 - val_accuracy: 0.5900\n",
      "Epoch 10/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.5798 - accuracy: 0.9504 - val_loss: 4.5034 - val_accuracy: 0.5867\n",
      "Epoch 11/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.5629 - accuracy: 0.9529 - val_loss: 4.5453 - val_accuracy: 0.5883\n",
      "Epoch 12/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 0.5473 - accuracy: 0.9554 - val_loss: 4.5101 - val_accuracy: 0.5917\n",
      "Epoch 13/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.5336 - accuracy: 0.9604 - val_loss: 4.6116 - val_accuracy: 0.5883\n",
      "Epoch 14/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.5199 - accuracy: 0.9596 - val_loss: 4.5364 - val_accuracy: 0.5917\n",
      "Epoch 15/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.5075 - accuracy: 0.9633 - val_loss: 4.5745 - val_accuracy: 0.5917\n",
      "Epoch 16/150\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 0.4933 - accuracy: 0.9646 - val_loss: 4.6203 - val_accuracy: 0.5883\n",
      "Epoch 17/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.4802 - accuracy: 0.9658 - val_loss: 4.5964 - val_accuracy: 0.5917\n",
      "Epoch 18/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.4692 - accuracy: 0.9696 - val_loss: 4.6363 - val_accuracy: 0.5850\n",
      "Epoch 19/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.4578 - accuracy: 0.9700 - val_loss: 4.6448 - val_accuracy: 0.5883\n",
      "Epoch 20/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.4450 - accuracy: 0.9758 - val_loss: 4.6054 - val_accuracy: 0.5917\n",
      "Epoch 21/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.4349 - accuracy: 0.9750 - val_loss: 4.5722 - val_accuracy: 0.5950\n",
      "Epoch 22/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.4242 - accuracy: 0.9792 - val_loss: 4.6339 - val_accuracy: 0.5933\n",
      "Epoch 23/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 128ms/step - loss: 0.4129 - accuracy: 0.9800 - val_loss: 4.6382 - val_accuracy: 0.5933\n",
      "Epoch 24/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.4034 - accuracy: 0.9796 - val_loss: 4.5784 - val_accuracy: 0.5950\n",
      "Epoch 25/150\n",
      "8/8 [==============================] - 1s 126ms/step - loss: 0.3933 - accuracy: 0.9833 - val_loss: 4.5866 - val_accuracy: 0.5933\n",
      "Epoch 26/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.3818 - accuracy: 0.9837 - val_loss: 4.6426 - val_accuracy: 0.5933\n",
      "Epoch 27/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 0.3719 - accuracy: 0.9854 - val_loss: 4.6675 - val_accuracy: 0.5933\n",
      "Epoch 28/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 0.3629 - accuracy: 0.9875 - val_loss: 4.5708 - val_accuracy: 0.5967\n",
      "Epoch 29/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.3534 - accuracy: 0.9867 - val_loss: 4.5811 - val_accuracy: 0.5967\n",
      "Epoch 30/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.3446 - accuracy: 0.9887 - val_loss: 4.6585 - val_accuracy: 0.5933\n",
      "Epoch 31/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 0.3348 - accuracy: 0.9879 - val_loss: 4.6276 - val_accuracy: 0.5933\n",
      "Epoch 32/150\n",
      "8/8 [==============================] - 1s 126ms/step - loss: 0.3278 - accuracy: 0.9887 - val_loss: 4.6361 - val_accuracy: 0.5950\n",
      "Epoch 33/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.3181 - accuracy: 0.9892 - val_loss: 4.6289 - val_accuracy: 0.5950\n",
      "Epoch 34/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.3094 - accuracy: 0.9904 - val_loss: 4.6684 - val_accuracy: 0.5917\n",
      "Epoch 35/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.3013 - accuracy: 0.9912 - val_loss: 4.6177 - val_accuracy: 0.5933\n",
      "Epoch 36/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.2933 - accuracy: 0.9917 - val_loss: 4.5788 - val_accuracy: 0.5967\n",
      "Epoch 37/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.2860 - accuracy: 0.9925 - val_loss: 4.6624 - val_accuracy: 0.5950\n",
      "Epoch 38/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.2781 - accuracy: 0.9921 - val_loss: 4.6548 - val_accuracy: 0.5933\n",
      "Epoch 39/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 0.2709 - accuracy: 0.9921 - val_loss: 4.5775 - val_accuracy: 0.5983\n",
      "Epoch 40/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.2636 - accuracy: 0.9937 - val_loss: 4.5973 - val_accuracy: 0.5967\n",
      "Epoch 41/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.2562 - accuracy: 0.9933 - val_loss: 4.5877 - val_accuracy: 0.5983\n",
      "Epoch 42/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.2503 - accuracy: 0.9937 - val_loss: 4.4381 - val_accuracy: 0.6033\n",
      "Epoch 43/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.2456 - accuracy: 0.9946 - val_loss: 4.4970 - val_accuracy: 0.6050\n",
      "Epoch 44/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.2386 - accuracy: 0.9950 - val_loss: 4.5927 - val_accuracy: 0.6017\n",
      "Epoch 45/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.2331 - accuracy: 0.9967 - val_loss: 4.6764 - val_accuracy: 0.5833\n",
      "Epoch 46/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.2278 - accuracy: 0.9967 - val_loss: 4.6828 - val_accuracy: 0.5983\n",
      "Epoch 47/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.2212 - accuracy: 0.9971 - val_loss: 4.5831 - val_accuracy: 0.5933\n",
      "Epoch 48/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.2137 - accuracy: 0.9971 - val_loss: 4.5224 - val_accuracy: 0.6050\n",
      "Epoch 49/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 0.2086 - accuracy: 0.9983 - val_loss: 4.6129 - val_accuracy: 0.5933\n",
      "Epoch 50/150\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 0.2020 - accuracy: 0.9983 - val_loss: 4.6243 - val_accuracy: 0.6033\n",
      "Epoch 51/150\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 0.1960 - accuracy: 0.9975 - val_loss: 4.5752 - val_accuracy: 0.6033\n",
      "Epoch 52/150\n",
      "8/8 [==============================] - 1s 126ms/step - loss: 0.1922 - accuracy: 0.9979 - val_loss: 4.6658 - val_accuracy: 0.5950\n",
      "Epoch 53/150\n",
      "8/8 [==============================] - 1s 126ms/step - loss: 0.1855 - accuracy: 0.9979 - val_loss: 4.6348 - val_accuracy: 0.6050\n",
      "Epoch 54/150\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 0.1805 - accuracy: 0.9992 - val_loss: 4.5817 - val_accuracy: 0.6033\n",
      "Epoch 55/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.1758 - accuracy: 0.9987 - val_loss: 4.6293 - val_accuracy: 0.6000\n",
      "Epoch 56/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.1708 - accuracy: 0.9987 - val_loss: 4.6062 - val_accuracy: 0.6017\n",
      "Epoch 57/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 0.1654 - accuracy: 0.9987 - val_loss: 4.6511 - val_accuracy: 0.6017\n",
      "Epoch 58/150\n",
      "8/8 [==============================] - 1s 126ms/step - loss: 0.1607 - accuracy: 0.9987 - val_loss: 4.5902 - val_accuracy: 0.6017\n",
      "Epoch 59/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.1560 - accuracy: 0.9992 - val_loss: 4.6419 - val_accuracy: 0.6017\n",
      "Epoch 60/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.1523 - accuracy: 0.9992 - val_loss: 4.6457 - val_accuracy: 0.6000\n",
      "Epoch 61/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.1482 - accuracy: 0.9992 - val_loss: 4.6018 - val_accuracy: 0.6017\n",
      "Epoch 62/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.1448 - accuracy: 0.9992 - val_loss: 4.5478 - val_accuracy: 0.6083\n",
      "Epoch 63/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.1411 - accuracy: 0.9992 - val_loss: 4.5972 - val_accuracy: 0.6067\n",
      "Epoch 64/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.1372 - accuracy: 0.9992 - val_loss: 4.5482 - val_accuracy: 0.6083\n",
      "Epoch 65/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 0.1345 - accuracy: 0.9992 - val_loss: 4.6911 - val_accuracy: 0.5967\n",
      "Epoch 66/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.1314 - accuracy: 1.0000 - val_loss: 4.8031 - val_accuracy: 0.6017\n",
      "Epoch 67/150\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 0.1335 - accuracy: 0.9987 - val_loss: 4.7500 - val_accuracy: 0.6050\n",
      "Epoch 68/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.1286 - accuracy: 0.9992 - val_loss: 4.4280 - val_accuracy: 0.6200\n",
      "Epoch 69/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.1223 - accuracy: 0.9992 - val_loss: 4.5379 - val_accuracy: 0.6117\n",
      "Epoch 70/150\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 0.1185 - accuracy: 1.0000 - val_loss: 4.6184 - val_accuracy: 0.6067\n",
      "Epoch 71/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.1150 - accuracy: 1.0000 - val_loss: 4.5966 - val_accuracy: 0.6083\n",
      "Epoch 72/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.1116 - accuracy: 1.0000 - val_loss: 4.5623 - val_accuracy: 0.6083\n",
      "Epoch 73/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.1081 - accuracy: 0.9996 - val_loss: 4.5750 - val_accuracy: 0.6083\n",
      "Epoch 74/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.1053 - accuracy: 1.0000 - val_loss: 4.6224 - val_accuracy: 0.6083\n",
      "Epoch 75/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.1023 - accuracy: 1.0000 - val_loss: 4.6589 - val_accuracy: 0.6033\n",
      "Epoch 76/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0997 - accuracy: 1.0000 - val_loss: 4.6084 - val_accuracy: 0.6100\n",
      "Epoch 77/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0971 - accuracy: 1.0000 - val_loss: 4.6166 - val_accuracy: 0.6100\n",
      "Epoch 78/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0947 - accuracy: 1.0000 - val_loss: 4.6558 - val_accuracy: 0.6083\n",
      "Epoch 79/150\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 0.0920 - accuracy: 1.0000 - val_loss: 4.6411 - val_accuracy: 0.6117\n",
      "Epoch 80/150\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 0.0905 - accuracy: 1.0000 - val_loss: 4.5752 - val_accuracy: 0.6117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0884 - accuracy: 1.0000 - val_loss: 4.6102 - val_accuracy: 0.6083\n",
      "Epoch 82/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0867 - accuracy: 1.0000 - val_loss: 4.6837 - val_accuracy: 0.6083\n",
      "Epoch 83/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 0.0845 - accuracy: 0.9996 - val_loss: 4.6429 - val_accuracy: 0.6100\n",
      "Epoch 84/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0820 - accuracy: 1.0000 - val_loss: 4.6682 - val_accuracy: 0.6083\n",
      "Epoch 85/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 0.0802 - accuracy: 1.0000 - val_loss: 4.5862 - val_accuracy: 0.6117\n",
      "Epoch 86/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0780 - accuracy: 1.0000 - val_loss: 4.6973 - val_accuracy: 0.6100\n",
      "Epoch 87/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0763 - accuracy: 1.0000 - val_loss: 4.6583 - val_accuracy: 0.6133\n",
      "Epoch 88/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0742 - accuracy: 1.0000 - val_loss: 4.5687 - val_accuracy: 0.6183\n",
      "Epoch 89/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0728 - accuracy: 1.0000 - val_loss: 4.6497 - val_accuracy: 0.6133\n",
      "Epoch 90/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0711 - accuracy: 1.0000 - val_loss: 4.6836 - val_accuracy: 0.6083\n",
      "Epoch 91/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0692 - accuracy: 1.0000 - val_loss: 4.6556 - val_accuracy: 0.6150\n",
      "Epoch 92/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0675 - accuracy: 1.0000 - val_loss: 4.6070 - val_accuracy: 0.6167\n",
      "Epoch 93/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0662 - accuracy: 1.0000 - val_loss: 4.6631 - val_accuracy: 0.6117\n",
      "Epoch 94/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0646 - accuracy: 1.0000 - val_loss: 4.6407 - val_accuracy: 0.6217\n",
      "Epoch 95/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 0.0631 - accuracy: 1.0000 - val_loss: 4.6681 - val_accuracy: 0.6117\n",
      "Epoch 96/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0618 - accuracy: 1.0000 - val_loss: 4.6462 - val_accuracy: 0.6183\n",
      "Epoch 97/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0605 - accuracy: 1.0000 - val_loss: 4.6590 - val_accuracy: 0.6117\n",
      "Epoch 98/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 0.0591 - accuracy: 1.0000 - val_loss: 4.6439 - val_accuracy: 0.6183\n",
      "Epoch 99/150\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 0.0579 - accuracy: 1.0000 - val_loss: 4.6372 - val_accuracy: 0.6167\n",
      "Epoch 100/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0566 - accuracy: 1.0000 - val_loss: 4.6610 - val_accuracy: 0.6200\n",
      "Epoch 101/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0553 - accuracy: 1.0000 - val_loss: 4.6626 - val_accuracy: 0.6167\n",
      "Epoch 102/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0540 - accuracy: 1.0000 - val_loss: 4.6555 - val_accuracy: 0.6183\n",
      "Epoch 103/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0531 - accuracy: 1.0000 - val_loss: 4.6534 - val_accuracy: 0.6183\n",
      "Epoch 104/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0519 - accuracy: 1.0000 - val_loss: 4.6707 - val_accuracy: 0.6183\n",
      "Epoch 105/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0509 - accuracy: 1.0000 - val_loss: 4.6602 - val_accuracy: 0.6183\n",
      "Epoch 106/150\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 0.0498 - accuracy: 1.0000 - val_loss: 4.6515 - val_accuracy: 0.6200\n",
      "Epoch 107/150\n",
      "8/8 [==============================] - 1s 127ms/step - loss: 0.0488 - accuracy: 1.0000 - val_loss: 4.6595 - val_accuracy: 0.6183\n",
      "Epoch 108/150\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 0.0478 - accuracy: 1.0000 - val_loss: 4.6573 - val_accuracy: 0.6183\n",
      "Epoch 109/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 0.0469 - accuracy: 1.0000 - val_loss: 4.6556 - val_accuracy: 0.6183\n",
      "Epoch 110/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 0.0459 - accuracy: 1.0000 - val_loss: 4.6646 - val_accuracy: 0.6200\n",
      "Epoch 111/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0450 - accuracy: 1.0000 - val_loss: 4.6638 - val_accuracy: 0.6167\n",
      "Epoch 112/150\n",
      "8/8 [==============================] - 1s 126ms/step - loss: 0.0442 - accuracy: 1.0000 - val_loss: 4.6378 - val_accuracy: 0.6217\n",
      "Epoch 113/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0433 - accuracy: 1.0000 - val_loss: 4.6695 - val_accuracy: 0.6200\n",
      "Epoch 114/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0426 - accuracy: 1.0000 - val_loss: 4.6587 - val_accuracy: 0.6217\n",
      "Epoch 115/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0417 - accuracy: 1.0000 - val_loss: 4.6732 - val_accuracy: 0.6217\n",
      "Epoch 116/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0409 - accuracy: 1.0000 - val_loss: 4.6725 - val_accuracy: 0.6200\n",
      "Epoch 117/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0402 - accuracy: 1.0000 - val_loss: 4.6683 - val_accuracy: 0.6183\n",
      "Epoch 118/150\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 0.0395 - accuracy: 1.0000 - val_loss: 4.6478 - val_accuracy: 0.6233\n",
      "Epoch 119/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0387 - accuracy: 1.0000 - val_loss: 4.6718 - val_accuracy: 0.6200\n",
      "Epoch 120/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0381 - accuracy: 1.0000 - val_loss: 4.6754 - val_accuracy: 0.6200\n",
      "Epoch 121/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0374 - accuracy: 1.0000 - val_loss: 4.6676 - val_accuracy: 0.6217\n",
      "Epoch 122/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0367 - accuracy: 1.0000 - val_loss: 4.6830 - val_accuracy: 0.6183\n",
      "Epoch 123/150\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 0.0360 - accuracy: 1.0000 - val_loss: 4.6700 - val_accuracy: 0.6233\n",
      "Epoch 124/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0353 - accuracy: 1.0000 - val_loss: 4.6659 - val_accuracy: 0.6217\n",
      "Epoch 125/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 0.0348 - accuracy: 1.0000 - val_loss: 4.6856 - val_accuracy: 0.6200\n",
      "Epoch 126/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0343 - accuracy: 1.0000 - val_loss: 4.6709 - val_accuracy: 0.6200\n",
      "Epoch 127/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0337 - accuracy: 1.0000 - val_loss: 4.6895 - val_accuracy: 0.6200\n",
      "Epoch 128/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0330 - accuracy: 1.0000 - val_loss: 4.6989 - val_accuracy: 0.6183\n",
      "Epoch 129/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0325 - accuracy: 1.0000 - val_loss: 4.6812 - val_accuracy: 0.6183\n",
      "Epoch 130/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0320 - accuracy: 1.0000 - val_loss: 4.6711 - val_accuracy: 0.6217\n",
      "Epoch 131/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0314 - accuracy: 1.0000 - val_loss: 4.6617 - val_accuracy: 0.6200\n",
      "Epoch 132/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0310 - accuracy: 1.0000 - val_loss: 4.6713 - val_accuracy: 0.6217\n",
      "Epoch 133/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0305 - accuracy: 1.0000 - val_loss: 4.6737 - val_accuracy: 0.6183\n",
      "Epoch 134/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0299 - accuracy: 1.0000 - val_loss: 4.6748 - val_accuracy: 0.6217\n",
      "Epoch 135/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0295 - accuracy: 1.0000 - val_loss: 4.6995 - val_accuracy: 0.6183\n",
      "Epoch 136/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0290 - accuracy: 1.0000 - val_loss: 4.6993 - val_accuracy: 0.6183\n",
      "Epoch 137/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0286 - accuracy: 1.0000 - val_loss: 4.6604 - val_accuracy: 0.6217\n",
      "Epoch 138/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0281 - accuracy: 1.0000 - val_loss: 4.6685 - val_accuracy: 0.6217\n",
      "Epoch 139/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0277 - accuracy: 1.0000 - val_loss: 4.6856 - val_accuracy: 0.6200\n",
      "Epoch 140/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0272 - accuracy: 1.0000 - val_loss: 4.7099 - val_accuracy: 0.6183\n",
      "Epoch 141/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0268 - accuracy: 1.0000 - val_loss: 4.7003 - val_accuracy: 0.6183\n",
      "Epoch 142/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0264 - accuracy: 1.0000 - val_loss: 4.6826 - val_accuracy: 0.6217\n",
      "Epoch 143/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0260 - accuracy: 1.0000 - val_loss: 4.6794 - val_accuracy: 0.6217\n",
      "Epoch 144/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 0.0256 - accuracy: 1.0000 - val_loss: 4.6823 - val_accuracy: 0.6217\n",
      "Epoch 145/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0253 - accuracy: 1.0000 - val_loss: 4.7110 - val_accuracy: 0.6167\n",
      "Epoch 146/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0249 - accuracy: 1.0000 - val_loss: 4.7063 - val_accuracy: 0.6200\n",
      "Epoch 147/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0245 - accuracy: 1.0000 - val_loss: 4.6815 - val_accuracy: 0.6217\n",
      "Epoch 148/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0242 - accuracy: 1.0000 - val_loss: 4.7002 - val_accuracy: 0.6200\n",
      "Epoch 149/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0239 - accuracy: 1.0000 - val_loss: 4.6872 - val_accuracy: 0.6200\n",
      "Epoch 150/150\n",
      "8/8 [==============================] - 1s 121ms/step - loss: 0.0235 - accuracy: 1.0000 - val_loss: 4.6671 - val_accuracy: 0.6250\n",
      "Epoch 1/150\n",
      "8/8 [==============================] - 1s 155ms/step - loss: 0.0453 - accuracy: 0.9979 - val_loss: 5.2318 - val_accuracy: 0.5717\n",
      "Epoch 2/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0427 - accuracy: 0.9979 - val_loss: 6.1307 - val_accuracy: 0.5467\n",
      "Epoch 3/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 0.0359 - accuracy: 0.9987 - val_loss: 5.1989 - val_accuracy: 0.5750\n",
      "Epoch 4/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 0.0341 - accuracy: 0.9979 - val_loss: 5.8318 - val_accuracy: 0.5467\n",
      "Epoch 5/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 0.0303 - accuracy: 0.9987 - val_loss: 5.2999 - val_accuracy: 0.5683\n",
      "Epoch 6/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0266 - accuracy: 0.9992 - val_loss: 5.8463 - val_accuracy: 0.5483\n",
      "Epoch 7/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0230 - accuracy: 1.0000 - val_loss: 5.4621 - val_accuracy: 0.5533\n",
      "Epoch 8/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0215 - accuracy: 1.0000 - val_loss: 5.7741 - val_accuracy: 0.5517\n",
      "Epoch 9/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0209 - accuracy: 1.0000 - val_loss: 5.6466 - val_accuracy: 0.5550\n",
      "Epoch 10/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0201 - accuracy: 1.0000 - val_loss: 5.6250 - val_accuracy: 0.5533\n",
      "Epoch 11/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0196 - accuracy: 1.0000 - val_loss: 5.7011 - val_accuracy: 0.5533\n",
      "Epoch 12/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0191 - accuracy: 1.0000 - val_loss: 5.6601 - val_accuracy: 0.5533\n",
      "Epoch 13/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0187 - accuracy: 1.0000 - val_loss: 5.7144 - val_accuracy: 0.5533\n",
      "Epoch 14/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0184 - accuracy: 1.0000 - val_loss: 5.6875 - val_accuracy: 0.5533\n",
      "Epoch 15/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0182 - accuracy: 1.0000 - val_loss: 5.6844 - val_accuracy: 0.5533\n",
      "Epoch 16/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0179 - accuracy: 1.0000 - val_loss: 5.7135 - val_accuracy: 0.5550\n",
      "Epoch 17/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0176 - accuracy: 1.0000 - val_loss: 5.6975 - val_accuracy: 0.5550\n",
      "Epoch 18/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 0.0173 - accuracy: 1.0000 - val_loss: 5.7188 - val_accuracy: 0.5533\n",
      "Epoch 19/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 0.0171 - accuracy: 1.0000 - val_loss: 5.7068 - val_accuracy: 0.5550\n",
      "Epoch 20/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0168 - accuracy: 1.0000 - val_loss: 5.7237 - val_accuracy: 0.5533\n",
      "Epoch 21/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 0.0166 - accuracy: 1.0000 - val_loss: 5.7303 - val_accuracy: 0.5533\n",
      "Epoch 22/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0164 - accuracy: 1.0000 - val_loss: 5.7210 - val_accuracy: 0.5550\n",
      "Epoch 23/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0162 - accuracy: 1.0000 - val_loss: 5.7316 - val_accuracy: 0.5567\n",
      "Epoch 24/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0160 - accuracy: 1.0000 - val_loss: 5.7293 - val_accuracy: 0.5567\n",
      "Epoch 25/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0158 - accuracy: 1.0000 - val_loss: 5.7397 - val_accuracy: 0.5567\n",
      "Epoch 26/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0156 - accuracy: 1.0000 - val_loss: 5.7362 - val_accuracy: 0.5567\n",
      "Epoch 27/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0154 - accuracy: 1.0000 - val_loss: 5.7423 - val_accuracy: 0.5567\n",
      "Epoch 28/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0152 - accuracy: 1.0000 - val_loss: 5.7411 - val_accuracy: 0.5567\n",
      "Epoch 29/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0150 - accuracy: 1.0000 - val_loss: 5.7437 - val_accuracy: 0.5567\n",
      "Epoch 30/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0148 - accuracy: 1.0000 - val_loss: 5.7500 - val_accuracy: 0.5550\n",
      "Epoch 31/150\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 0.0147 - accuracy: 1.0000 - val_loss: 5.7451 - val_accuracy: 0.5567\n",
      "Epoch 32/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0145 - accuracy: 1.0000 - val_loss: 5.7491 - val_accuracy: 0.5567\n",
      "Epoch 33/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 0.0143 - accuracy: 1.0000 - val_loss: 5.7505 - val_accuracy: 0.5567\n",
      "Epoch 34/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0141 - accuracy: 1.0000 - val_loss: 5.7548 - val_accuracy: 0.5550\n",
      "Epoch 35/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0140 - accuracy: 1.0000 - val_loss: 5.7541 - val_accuracy: 0.5550\n",
      "Epoch 36/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0138 - accuracy: 1.0000 - val_loss: 5.7567 - val_accuracy: 0.5550\n",
      "Epoch 37/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0137 - accuracy: 1.0000 - val_loss: 5.7618 - val_accuracy: 0.5567\n",
      "Epoch 38/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0135 - accuracy: 1.0000 - val_loss: 5.7538 - val_accuracy: 0.5550\n",
      "Epoch 39/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0134 - accuracy: 1.0000 - val_loss: 5.7588 - val_accuracy: 0.5550\n",
      "Epoch 40/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 5.7612 - val_accuracy: 0.5550\n",
      "Epoch 41/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0131 - accuracy: 1.0000 - val_loss: 5.7591 - val_accuracy: 0.5550\n",
      "Epoch 42/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0129 - accuracy: 1.0000 - val_loss: 5.7612 - val_accuracy: 0.5550\n",
      "Epoch 43/150\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 0.0128 - accuracy: 1.0000 - val_loss: 5.7658 - val_accuracy: 0.5550\n",
      "Epoch 44/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0127 - accuracy: 1.0000 - val_loss: 5.7666 - val_accuracy: 0.5550\n",
      "Epoch 45/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0125 - accuracy: 1.0000 - val_loss: 5.7686 - val_accuracy: 0.5550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0124 - accuracy: 1.0000 - val_loss: 5.7691 - val_accuracy: 0.5550\n",
      "Epoch 47/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 5.7655 - val_accuracy: 0.5550\n",
      "Epoch 48/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 5.7713 - val_accuracy: 0.5550\n",
      "Epoch 49/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 0.0120 - accuracy: 1.0000 - val_loss: 5.7700 - val_accuracy: 0.5550\n",
      "Epoch 50/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 5.7707 - val_accuracy: 0.5550\n",
      "Epoch 51/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0118 - accuracy: 1.0000 - val_loss: 5.7699 - val_accuracy: 0.5550\n",
      "Epoch 52/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 5.7695 - val_accuracy: 0.5567\n",
      "Epoch 53/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 5.7722 - val_accuracy: 0.5567\n",
      "Epoch 54/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 5.7722 - val_accuracy: 0.5567\n",
      "Epoch 55/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 5.7785 - val_accuracy: 0.5550\n",
      "Epoch 56/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 5.7754 - val_accuracy: 0.5567\n",
      "Epoch 57/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 5.7731 - val_accuracy: 0.5567\n",
      "Epoch 58/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 5.7761 - val_accuracy: 0.5550\n",
      "Epoch 59/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 5.7731 - val_accuracy: 0.5567\n",
      "Epoch 60/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 5.7781 - val_accuracy: 0.5567\n",
      "Epoch 61/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 5.7760 - val_accuracy: 0.5567\n",
      "Epoch 62/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 5.7761 - val_accuracy: 0.5567\n",
      "Epoch 63/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 5.7774 - val_accuracy: 0.5567\n",
      "Epoch 64/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 5.7788 - val_accuracy: 0.5567\n",
      "Epoch 65/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 5.7825 - val_accuracy: 0.5567\n",
      "Epoch 66/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 5.7754 - val_accuracy: 0.5567\n",
      "Epoch 67/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 5.7800 - val_accuracy: 0.5567\n",
      "Epoch 68/150\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 0.0100 - accuracy: 1.0000 - val_loss: 5.7810 - val_accuracy: 0.5567\n",
      "Epoch 69/150\n",
      "8/8 [==============================] - 1s 127ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 5.7851 - val_accuracy: 0.5567\n",
      "Epoch 70/150\n",
      "8/8 [==============================] - 1s 130ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 5.7821 - val_accuracy: 0.5567\n",
      "Epoch 71/150\n",
      "8/8 [==============================] - 1s 126ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 5.7771 - val_accuracy: 0.5567\n",
      "Epoch 72/150\n",
      "8/8 [==============================] - 1s 127ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 5.7837 - val_accuracy: 0.5567\n",
      "Epoch 73/150\n",
      "8/8 [==============================] - 1s 126ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 5.7819 - val_accuracy: 0.5567\n",
      "Epoch 74/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 5.7856 - val_accuracy: 0.5567\n",
      "Epoch 75/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 5.7843 - val_accuracy: 0.5567\n",
      "Epoch 76/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 5.7853 - val_accuracy: 0.5567\n",
      "Epoch 77/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 5.7832 - val_accuracy: 0.5567\n",
      "Epoch 78/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 5.7861 - val_accuracy: 0.5567\n",
      "Epoch 79/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 5.7836 - val_accuracy: 0.5567\n",
      "Epoch 80/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 5.7844 - val_accuracy: 0.5567\n",
      "Epoch 81/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 5.7846 - val_accuracy: 0.5567\n",
      "Epoch 82/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 5.7848 - val_accuracy: 0.5567\n",
      "Epoch 83/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 5.7839 - val_accuracy: 0.5567\n",
      "Epoch 84/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 5.7867 - val_accuracy: 0.5567\n",
      "Epoch 85/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 5.7872 - val_accuracy: 0.5583\n",
      "Epoch 86/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 5.7815 - val_accuracy: 0.5567\n",
      "Epoch 87/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 5.7933 - val_accuracy: 0.5583\n",
      "Epoch 88/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 5.7815 - val_accuracy: 0.5567\n",
      "Epoch 89/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 5.7844 - val_accuracy: 0.5583\n",
      "Epoch 90/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 5.7890 - val_accuracy: 0.5567\n",
      "Epoch 91/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 5.7863 - val_accuracy: 0.5567\n",
      "Epoch 92/150\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 5.7899 - val_accuracy: 0.5567\n",
      "Epoch 93/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 5.7834 - val_accuracy: 0.5567\n",
      "Epoch 94/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 5.7908 - val_accuracy: 0.5567\n",
      "Epoch 95/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 5.7899 - val_accuracy: 0.5567\n",
      "Epoch 96/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 5.7841 - val_accuracy: 0.5567\n",
      "Epoch 97/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 5.7909 - val_accuracy: 0.5583\n",
      "Epoch 98/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 5.7875 - val_accuracy: 0.5567\n",
      "Epoch 99/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 5.7873 - val_accuracy: 0.5567\n",
      "Epoch 100/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 5.7913 - val_accuracy: 0.5567\n",
      "Epoch 101/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 5.7877 - val_accuracy: 0.5567\n",
      "Epoch 102/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 5.7898 - val_accuracy: 0.5567\n",
      "Epoch 103/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 5.7916 - val_accuracy: 0.5567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 5.7874 - val_accuracy: 0.5583\n",
      "Epoch 105/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 5.7868 - val_accuracy: 0.5583\n",
      "Epoch 106/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 5.7901 - val_accuracy: 0.5600\n",
      "Epoch 107/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 5.7875 - val_accuracy: 0.5567\n",
      "Epoch 108/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 5.7890 - val_accuracy: 0.5583\n",
      "Epoch 109/150\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 5.7906 - val_accuracy: 0.5583\n",
      "Epoch 110/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 5.7922 - val_accuracy: 0.5583\n",
      "Epoch 111/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 5.7887 - val_accuracy: 0.5583\n",
      "Epoch 112/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 5.7942 - val_accuracy: 0.5600\n",
      "Epoch 113/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 5.7876 - val_accuracy: 0.5583\n",
      "Epoch 114/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 5.7906 - val_accuracy: 0.5583\n",
      "Epoch 115/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 5.7869 - val_accuracy: 0.5583\n",
      "Epoch 116/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 5.7925 - val_accuracy: 0.5583\n",
      "Epoch 117/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 5.7920 - val_accuracy: 0.5583\n",
      "Epoch 118/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 5.7916 - val_accuracy: 0.5583\n",
      "Epoch 119/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 5.7892 - val_accuracy: 0.5583\n",
      "Epoch 120/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 5.7908 - val_accuracy: 0.5583\n",
      "Epoch 121/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 5.7902 - val_accuracy: 0.5600\n",
      "Epoch 122/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 5.7911 - val_accuracy: 0.5600\n",
      "Epoch 123/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 5.7919 - val_accuracy: 0.5583\n",
      "Epoch 124/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 5.7895 - val_accuracy: 0.5583\n",
      "Epoch 125/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 5.7956 - val_accuracy: 0.5600\n",
      "Epoch 126/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 5.7914 - val_accuracy: 0.5600\n",
      "Epoch 127/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 5.7918 - val_accuracy: 0.5600\n",
      "Epoch 128/150\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 5.7899 - val_accuracy: 0.5583\n",
      "Epoch 129/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 5.7928 - val_accuracy: 0.5583\n",
      "Epoch 130/150\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 5.7931 - val_accuracy: 0.5600\n",
      "Epoch 131/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 5.7901 - val_accuracy: 0.5600\n",
      "Epoch 132/150\n",
      "8/8 [==============================] - 1s 122ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 5.7905 - val_accuracy: 0.5617\n",
      "Epoch 133/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 5.7907 - val_accuracy: 0.5617\n",
      "Epoch 134/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 5.7910 - val_accuracy: 0.5617\n",
      "Epoch 135/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 5.7976 - val_accuracy: 0.5633\n",
      "Epoch 136/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 5.7901 - val_accuracy: 0.5600\n",
      "Epoch 137/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 5.7954 - val_accuracy: 0.5633\n",
      "Epoch 138/150\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 5.7929 - val_accuracy: 0.5633\n",
      "Epoch 139/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 5.7905 - val_accuracy: 0.5633\n",
      "Epoch 140/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 5.7948 - val_accuracy: 0.5633\n",
      "Epoch 141/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 5.7891 - val_accuracy: 0.5633\n",
      "Epoch 142/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 5.7940 - val_accuracy: 0.5633\n",
      "Epoch 143/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 5.7948 - val_accuracy: 0.5633\n",
      "Epoch 144/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 5.7947 - val_accuracy: 0.5633\n",
      "Epoch 145/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 5.7927 - val_accuracy: 0.5633\n",
      "Epoch 146/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 5.7946 - val_accuracy: 0.5633\n",
      "Epoch 147/150\n",
      "8/8 [==============================] - 1s 123ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 5.7926 - val_accuracy: 0.5633\n",
      "Epoch 148/150\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 5.7950 - val_accuracy: 0.5633\n",
      "Epoch 149/150\n",
      "8/8 [==============================] - 1s 129ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 5.7882 - val_accuracy: 0.5633\n",
      "Epoch 150/150\n",
      "8/8 [==============================] - 1s 124ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 5.7943 - val_accuracy: 0.5633\n",
      "Epoch 1/150\n",
      "8/8 [==============================] - 1s 153ms/step - loss: 0.1803 - accuracy: 0.9862 - val_loss: 4.1958 - val_accuracy: 0.6450\n",
      "Epoch 2/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-11 17:51:29.213556: W tensorflow/core/common_runtime/bfc_allocator.cc:479] Allocator (GPU_0_bfc) ran out of memory trying to allocate 160.84MiB (rounded to 168655104)requested by op gradients/CudnnRNN_grad/CudnnRNNBackprop\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-11 17:51:29.213625: I tensorflow/core/common_runtime/bfc_allocator.cc:1033] BFCAllocator dump for GPU_0_bfc\n",
      "2023-07-11 17:51:29.213663: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (256): \tTotal Chunks: 41, Chunks in use: 41. 10.2KiB allocated for chunks. 10.2KiB in use in bin. 208B client-requested in use in bin.\n",
      "2023-07-11 17:51:29.213690: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-07-11 17:51:29.213717: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (1024): \tTotal Chunks: 3, Chunks in use: 2. 4.0KiB allocated for chunks. 2.5KiB in use in bin. 2.2KiB client-requested in use in bin.\n",
      "2023-07-11 17:51:29.213763: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (2048): \tTotal Chunks: 1, Chunks in use: 0. 2.5KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-07-11 17:51:29.213791: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (4096): \tTotal Chunks: 11, Chunks in use: 10. 45.0KiB allocated for chunks. 40.0KiB in use in bin. 40.0KiB client-requested in use in bin.\n",
      "2023-07-11 17:51:29.213817: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (8192): \tTotal Chunks: 9, Chunks in use: 8. 93.8KiB allocated for chunks. 82.5KiB in use in bin. 80.0KiB client-requested in use in bin.\n",
      "2023-07-11 17:51:29.213843: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (16384): \tTotal Chunks: 1, Chunks in use: 1. 17.2KiB allocated for chunks. 17.2KiB in use in bin. 10.0KiB client-requested in use in bin.\n",
      "2023-07-11 17:51:29.213869: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (32768): \tTotal Chunks: 1, Chunks in use: 1. 58.8KiB allocated for chunks. 58.8KiB in use in bin. 58.6KiB client-requested in use in bin.\n",
      "2023-07-11 17:51:29.213899: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (65536): \tTotal Chunks: 6, Chunks in use: 6. 703.5KiB allocated for chunks. 703.5KiB in use in bin. 703.1KiB client-requested in use in bin.\n",
      "2023-07-11 17:51:29.213925: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (131072): \tTotal Chunks: 2, Chunks in use: 2. 385.0KiB allocated for chunks. 385.0KiB in use in bin. 300.0KiB client-requested in use in bin.\n",
      "2023-07-11 17:51:29.213950: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (262144): \tTotal Chunks: 8, Chunks in use: 8. 2.95MiB allocated for chunks. 2.95MiB in use in bin. 2.80MiB client-requested in use in bin.\n",
      "2023-07-11 17:51:29.213975: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (524288): \tTotal Chunks: 2, Chunks in use: 2. 1.22MiB allocated for chunks. 1.22MiB in use in bin. 937.5KiB client-requested in use in bin.\n",
      "2023-07-11 17:51:29.214001: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (1048576): \tTotal Chunks: 9, Chunks in use: 9. 10.16MiB allocated for chunks. 10.16MiB in use in bin. 9.60MiB client-requested in use in bin.\n",
      "2023-07-11 17:51:29.214024: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-07-11 17:51:29.214046: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-07-11 17:51:29.214068: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-07-11 17:51:29.214095: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (16777216): \tTotal Chunks: 3, Chunks in use: 2. 85.84MiB allocated for chunks. 58.59MiB in use in bin. 58.59MiB client-requested in use in bin.\n",
      "2023-07-11 17:51:29.214123: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (33554432): \tTotal Chunks: 4, Chunks in use: 3. 159.59MiB allocated for chunks. 125.26MiB in use in bin. 97.96MiB client-requested in use in bin.\n",
      "2023-07-11 17:51:29.214150: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (67108864): \tTotal Chunks: 10, Chunks in use: 9. 902.07MiB allocated for chunks. 784.13MiB in use in bin. 764.56MiB client-requested in use in bin.\n",
      "2023-07-11 17:51:29.214186: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (134217728): \tTotal Chunks: 2, Chunks in use: 2. 323.28MiB allocated for chunks. 323.28MiB in use in bin. 235.38MiB client-requested in use in bin.\n",
      "2023-07-11 17:51:29.214212: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Bin (268435456): \tTotal Chunks: 6, Chunks in use: 6. 1.66GiB allocated for chunks. 1.66GiB in use in bin. 1.61GiB client-requested in use in bin.\n",
      "2023-07-11 17:51:29.214238: I tensorflow/core/common_runtime/bfc_allocator.cc:1056] Bin for 160.84MiB was 128.00MiB, Chunk State: \n",
      "2023-07-11 17:51:29.214258: I tensorflow/core/common_runtime/bfc_allocator.cc:1069] Next region of size 3336503296\n",
      "2023-07-11 17:51:29.214287: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be4000000 of size 1280 next 1\n",
      "2023-07-11 17:51:29.214309: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be4000500 of size 256 next 2\n",
      "2023-07-11 17:51:29.214329: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be4000600 of size 256 next 3\n",
      "2023-07-11 17:51:29.214349: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be4000700 of size 256 next 5\n",
      "2023-07-11 17:51:29.214369: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be4000800 of size 256 next 6\n",
      "2023-07-11 17:51:29.214388: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be4000900 of size 256 next 4\n",
      "2023-07-11 17:51:29.214408: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be4000a00 of size 256 next 13\n",
      "2023-07-11 17:51:29.214428: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be4000b00 of size 256 next 16\n",
      "2023-07-11 17:51:29.214448: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be4000c00 of size 256 next 22\n",
      "2023-07-11 17:51:29.214468: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be4000d00 of size 256 next 7\n",
      "2023-07-11 17:51:29.214487: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be4000e00 of size 256 next 12\n",
      "2023-07-11 17:51:29.214507: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be4000f00 of size 256 next 35\n",
      "2023-07-11 17:51:29.214527: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be4001000 of size 256 next 36\n",
      "2023-07-11 17:51:29.214546: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be4001100 of size 256 next 37\n",
      "2023-07-11 17:51:29.214566: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be4001200 of size 256 next 38\n",
      "2023-07-11 17:51:29.214586: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be4001300 of size 256 next 52\n",
      "2023-07-11 17:51:29.214605: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be4001400 of size 256 next 53\n",
      "2023-07-11 17:51:29.214625: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be4001500 of size 256 next 54\n",
      "2023-07-11 17:51:29.214645: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be4001600 of size 256 next 55\n",
      "2023-07-11 17:51:29.214665: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be4001700 of size 256 next 57\n",
      "2023-07-11 17:51:29.214685: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be4001800 of size 256 next 58\n",
      "2023-07-11 17:51:29.214705: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be4001900 of size 256 next 59\n",
      "2023-07-11 17:51:29.214724: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be4001a00 of size 256 next 14\n",
      "2023-07-11 17:51:29.214750: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be4001b00 of size 4096 next 15\n",
      "2023-07-11 17:51:29.214773: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be4002b00 of size 256 next 18\n",
      "2023-07-11 17:51:29.214793: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be4002c00 of size 256 next 24\n",
      "2023-07-11 17:51:29.214812: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be4002d00 of size 256 next 28\n",
      "2023-07-11 17:51:29.214831: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be4002e00 of size 256 next 29\n",
      "2023-07-11 17:51:29.214850: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be4002f00 of size 256 next 32\n",
      "2023-07-11 17:51:29.214869: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be4003000 of size 256 next 33\n",
      "2023-07-11 17:51:29.214888: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be4003100 of size 256 next 34\n",
      "2023-07-11 17:51:29.214907: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be4003200 of size 256 next 21\n",
      "2023-07-11 17:51:29.214927: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be4003300 of size 4096 next 23\n",
      "2023-07-11 17:51:29.214947: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be4004300 of size 120064 next 25\n",
      "2023-07-11 17:51:29.214967: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be4021800 of size 684544 next 8\n",
      "2023-07-11 17:51:29.214987: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be40c8a00 of size 409600 next 9\n",
      "2023-07-11 17:51:29.215008: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be412ca00 of size 409600 next 39\n",
      "2023-07-11 17:51:29.215028: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be4190a00 of size 4096 next 41\n",
      "2023-07-11 17:51:29.215047: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be4191a00 of size 4096 next 43\n",
      "2023-07-11 17:51:29.215066: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be4192a00 of size 120064 next 44\n",
      "2023-07-11 17:51:29.215085: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be41aff00 of size 510720 next 10\n",
      "2023-07-11 17:51:29.215105: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be422ca00 of size 1048576 next 11\n",
      "2023-07-11 17:51:29.215125: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be432ca00 of size 1048576 next 17\n",
      "2023-07-11 17:51:29.215144: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be442ca00 of size 1048576 next 40\n",
      "2023-07-11 17:51:29.215163: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be452ca00 of size 1048576 next 42\n",
      "2023-07-11 17:51:29.215183: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be462ca00 of size 59342848 next 27\n",
      "2023-07-11 17:51:29.215204: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be7ec4a00 of size 30720000 next 26\n",
      "2023-07-11 17:51:29.215224: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0be9c10a00 of size 152551424 next 20\n",
      "2023-07-11 17:51:29.215243: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0bf2d8ca00 of size 122880000 next 19\n",
      "2023-07-11 17:51:29.215263: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0bfa2bca00 of size 288000000 next 30\n",
      "2023-07-11 17:51:29.215283: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c0b565200 of size 288000000 next 31\n",
      "2023-07-11 17:51:29.215303: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c1c80da00 of size 1048576 next 45\n",
      "2023-07-11 17:51:29.215322: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c1c90da00 of size 4096 next 46\n",
      "2023-07-11 17:51:29.215341: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c1c90ea00 of size 122880000 next 47\n",
      "2023-07-11 17:51:29.215360: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c23e3ea00 of size 1048576 next 48\n",
      "2023-07-11 17:51:29.215379: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c23f3ea00 of size 4096 next 49\n",
      "2023-07-11 17:51:29.215398: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c23f3fa00 of size 30720000 next 50\n",
      "2023-07-11 17:51:29.215417: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c25c8ba00 of size 120064 next 51\n",
      "2023-07-11 17:51:29.215436: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c25ca8f00 of size 1280 next 56\n",
      "2023-07-11 17:51:29.215456: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c25ca9400 of size 72000512 next 63\n",
      "2023-07-11 17:51:29.215476: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c2a153800 of size 4096 next 82\n",
      "2023-07-11 17:51:29.215495: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c2a154800 of size 256 next 78\n",
      "2023-07-11 17:51:29.215514: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c2a154900 of size 256 next 110\n",
      "2023-07-11 17:51:29.215532: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c2a154a00 of size 256 next 95\n",
      "2023-07-11 17:51:29.215551: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c2a154b00 of size 256 next 76\n",
      "2023-07-11 17:51:29.215570: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c2a154c00 of size 256 next 93\n",
      "2023-07-11 17:51:29.215589: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] Free  at 7f0c2a154d00 of size 1536 next 103\n",
      "2023-07-11 17:51:29.215609: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c2a155300 of size 17664 next 91\n",
      "2023-07-11 17:51:29.215629: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c2a159800 of size 10240 next 62\n",
      "2023-07-11 17:51:29.215650: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c2a15c000 of size 256 next 74\n",
      "2023-07-11 17:51:29.215669: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c2a15c100 of size 256 next 87\n",
      "2023-07-11 17:51:29.215688: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c2a15c200 of size 256 next 69\n",
      "2023-07-11 17:51:29.215708: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c2a15c300 of size 4096 next 88\n",
      "2023-07-11 17:51:29.215727: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c2a15d300 of size 10240 next 98\n",
      "2023-07-11 17:51:29.215747: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c2a15fb00 of size 12800 next 65\n",
      "2023-07-11 17:51:29.215766: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c2a162d00 of size 10240 next 66\n",
      "2023-07-11 17:51:29.215785: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c2a165500 of size 4096 next 67\n",
      "2023-07-11 17:51:29.215804: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c2a166500 of size 256 next 97\n",
      "2023-07-11 17:51:29.215823: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c2a166600 of size 256 next 96\n",
      "2023-07-11 17:51:29.215843: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c2a166700 of size 120064 next 71\n",
      "2023-07-11 17:51:29.215862: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c2a183c00 of size 4096 next 101\n",
      "2023-07-11 17:51:29.215882: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c2a184c00 of size 600064 next 107\n",
      "2023-07-11 17:51:29.215901: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c2a217400 of size 480000 next 105\n",
      "2023-07-11 17:51:29.215923: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c2a28c700 of size 120064 next 114\n",
      "2023-07-11 17:51:29.215942: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c2a2a9c00 of size 120064 next 73\n",
      "2023-07-11 17:51:29.215961: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] Free  at 7f0c2a2c7100 of size 2560 next 108\n",
      "2023-07-11 17:51:29.215980: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c2a2c7b00 of size 10240 next 118\n",
      "2023-07-11 17:51:29.215999: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c2a2ca300 of size 10240 next 81\n",
      "2023-07-11 17:51:29.216018: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] Free  at 7f0c2a2ccb00 of size 5120 next 61\n",
      "2023-07-11 17:51:29.216037: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c2a2cdf00 of size 10240 next 75\n",
      "2023-07-11 17:51:29.216056: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c2a2d0700 of size 10240 next 94\n",
      "2023-07-11 17:51:29.216075: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] Free  at 7f0c2a2d2f00 of size 11520 next 77\n",
      "2023-07-11 17:51:29.216095: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c2a2d5c00 of size 195584 next 83\n",
      "2023-07-11 17:51:29.216116: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c2a305800 of size 198656 next 68\n",
      "2023-07-11 17:51:29.216135: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c2a336000 of size 72000000 next 60\n",
      "2023-07-11 17:51:29.216155: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c2e7e0200 of size 91238400 next 89\n",
      "2023-07-11 17:51:29.216175: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c33ee3200 of size 307200 next 90\n",
      "2023-07-11 17:51:29.216197: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c33f2e200 of size 72000000 next 86\n",
      "2023-07-11 17:51:29.216217: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c383d8400 of size 36000000 next 123\n",
      "2023-07-11 17:51:29.216236: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] Free  at 7f0c3a62d500 of size 36000000 next 122\n",
      "2023-07-11 17:51:29.216255: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c3c882600 of size 1355776 next 80\n",
      "2023-07-11 17:51:29.216275: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c3c9cd600 of size 60160 next 84\n",
      "2023-07-11 17:51:29.216295: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c3c9dc100 of size 364544 next 79\n",
      "2023-07-11 17:51:29.216315: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c3ca35100 of size 1466368 next 116\n",
      "2023-07-11 17:51:29.216334: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] Free  at 7f0c3cb9b100 of size 123666432 next 106\n",
      "2023-07-11 17:51:29.216354: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c4418b100 of size 36000000 next 72\n",
      "2023-07-11 17:51:29.216375: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c463e0200 of size 1536256 next 85\n",
      "2023-07-11 17:51:29.216396: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c46557300 of size 307200 next 102\n",
      "2023-07-11 17:51:29.216415: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c465a2300 of size 307200 next 117\n",
      "2023-07-11 17:51:29.216434: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] Free  at 7f0c465ed300 of size 28569344 next 99\n",
      "2023-07-11 17:51:29.216454: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c4812c200 of size 123936768 next 109\n",
      "2023-07-11 17:51:29.216473: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c4f75e200 of size 186429952 next 111\n",
      "2023-07-11 17:51:29.216495: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c5a929400 of size 288000000 next 113\n",
      "2023-07-11 17:51:29.216514: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c6bbd1c00 of size 288000000 next 104\n",
      "2023-07-11 17:51:29.216534: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c7ce7a400 of size 73285376 next 120\n",
      "2023-07-11 17:51:29.216554: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c8145e300 of size 256 next 92\n",
      "2023-07-11 17:51:29.216573: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c8145e400 of size 72000000 next 112\n",
      "2023-07-11 17:51:29.216592: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c85908600 of size 288000000 next 100\n",
      "2023-07-11 17:51:29.216612: I tensorflow/core/common_runtime/bfc_allocator.cc:1089] InUse at 7f0c96bb0e00 of size 337900032 next 18446744073709551615\n",
      "2023-07-11 17:51:29.216631: I tensorflow/core/common_runtime/bfc_allocator.cc:1094]      Summary of in-use Chunks by size: \n",
      "2023-07-11 17:51:29.216655: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 41 Chunks of size 256 totalling 10.2KiB\n",
      "2023-07-11 17:51:29.216678: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 2 Chunks of size 1280 totalling 2.5KiB\n",
      "2023-07-11 17:51:29.216713: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 10 Chunks of size 4096 totalling 40.0KiB\n",
      "2023-07-11 17:51:29.216735: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 7 Chunks of size 10240 totalling 70.0KiB\n",
      "2023-07-11 17:51:29.216756: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 12800 totalling 12.5KiB\n",
      "2023-07-11 17:51:29.216778: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 17664 totalling 17.2KiB\n",
      "2023-07-11 17:51:29.216799: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 60160 totalling 58.8KiB\n",
      "2023-07-11 17:51:29.216821: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 6 Chunks of size 120064 totalling 703.5KiB\n",
      "2023-07-11 17:51:29.216843: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 195584 totalling 191.0KiB\n",
      "2023-07-11 17:51:29.216864: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 198656 totalling 194.0KiB\n",
      "2023-07-11 17:51:29.216885: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 3 Chunks of size 307200 totalling 900.0KiB\n",
      "2023-07-11 17:51:29.216906: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 364544 totalling 356.0KiB\n",
      "2023-07-11 17:51:29.216928: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 2 Chunks of size 409600 totalling 800.0KiB\n",
      "2023-07-11 17:51:29.216949: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 480000 totalling 468.8KiB\n",
      "2023-07-11 17:51:29.216971: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 510720 totalling 498.8KiB\n",
      "2023-07-11 17:51:29.216992: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 600064 totalling 586.0KiB\n",
      "2023-07-11 17:51:29.217014: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 684544 totalling 668.5KiB\n",
      "2023-07-11 17:51:29.217034: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 6 Chunks of size 1048576 totalling 6.00MiB\n",
      "2023-07-11 17:51:29.217055: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 1355776 totalling 1.29MiB\n",
      "2023-07-11 17:51:29.217076: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 1466368 totalling 1.40MiB\n",
      "2023-07-11 17:51:29.217096: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 1536256 totalling 1.46MiB\n",
      "2023-07-11 17:51:29.217118: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 2 Chunks of size 30720000 totalling 58.59MiB\n",
      "2023-07-11 17:51:29.217140: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 2 Chunks of size 36000000 totalling 68.66MiB\n",
      "2023-07-11 17:51:29.217162: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 59342848 totalling 56.59MiB\n",
      "2023-07-11 17:51:29.217184: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 3 Chunks of size 72000000 totalling 205.99MiB\n",
      "2023-07-11 17:51:29.217206: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 72000512 totalling 68.67MiB\n",
      "2023-07-11 17:51:29.217228: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 73285376 totalling 69.89MiB\n",
      "2023-07-11 17:51:29.217249: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 91238400 totalling 87.01MiB\n",
      "2023-07-11 17:51:29.217271: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 2 Chunks of size 122880000 totalling 234.38MiB\n",
      "2023-07-11 17:51:29.217300: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 123936768 totalling 118.20MiB\n",
      "2023-07-11 17:51:29.217322: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 152551424 totalling 145.48MiB\n",
      "2023-07-11 17:51:29.217345: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 186429952 totalling 177.79MiB\n",
      "2023-07-11 17:51:29.217365: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 5 Chunks of size 288000000 totalling 1.34GiB\n",
      "2023-07-11 17:51:29.217387: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] 1 Chunks of size 337900032 totalling 322.25MiB\n",
      "2023-07-11 17:51:29.217407: I tensorflow/core/common_runtime/bfc_allocator.cc:1101] Sum Total of in-use chunks: 2.93GiB\n",
      "2023-07-11 17:51:29.217428: I tensorflow/core/common_runtime/bfc_allocator.cc:1103] total_region_allocated_bytes_: 3336503296 memory_limit_: 3336503296 available bytes: 0 curr_region_allocation_bytes_: 6673006592\n",
      "2023-07-11 17:51:29.217457: I tensorflow/core/common_runtime/bfc_allocator.cc:1109] Stats: \n",
      "Limit:                      3336503296\n",
      "InUse:                      3148246784\n",
      "MaxInUse:                   3184248064\n",
      "NumAllocs:                      550911\n",
      "MaxAllocSize:                337900032\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2023-07-11 17:51:29.217505: W tensorflow/core/common_runtime/bfc_allocator.cc:491] *********************************************___**********x****************************************x\n",
      "2023-07-11 17:51:29.217557: E tensorflow/stream_executor/dnn.cc:868] OOM when allocating tensor with shape[168655104] and type uint8 on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-11 17:51:29.217619: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at cudnn_rnn_ops.cc:1970 : INTERNAL: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 30000, 256, 1, 30, 10, 256] \n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Graph execution error:\n\nFailed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 30000, 256, 1, 30, 10, 256] \n\t [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\n\t [[Adam/gradients/PartitionedCall]] [Op:__inference_train_function_4700]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m outputs_data, target_data \u001b[38;5;241m=\u001b[39m get_batch_outputs(batch_record, data_batch_size)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#Training\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mtraining_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs_data\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m training_model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInternalError\u001b[0m: Graph execution error:\n\nFailed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 30000, 256, 1, 30, 10, 256] \n\t [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\n\t [[Adam/gradients/PartitionedCall]] [Op:__inference_train_function_4700]"
     ]
    }
   ],
   "source": [
    "# Run for the batches \n",
    "for batch_record in range(batch_records + 1):\n",
    "    \n",
    "    # Validate for the limit size of the dataset\n",
    "    # Limit the last step not to exceed the data size\n",
    "    if batch_record > (num_records / data_batch_size):\n",
    "        input_batch_size = math.floor((supported_batch_records % 1) * data_batch_size)\n",
    "    \n",
    "    # Get the inputs\n",
    "    inputs_data = get_batch_inputs(batch_record, data_batch_size)\n",
    "    \n",
    "    # Get the outputs\n",
    "    outputs_data, target_data = get_batch_outputs(batch_record, data_batch_size)\n",
    "    \n",
    "    #Training\n",
    "    training_model.fit([inputs_data, outputs_data], outputs_data, batch_size = batch_size, epochs = epochs, validation_split = 0.2)\n",
    "    training_model.save('training_model.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "chatbot.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
