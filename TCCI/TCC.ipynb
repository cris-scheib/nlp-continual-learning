{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86547416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 30 15:51:16 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 520.61.05    Driver Version: 520.61.05    CUDA Version: 11.8     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0  On |                  N/A |\n",
      "| 30%   30C    P8    N/A /  75W |    266MiB /  4096MiB |     24%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1556      G   /usr/lib/xorg/Xorg                 74MiB |\n",
      "|    0   N/A  N/A      1734      G   /usr/bin/gnome-shell              118MiB |\n",
      "|    0   N/A  N/A      3117      G   ...280675765272595680,131072       70MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa4b4a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-30 15:51:17.630976: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-30 15:51:18.451929: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-03-30 15:51:19.805743: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-30 15:51:19.806080: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-30 15:51:19.806114: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "from matplotlib import pylab\n",
    "import matplotlib\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib inline\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from sklearn.utils import shuffle\n",
    "import word2vec\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a4ff641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.python.util.deprecation as deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "checkpoint_directory = \"/tmp/training_checkpoints\"\n",
    "checkpoint_prefix = os.path.join(checkpoint_directory, \"ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a0dd06",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "* English vocabulary: [`vocab.50K.en`] created in the creating-vocabulary.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb7635e",
   "metadata": {},
   "source": [
    "### Loading the Datasets and Building the Vocabulary\n",
    "\n",
    "First, we build the vocabulary dictionaries for the source and target (English) language. \n",
    "The vocabularies are found in the file `vocab.50K.en`(English)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18b2e809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary: [('<unk>', 0), ('<s>', 1), ('</s>', 2), ('.', 3), ('the', 4), (',', 5), ('a', 6), ('to', 7), ('and', 8), ('i', 9)]\n",
      "Reverse dictionary: [(0, '<unk>'), (1, '<s>'), (2, '</s>'), (3, '.'), (4, 'the'), (5, ','), (6, 'a'), (7, 'to'), (8, 'and'), (9, 'i')]\n",
      "Vocabulary size:  50000\n"
     ]
    }
   ],
   "source": [
    "# Word string -> ID mapping\n",
    "dictionary = dict()\n",
    "\n",
    "vocabulary_size = len(dictionary)\n",
    "with open('data/vocab.50K.en', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        # disregard the new line aka `\\n`\n",
    "        dictionary[line[:-1]] = len(dictionary)\n",
    "        \n",
    "vocabulary_size = len(dictionary)\n",
    "reverse_dictionary = dict(zip(dictionary.values(),dictionary.keys()))\n",
    "\n",
    "print('Dictionary:', list(dictionary.items())[:10], end = '\\n')\n",
    "print('Reverse dictionary:', list(reverse_dictionary.items())[:10], end = '\\n')\n",
    "print('Vocabulary size: ', vocabulary_size, end = '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1623674",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "Here we load the data from the dataset.csv file (generated in the other script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58d59dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('data/dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ecca3e",
   "metadata": {},
   "source": [
    "### Data pre-processing\n",
    "Transform to lower, remove the new line and the punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c04fa895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowerDataset(data):\n",
    "    return data.str.lower() \n",
    "    \n",
    "def cleanDataset(data):\n",
    "    return data.str.replace('/r/','')                  \\\n",
    "                .str.replace(')','', regex=False)      \\\n",
    "                .str.replace('(','', regex=False)      \\\n",
    "                .str.replace(']','', regex=False)      \\\n",
    "                .str.replace('[','', regex=False)      \\\n",
    "                .str.replace('!','')                   \\\n",
    "                .str.replace('\"','')                   \\\n",
    "    \n",
    "def paddDataset(data):\n",
    "    return data.str.replace(',', ' ,')                 \\\n",
    "                .str.replace('.',' . ', regex=False)    \\\n",
    "                .str.replace('?',' ?', regex=False)    \\\n",
    "                .str.replace('\\n',' ')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42052bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wt = nltk.tokenize.WhitespaceTokenizer()\n",
    "for column in dataset.columns:    \n",
    "    dataset[column] = lowerDataset(dataset[column]) \n",
    "    dataset[column] = cleanDataset(dataset[column])\n",
    "    dataset[column] = paddDataset(dataset[column])                                    \n",
    "    dataset[column] = dataset[column].apply(wt.tokenize)\n",
    "dataset = shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6db800f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>884630</th>\n",
       "      <td>[is, canine, flatulence, a, problem, ?, what, ...</td>\n",
       "      <td>[i, don't, think, it's, the, kind, of, problem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753750</th>\n",
       "      <td>[the, opening, to, friends, spells, it, as, f,...</td>\n",
       "      <td>[next, time, on, kids, next, door, fiendish, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200415</th>\n",
       "      <td>[virtual, teachers, of, reddit, due, to, covid...</td>\n",
       "      <td>[i, had, a, kid, who, was, stuck, in, the, mid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203697</th>\n",
       "      <td>[girls, of, reddit, ,, what, is, your, ideal, ...</td>\n",
       "      <td>[i, goooooooooot, this, ., first, of, all, ,, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791426</th>\n",
       "      <td>[it's, 3:54, a, ., m, ., ,, your, tv, ,, radio...</td>\n",
       "      <td>[yellowstone, just, blew, up, ., ., ., i, woul...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 question  \\\n",
       "884630  [is, canine, flatulence, a, problem, ?, what, ...   \n",
       "753750  [the, opening, to, friends, spells, it, as, f,...   \n",
       "200415  [virtual, teachers, of, reddit, due, to, covid...   \n",
       "203697  [girls, of, reddit, ,, what, is, your, ideal, ...   \n",
       "791426  [it's, 3:54, a, ., m, ., ,, your, tv, ,, radio...   \n",
       "\n",
       "                                                   answer  \n",
       "884630  [i, don't, think, it's, the, kind, of, problem...  \n",
       "753750  [next, time, on, kids, next, door, fiendish, r...  \n",
       "200415  [i, had, a, kid, who, was, stuck, in, the, mid...  \n",
       "203697  [i, goooooooooot, this, ., first, of, all, ,, ...  \n",
       "791426  [yellowstone, just, blew, up, ., ., ., i, woul...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada21a8a",
   "metadata": {},
   "source": [
    "### Data analysis\n",
    "Mean sentence length and standard deviation of sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f68588d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Questions) Average sentence length:  17.116929628176226\n",
      "(Questions) Standard deviation of sentence length:  9.138660652379492\n",
      "(Answers) Average sentence length:  54.4734629738071\n",
      "(Answers) Standard deviation of sentence length:  844.5689123041486\n"
     ]
    }
   ],
   "source": [
    "print('(Questions) Average sentence length: ', dataset['question'].str.len().mean())\n",
    "print('(Questions) Standard deviation of sentence length: ', dataset['question'].str.len().std())\n",
    "\n",
    "print('(Answers) Average sentence length: ', dataset['answer'].str.len().mean())\n",
    "print('(Answers) Standard deviation of sentence length: ', dataset['answer'].str.len().std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbe4985",
   "metadata": {},
   "source": [
    "### Update the sentences to fixed length\n",
    "Update all sentences with a fixed size, to process the sentences as batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44f77928",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sent_length = {'question' : 30, 'answer': 70}\n",
    "\n",
    "def padding_sent(source):\n",
    "    padded = []\n",
    "    for tokens in dataset[source]: \n",
    "        # adding the start token\n",
    "        tokens.insert(0, '<s>')  \n",
    "\n",
    "        if len(tokens) >= max_sent_length[source]:\n",
    "            tokens = tokens[:(max_sent_length[source] - 1)]\n",
    "            tokens.append('</s>')\n",
    "\n",
    "        if len(tokens) < max_sent_length[source]:\n",
    "            tokens.extend(['</s>' for _ in range(max_sent_length[source] - len(tokens))])  \n",
    "\n",
    "        padded.append(tokens)\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cc61866",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = padding_sent('question')\n",
    "answers = padding_sent('answer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0924d3d3",
   "metadata": {},
   "source": [
    "### Create the reverse dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7ad51f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reverse_dataset(source):\n",
    "    reverse_tokens = []\n",
    "    reverse_dataset = []\n",
    "    for tokens in source: \n",
    "        for token in tokens: \n",
    "            if token not in dictionary.keys():\n",
    "                reverse_tokens.append(dictionary['<unk>'])\n",
    "            else:\n",
    "                reverse_tokens.append(dictionary[token])\n",
    "        reverse_dataset.append(reverse_tokens)\n",
    "        reverse_tokens = []\n",
    "    return reverse_dataset\n",
    "\n",
    "train_inputs =  np.array(create_reverse_dataset(questions), dtype=np.int32)\n",
    "train_outputs =  np.array(create_reverse_dataset(answers), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467bf5ec",
   "metadata": {},
   "source": [
    "### Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c1c2d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_cursors = [0 for _ in range(train_inputs.shape[0])]\n",
    "batch_size = 32\n",
    "embedding_size = 64\n",
    "steps = 80000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00e9857b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "with window_size = 2:\n",
      "    batch: [['<s>', \"what's\", 'like', 'being'], ['<s>', 'what', \"haven't\", 'you'], ['<s>', 'what', 'the', 'dumbest'], ['<s>', 'what', \"i'm\", 'upper'], ['<s>', 'time', 'brag', ','], ['<s>', \"you're\", 'into', 'custody'], ['<s>', 'what', 'your', 'favorite'], ['<s>', 'for', 'who', 'got']]\n",
      "    labels: ['it', 'movies', 'was', 'screams', 'to', 'taken', 'is', 'those']\n",
      "Defining 4 embedding lookups representing each word in the context\n",
      "Stacked embedding size: [32, 64, 4]\n",
      "Reduced mean embedding size: [32, 64]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-30 15:24:20.344039: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-30 15:24:20.527713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-30 15:24:20.952423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-30 15:24:20.953194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-30 15:24:22.308427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-30 15:24:22.311266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-30 15:24:22.311629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-30 15:24:22.314614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3190 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "2023-03-30 15:24:22.345394: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 2000: 3.030542\n",
      "Average loss at step 4000: 1.641926\n",
      "Average loss at step 6000: 1.438059\n",
      "Average loss at step 8000: 1.319586\n",
      "Average loss at step 10000: 1.260281\n",
      "Nearest to and: cognitively, literally, teenagers, tries, button, kielbasa, 4th, 2,\n",
      "Nearest to when: go, get, how, slack-jawed, rather, iodine, own, something,\n",
      "Nearest to .: essex, whip, dictatorship, loves, surfaces, esl, croatia, defunct,\n",
      "Nearest to get: when, go, tomorrow, want, <s>, how, uncles, think,\n",
      "Nearest to why: there, where, who, <s>, how, not, now, gone,\n",
      "Nearest to one: something, starting, where, an, everyone, <s>, this, barns,\n",
      "Nearest to most: worst, dumbest, strangest, fastest, scariest, stupidest, best, biggest,\n",
      "Nearest to back: elixir, roadshow, 2020, giant, now, over, still, ;3;42u1re,\n",
      "Nearest to day: non-electronic, jew, hobbled, oils, inconsequential, ,when, rekindle, allison,\n",
      "Nearest to .: essex, whip, dictatorship, loves, surfaces, esl, croatia, defunct,\n",
      "Nearest to slowly: mirror, conduit, abound, countries, enroll, intending, 2007, cons,\n",
      "Nearest to cost: carotid, want/have, amputation, flash, 3$, doctor's, bjj, tattooed,\n",
      "Nearest to meal: hobby, persistently, mismanaging, coordinator, scientist, amphitheater, impregnated, 737,\n",
      "Nearest to cost: carotid, want/have, amputation, flash, 3$, doctor's, bjj, tattooed,\n",
      "Nearest to cost: carotid, want/have, amputation, flash, 3$, doctor's, bjj, tattooed,\n",
      "Nearest to slowly: mirror, conduit, abound, countries, enroll, intending, 2007, cons,\n",
      "Nearest to rule: up/down, ratatouille, story, radioshack, sexist, reaspond, begun, bludgeoning,\n",
      "Nearest to travel: frosted, mandalorian, nola, apparantly, assuming, pretend, waiters, reccomended,\n",
      "Nearest to exist: anybody, uninterrupted, googly, giggity, sodomize, plain, hastings, megadeth,\n",
      "Nearest to science: loaned, mph, defenseless, non-smoking, spoonfuls, echoes, well-liked, bitchy,\n",
      "Average loss at step 12000: 1.209775\n",
      "Average loss at step 14000: 1.182763\n",
      "Average loss at step 16000: 1.170156\n",
      "Average loss at step 18000: 1.141299\n",
      "Average loss at step 20000: 1.127094\n",
      "Nearest to and: crack, 4th, doorstop, subject's, doggo, purpose, innocence, kielbasa,\n",
      "Nearest to when: slack-jawed, tuck, go, lose, how, read, wiesel, start,\n",
      "Nearest to .: s, j, european, whip, loves, dictatorship, o, novella,\n",
      "Nearest to get: want, go, learn, wish, find, die, believe, add,\n",
      "Nearest to why: where, there, obviously, not, negging, 15mph, how, pet,\n",
      "Nearest to one: something, sick, starting, grossest, luck, steve, barns, 1975,\n",
      "Nearest to most: fastest, dumbest, scariest, strangest, worst, stupidest, laziest, coolest,\n",
      "Nearest to back: obviously, still, giant, 2020, elixir, diseases, over, charge,\n",
      "Nearest to day: non-electronic, jew, inconsequential, entire, bend, ,when, gave, drunken,\n",
      "Nearest to .: s, j, european, whip, loves, dictatorship, o, novella,\n",
      "Nearest to slowly: conduit, countries, rape, 2007, mirror, rear, tasked, abound,\n",
      "Nearest to cost: carotid, want/have, amputation, flash, doctor's, seduce, tattooed, bjj,\n",
      "Nearest to meal: scientist, hobby, pimps, lamp, coordinator, ventilator, impregnated, phonetics,\n",
      "Nearest to cost: carotid, want/have, amputation, flash, doctor's, seduce, tattooed, bjj,\n",
      "Nearest to cost: carotid, want/have, amputation, flash, doctor's, seduce, tattooed, bjj,\n",
      "Nearest to slowly: conduit, countries, rape, 2007, mirror, rear, tasked, abound,\n",
      "Nearest to rule: story, up/down, heavens, stranger, circulatory, ring, ratatouille, fuse,\n",
      "Nearest to travel: pretend, frosted, assuming, nola, upvote, spend, know, mandalorian,\n",
      "Nearest to exist: anybody, media's, hastings, googly, uninterrupted, giggity, nee, bugged,\n",
      "Nearest to science: replace, burglar, grab, spoonfuls, remove, cat, rename, minor,\n",
      "Average loss at step 22000: 1.134156\n",
      "Average loss at step 24000: 1.132286\n",
      "Average loss at step 26000: 1.121677\n",
      "Average loss at step 28000: 1.129038\n",
      "Average loss at step 30000: 1.112216\n",
      "Nearest to and: crack, doggo, subject's, womb, doorstop, strategies/techniques, jackfruit, purpose,\n",
      "Nearest to when: tuck, slack-jawed, wiesel, dispose, sharpie, how, exhaustive, brock,\n",
      "Nearest to .: s, j, o, novella, european, whip, loves, gold,\n",
      "Nearest to get: learn, see, want, wish, pick, make, go, add,\n",
      "Nearest to why: negging, where, thrown, care, 15mph, storied, how, shockingly,\n",
      "Nearest to one: something, grossest, sick, 1975, 1, steve, mantra, proud,\n",
      "Nearest to most: scariest, fastest, dumbest, worst, laziest, stupidest, strangest, happiest,\n",
      "Nearest to back: obviously, charge, giant, teleported, elixir, over, overpopulation, diseases,\n",
      "Nearest to day: jew, non-electronic, tip, entire, inconsequential, mistake, insider, drunken,\n",
      "Nearest to .: s, j, o, novella, european, whip, loves, gold,\n",
      "Nearest to slowly: doing, tasked, meanest/rudest, rear, sitting, rape, 15mph, thanos',\n",
      "Nearest to cost: carotid, want/have, amputation, flash, tattooed, seduce, doctor's, after-school,\n",
      "Nearest to meal: lamp, scientist, hobby, hundred, doomsday, trip, pimps, beautiful,\n",
      "Nearest to cost: carotid, want/have, amputation, flash, tattooed, seduce, doctor's, after-school,\n",
      "Nearest to cost: carotid, want/have, amputation, flash, tattooed, seduce, doctor's, after-school,\n",
      "Nearest to slowly: doing, tasked, meanest/rudest, rear, sitting, rape, 15mph, thanos',\n",
      "Nearest to rule: story, stranger, up/down, heavens, stacking, begun, unsettling, circulatory,\n",
      "Nearest to travel: pretend, frosted, upvote, assuming, talk, nola, spend, give,\n",
      "Nearest to exist: anybody, media's, hastings, googly, bugged, nee, notoriously, since,\n",
      "Nearest to science: burglar, spoonfuls, replace, source, remove, sci-fi, place, cat,\n",
      "Average loss at step 32000: 1.127935\n",
      "Average loss at step 34000: 1.127772\n",
      "Average loss at step 36000: 1.130662\n",
      "Average loss at step 38000: 1.117846\n",
      "Average loss at step 40000: 1.143280\n",
      "Nearest to and: doggo, strategies/techniques, crack, womb, subject's, doorstop, douching, wiping,\n",
      "Nearest to when: slack-jawed, tuck, in/out, wiesel, sharpie, dispose, magnolia, how,\n",
      "Nearest to .: s, j, novella, o, v, unhealthier, whip, gold,\n",
      "Nearest to get: see, learn, hold, talk, want, change, wish, enjoy,\n",
      "Nearest to why: negging, pooh, care, imprisoning, shockingly, storied, mules, predict,\n",
      "Nearest to one: steve, 1975, 1, grossest, something, $1000, mantra, 'you're,\n",
      "Nearest to most: scariest, fastest, worst, biggest, dumbest, laziest, stupidest, coolest,\n",
      "Nearest to back: obviously, charge, giant, teleported, unseen, transported, upvote, frollo,\n",
      "Nearest to day: jew, non-electronic, insider, tip, playlist, mistake, lad, traveling,\n",
      "Nearest to .: s, j, novella, o, v, unhealthier, whip, gold,\n",
      "Nearest to slowly: doing, rear, meanest/rudest, afraid, stopping, loyal, tasked, 15mph,\n",
      "Nearest to cost: carotid, amputation, want/have, dusted, after-school, doctor's, seduce, flash,\n",
      "Nearest to meal: lamp, hundred, trip, scientist, hobby, doomsday, beautiful, man's,\n",
      "Nearest to cost: carotid, amputation, want/have, dusted, after-school, doctor's, seduce, flash,\n",
      "Nearest to cost: carotid, amputation, want/have, dusted, after-school, doctor's, seduce, flash,\n",
      "Nearest to slowly: doing, rear, meanest/rudest, afraid, stopping, loyal, tasked, 15mph,\n",
      "Nearest to rule: story, stranger, heavens, up/down, stacking, tip, fuse, circulatory,\n",
      "Nearest to travel: pretend, talk, upvote, frosted, decide, filter, nola, give,\n",
      "Nearest to exist: anybody, ing, nee, try, bugged, media's, dslr, hastings,\n",
      "Nearest to science: spoonfuls, source, defenseless, sizeable, place, generalize, burglar, sci-fi,\n",
      "Average loss at step 42000: 1.135002\n",
      "Average loss at step 44000: 1.136884\n",
      "Average loss at step 46000: 1.138116\n",
      "Average loss at step 48000: 1.150761\n",
      "Average loss at step 50000: 1.148743\n",
      "Nearest to and: strategies/techniques, doggo, womb, crack, doorstop, subject's, douching, at,\n",
      "Nearest to when: slack-jawed, in/out, tuck, sharpie, wiesel, stumble, how, neurological,\n",
      "Nearest to .: s, novella, v, unhealthier, o, j, predator, defunct,\n",
      "Nearest to get: see, learn, hold, wish, trust, talk, eat, miss,\n",
      "Nearest to why: care, negging, imprisoning, co-vid, pooh, shockingly, predict, storied,\n",
      "Nearest to one: 1, steve, grossest, 1975, $1000, sick, 'you're, mantra,\n",
      "Nearest to most: scariest, biggest, fastest, dumbest, laziest, worst, cheesiest, strangest,\n",
      "Nearest to back: giant, teleported, transported, unseen, charge, obviously, frollo, re-visit,\n",
      "Nearest to day: jew, non-electronic, mistake, insider, action, tip, hoopla, minute,\n",
      "Nearest to .: s, novella, v, unhealthier, o, j, predator, defunct,\n",
      "Nearest to slowly: doing, stopping, rear, meanest/rudest, afraid, currently, bitten, binge,\n",
      "Nearest to cost: carotid, dusted, want/have, doctor's, refusal, after-school, amputation, thinks,\n",
      "Nearest to meal: lamp, hundred, scientist, trip, man's, hobby, budget, sandwich,\n",
      "Nearest to cost: carotid, dusted, want/have, doctor's, refusal, after-school, amputation, thinks,\n",
      "Nearest to cost: carotid, dusted, want/have, doctor's, refusal, after-school, amputation, thinks,\n",
      "Nearest to slowly: doing, stopping, rear, meanest/rudest, afraid, currently, bitten, binge,\n",
      "Nearest to rule: story, stranger, stacking, circulatory, up/down, heavens, pick, fuse,\n",
      "Nearest to travel: upvote, pretend, talk, contrary, freeze, decide, observe, choose,\n",
      "Nearest to exist: anybody, try, ing, bugged, nee, waters, add, began,\n",
      "Nearest to science: source, sizeable, defenseless, mindfuck, wesson, diego, band-aids, francis,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 52000: 1.153049\n",
      "Average loss at step 54000: 1.155536\n",
      "Average loss at step 56000: 1.157846\n",
      "Average loss at step 58000: 1.156418\n",
      "Average loss at step 60000: 1.169359\n",
      "Nearest to and: strategies/techniques, doggo, womb, crack, doorstop, at, f****d, jackfruit,\n",
      "Nearest to when: in/out, slack-jawed, sharpie, blowing, how, while, neurological, recoup,\n",
      "Nearest to .: v, novella, s, o, unhealthier, predator, single-use, j,\n",
      "Nearest to get: see, hold, learn, stop, trust, ask, wish, eat,\n",
      "Nearest to why: imprisoning, ftl:, shockingly, mules, predict, co-vid, pooh, how,\n",
      "Nearest to one: 1, steve, 1975, grossest, $1000, sick, 'you're, playing,\n",
      "Nearest to most: biggest, scariest, cheesiest, chang, fastest, laziest, worst, dumbest,\n",
      "Nearest to back: tablet, transported, teleported, moo, giant, catered, re-visit, obviously,\n",
      "Nearest to day: jew, non-electronic, insider, action, hoopla, minute, mistake, skinning,\n",
      "Nearest to .: v, novella, s, o, unhealthier, predator, single-use, j,\n",
      "Nearest to slowly: doing, stopping, rear, currently, afraid, supposed, struggling, fed,\n",
      "Nearest to cost: carotid, dusted, after-school, generalize, vowel, refusal, doctor's, sucks,\n",
      "Nearest to meal: lamp, trip, hundred, man's, hobby, korean, sandwich, wrestler,\n",
      "Nearest to cost: carotid, dusted, after-school, generalize, vowel, refusal, doctor's, sucks,\n",
      "Nearest to cost: carotid, dusted, after-school, generalize, vowel, refusal, doctor's, sucks,\n",
      "Nearest to slowly: doing, stopping, rear, currently, afraid, supposed, struggling, fed,\n",
      "Nearest to rule: story, typo, stranger, stacking, circulatory, sub, smell/scent, heavens,\n",
      "Nearest to travel: upvote, pretend, talk, freeze, decide, observe, combine, communicate,\n",
      "Nearest to exist: anybody, try, bugged, ing, scotland, nee, waters, rattlesnake,\n",
      "Nearest to science: sizeable, mindfuck, armageddon, francis, band-aids, gearing, source, thyroid,\n",
      "Average loss at step 62000: 1.163813\n",
      "Average loss at step 64000: 1.171044\n",
      "Average loss at step 66000: 1.171138\n",
      "Average loss at step 68000: 1.183155\n",
      "Average loss at step 70000: 1.176886\n",
      "Nearest to and: strategies/techniques, womb, doorstop, doggo, crack, at, catherine, douching,\n",
      "Nearest to when: slack-jawed, in/out, sharpie, blowing, while, exhaustive, renting, neurological,\n",
      "Nearest to .: v, unhealthier, novella, predator, o, single-use, s, livid,\n",
      "Nearest to get: see, hold, stop, learn, trust, ask, kill, misinterpreted,\n",
      "Nearest to why: imprisoning, shockingly, ftl:, flattery, magicians, how, co-vid, mules,\n",
      "Nearest to one: 1, steve, grossest, 1975, 'you're, metaphorically, valkyrie, mantra,\n",
      "Nearest to most: biggest, scariest, cheesiest, chang, fastest, craziest, laziest, culture's,\n",
      "Nearest to back: tablet, moo, re-visit, transported, teleported, lads, catered, frollo,\n",
      "Nearest to day: jew, action, non-electronic, insider, minute, hoopla, hour, skinning,\n",
      "Nearest to .: v, unhealthier, novella, predator, o, single-use, s, livid,\n",
      "Nearest to slowly: doing, stopping, rear, currently, supposed, afraid, enjoying, struggling,\n",
      "Nearest to cost: dusted, carotid, after-school, doctor's, refusal, gash, vowel, flip,\n",
      "Nearest to meal: lamp, trip, hundred, man's, lantern, sandwich, hobby, chip,\n",
      "Nearest to cost: dusted, carotid, after-school, doctor's, refusal, gash, vowel, flip,\n",
      "Nearest to cost: dusted, carotid, after-school, doctor's, refusal, gash, vowel, flip,\n",
      "Nearest to slowly: doing, stopping, rear, currently, supposed, afraid, enjoying, struggling,\n",
      "Nearest to rule: story, typo, smell/scent, gesture, stacking, gadget, stranger, up/down,\n",
      "Nearest to travel: upvote, pretend, observe, freeze, decide, talk, prepare, filter,\n",
      "Nearest to exist: anybody, try, scotland, waters, ing, bugged, nee, add,\n",
      "Nearest to science: mindfuck, sizeable, source, gearing, thyroid, armageddon, out-dated, sci-fi,\n",
      "Average loss at step 72000: 1.180827\n",
      "Average loss at step 74000: 1.179968\n",
      "Average loss at step 76000: 1.193882\n",
      "Average loss at step 78000: 1.193577\n",
      "Average loss at step 80000: 1.198606\n",
      "Nearest to and: strategies/techniques, doorstop, womb, doggo, f****d, catherine, raj, crack,\n",
      "Nearest to when: slack-jawed, in/out, blowing, sharpie, renting, while, neurological, exhaustive,\n",
      "Nearest to .: v, unhealthier, novella, s, o, livid, predator, ,,\n",
      "Nearest to get: see, trust, hold, learn, ask, stop, stay, kill,\n",
      "Nearest to why: imprisoning, ftl:, shockingly, co-vid, believe, magicians, predict, flattery,\n",
      "Nearest to one: 1, steve, grossest, 1975, metaphorically, dialogue, 'you're, valkyrie,\n",
      "Nearest to most: biggest, scariest, cheesiest, chang, craziest, laziest, dumbest, culture's,\n",
      "Nearest to back: tablet, re-visit, frollo, lads, moo, transported, hurling, giant,\n",
      "Nearest to day: week, non-electronic, hoopla, skinning, minute, hour, time, jew,\n",
      "Nearest to .: v, unhealthier, novella, s, o, livid, predator, ,,\n",
      "Nearest to slowly: doing, currently, stopping, supposed, rear, enjoying, hoping, struggling,\n",
      "Nearest to cost: carotid, gash, dusted, after-school, recite, amputation, lewinsky, vowel,\n",
      "Nearest to meal: lamp, hundred, ball, trip, mushroom, lantern, capital, penny,\n",
      "Nearest to cost: carotid, gash, dusted, after-school, recite, amputation, lewinsky, vowel,\n",
      "Nearest to cost: carotid, gash, dusted, after-school, recite, amputation, lewinsky, vowel,\n",
      "Nearest to slowly: doing, currently, stopping, supposed, rear, enjoying, hoping, struggling,\n",
      "Nearest to rule: story, smell/scent, typo, stacking, up/down, secret, gesture, rumor,\n",
      "Nearest to travel: freeze, pretend, upvote, observe, prepare, decide, combine, fly,\n",
      "Nearest to exist: try, anybody, scotland, sporty, bugged, add, waters, nee,\n",
      "Nearest to science: mindfuck, sizeable, source, gearing, thyroid, agencies, armageddon, defenseless,\n"
     ]
    }
   ],
   "source": [
    "word2vec.define_data_and_hyperparameters(\n",
    "        train_inputs.shape[0], \n",
    "        max_sent_length['question'], \n",
    "        max_sent_length['answer'], \n",
    "        dictionary, \n",
    "        reverse_dictionary,  \n",
    "        train_inputs, \n",
    "        train_outputs, \n",
    "        embedding_size,\n",
    "        vocabulary_size)\n",
    "\n",
    "word2vec.print_some_batches()\n",
    "word2vec.define_word2vec_tensorflow(batch_size)\n",
    "word2vec.run_word2vec(batch_size, steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bc51f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data\n",
      "['is', 'the', 'virtual', 'girls', \"it's\"]\n",
      "['canine', 'opening', 'teachers', 'of', '3:54']\n",
      "['flatulence', 'to', 'of', '<unk>', 'a']\n",
      "['a', 'friends', '<unk>', ',', '.']\n",
      "['problem', 'spells', 'due', 'what', 'm']\n",
      "['?', 'it', 'to', 'is', '.']\n",
      "['what', 'as', 'covid-19', 'your', ',']\n",
      "['can', 'f', ',', 'ideal', 'your']\n",
      "['governments', '.', 'was', 'first', 'tv']\n",
      "['do', 'r', 'it', 'date', ',']\n",
      "['about', '.', 'shocking', '?', 'radio']\n",
      "['it', 'i', 'to', '</s>', ',']\n",
      "['?', '.', 'see', '</s>', 'cell']\n",
      "['</s>', 'e', 'how', '</s>', 'phone']\n",
      "['</s>', '.', 'some', '</s>', 'begins']\n",
      "['</s>', 'n', 'of', '</s>', 'transmitting']\n",
      "['</s>', '.', 'your', '</s>', 'an']\n",
      "['</s>', 'd', 'kids', '</s>', 'emergency']\n",
      "['</s>', '.', 'actually', '</s>', 'alert']\n",
      "['</s>', 's', 'live', '</s>', '.']\n",
      "\n",
      "Output data batch\n",
      "['i', 'next', 'i', 'i', 'yellowstone']\n",
      "[\"don't\", 'time', 'had', '<unk>', 'just']\n",
      "['think', 'on', 'a', 'this', 'blew']\n",
      "[\"it's\", 'kids', 'kid', '.', 'up']\n",
      "['the', 'next', 'who', 'first', '.']\n",
      "['kind', 'door', 'was', 'of', '.']\n",
      "['of', '<unk>', 'stuck', 'all', '.']\n",
      "['problems', 'robot', 'in', ',', 'i']\n",
      "['governments', '<unk>', 'the', 'find', 'would']\n",
      "['need', 'ends', 'middle', 'a', 'think']\n",
      "['to', 'november', 'of', 'location', 'to']\n",
      "['worry', 'december', 'a', 'that', 'myself']\n",
      "['about', 'seasons', 'custody', 'is', \"'you\"]\n",
      "['specifically', '</s>', 'thing', 'equal', 'have']\n",
      "['.', '</s>', '<unk>', '<unk>', 'approximately']\n",
      "['</s>', '</s>', '.', 'between', '12']\n",
      "['</s>', '</s>', 'he', 'your', 'hours']\n",
      "['</s>', '</s>', 'was', 'house', 'or']\n",
      "['</s>', '</s>', 'so', 'and', 'less']\n",
      "['</s>', '</s>', 'excited', 'hers', 'of']\n",
      "['</s>', '</s>', 'to', ',', 'fresh']\n",
      "['</s>', '</s>', 'show', 'and', '<unk>']\n",
      "['</s>', '</s>', 'us', 'maybe', 'safe']\n",
      "['</s>', '</s>', 'his', 'if', '<unk>']\n",
      "['</s>', '</s>', 'new', 'you', 'for']\n",
      "['</s>', '</s>', 'digs', 'feel', 'where']\n",
      "['</s>', '</s>', 'when', 'like', 'i']\n",
      "['</s>', '</s>', 'he', 'being', 'live']\n",
      "['</s>', '</s>', 'moves', 'a', 'in']\n",
      "['</s>', '</s>', 'in', 'gentleman', 'the']\n"
     ]
    }
   ],
   "source": [
    "class DataGenerator(object):\n",
    "\n",
    "    def __init__(self, batch_size, num_unroll, is_input, is_train):\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unroll = num_unroll\n",
    "        self._cursor = [0 for offset in range(self._batch_size)]\n",
    "        self._word_embeddings = np.load('embeddings.npy')\n",
    "        self._sent_ids = None\n",
    "        self._is_input = is_input\n",
    "        self._is_train = is_train\n",
    "\n",
    "    def next_batch(self, sent_ids):\n",
    "\n",
    "        sent_length = max_sent_length['question'] if self._is_input else max_sent_length['answer']\n",
    "\n",
    "        batch_data = np.zeros((self._batch_size, embedding_size), dtype=np.float32)\n",
    "        batch_labels = np.zeros((self._batch_size, vocabulary_size), dtype=np.float32)\n",
    "\n",
    "        for batch in range(self._batch_size):\n",
    "            sent_id = sent_ids[batch]\n",
    "            \n",
    "            if self._is_input:\n",
    "                sent_text = train_inputs[sent_id] if self._is_input else test_inputs[sent_id]\n",
    "            else:\n",
    "                sent_text = train_outputs[sent_id] if self._is_input else train_outputs[sent_id]\n",
    "            \n",
    "            batch_data[batch] = self._word_embeddings[sent_text[self._cursor[batch]],:]\n",
    "            batch_labels[batch] = np.zeros((vocabulary_size), dtype=np.float32)\n",
    "            batch_labels[batch, sent_text[self._cursor[batch] + 1]] = 1.0\n",
    "\n",
    "            self._cursor[batch] = (self._cursor[batch] + 1) % (sent_length - 1)\n",
    "\n",
    "        return batch_data,batch_labels\n",
    "\n",
    "    def unroll_batches(self,sent_ids):\n",
    "\n",
    "        if sent_ids is not None:\n",
    "            self._sent_ids = sent_ids\n",
    "            self._cursor = [0 for _ in range(self._batch_size)]\n",
    "        unroll_data, unroll_labels = [],[]\n",
    "\n",
    "        for unroll_ids in range(self._num_unroll):\n",
    "            data, labels = self.next_batch(self._sent_ids)\n",
    "            unroll_data.append(data)\n",
    "            unroll_labels.append(labels)\n",
    "        return unroll_data, unroll_labels, self._sent_ids\n",
    "\n",
    "    def reset_indices(self):\n",
    "        self._cursor = [0 for offset in range(self._batch_size)]\n",
    "\n",
    "dg = DataGenerator(batch_size=5, num_unroll=20, is_input=True, is_train=True)\n",
    "u_data, u_labels, _ = dg.unroll_batches([0,1,2,3,4])\n",
    "\n",
    "print('Input data')\n",
    "for _, lbl in zip(u_data,u_labels):\n",
    "    print([reverse_dictionary[w] for w in np.argmax(lbl,axis=1).tolist()])\n",
    "\n",
    "dg = DataGenerator(batch_size=5, num_unroll=30, is_input=False, is_train=True)\n",
    "u_data, u_labels, _ = dg.unroll_batches([0,1,2,3,4])\n",
    "\n",
    "print('\\nOutput data batch')\n",
    "for d_i,(_, lbl) in enumerate(zip(u_data,u_labels)):\n",
    "    print([reverse_dictionary[w] for w in np.argmax(lbl,axis=1).tolist()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d461925",
   "metadata": {},
   "source": [
    "## Building the Model with TensorFlow\n",
    "\n",
    "Define the hyperparameters, the input/output placeholders, the LSTM/Output layer parameters, the LSTM/output calculations, and finally the optimization steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cca169",
   "metadata": {},
   "source": [
    "### Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b60e521e",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_mat = np.load('embeddings.npy')\n",
    "input_size = emb_mat.shape[1]\n",
    "\n",
    "num_nodes = 128\n",
    "batch_size = 10\n",
    "\n",
    "encoder_num_unrollings = 20\n",
    "decoder_num_unrollings = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdf386c",
   "metadata": {},
   "source": [
    "### Input / Output Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15751651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining Encoder Data Placeholders\n",
      "Defining Decoder Data Placeholders\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "word_embeddings = tf.convert_to_tensor(value=emb_mat,name='embeddings')\n",
    "\n",
    "print('Defining Encoder Data Placeholders')\n",
    "encoder_train_inputs = []\n",
    "\n",
    "for ui in range(encoder_num_unrollings):\n",
    "    encoder_train_inputs.append(tf.compat.v1.placeholder(tf.float32, shape=[batch_size,input_size],name='train_inputs_%d'%ui))\n",
    "\n",
    "print('Defining Decoder Data Placeholders')\n",
    "\n",
    "decoder_train_inputs, decoder_train_labels, decoder_train_masks = [],[],[]\n",
    "\n",
    "for ui in range(decoder_num_unrollings):\n",
    "    decoder_train_inputs.append(tf.compat.v1.placeholder(tf.float32, shape=[batch_size,input_size],name='decoder_train_inputs_%d'%ui))\n",
    "    decoder_train_labels.append(tf.compat.v1.placeholder(tf.float32, shape=[batch_size,vocabulary_size], name = 'decoder_train_labels_%d'%ui))\n",
    "    decoder_train_masks.append(tf.compat.v1.placeholder(tf.float32, shape=[batch_size,1],name='decoder_train_masks_%d'%ui))\n",
    "\n",
    "\n",
    "encoder_test_input = [tf.compat.v1.placeholder(tf.float32, shape=[batch_size,input_size], name='test_input_%d'%ui) for ui in range(encoder_num_unrollings)]\n",
    "decoder_test_input = tf.nn.embedding_lookup(params=word_embeddings,ids=[dictionary['<s>']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52470c6",
   "metadata": {},
   "source": [
    "### Defining the Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a10af22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Model defined\n"
     ]
    }
   ],
   "source": [
    "with tf.compat.v1.variable_scope('Encoder'):\n",
    "\n",
    "    # Input gate\n",
    "    encoder_input_gate_x = tf.compat.v1.get_variable('input_gate_x', shape=[input_size, num_nodes], initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "    encoder_input_gate_m = tf.compat.v1.get_variable('input_gate_m', shape=[num_nodes, num_nodes], initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "    encoder_input_gate_b = tf.Variable(tf.random.uniform([1, num_nodes],-0.05, 0.05), name='input_gate_b')\n",
    "\n",
    "    # Forget gate\n",
    "    encoder_forget_gate_x = tf.compat.v1.get_variable('forget_gate_x', shape=[input_size, num_nodes], initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "    encoder_forget_gate_m = tf.compat.v1.get_variable('forget_gate_m', shape=[num_nodes, num_nodes], initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "    encoder_forget_gate_b = tf.Variable(tf.random.uniform([1, num_nodes],-0.05, 0.05), name='forget_gate_b')\n",
    "\n",
    "    # Candidate value (c~_t)\n",
    "    encoder_candidate_value_x = tf.compat.v1.get_variable('candidate_value_x', shape=[input_size, num_nodes], initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "    encoder_candidate_value_m = tf.compat.v1.get_variable('candidate_value_m', shape=[num_nodes, num_nodes], initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "    encoder_candidate_value_b = tf.Variable(tf.random.uniform([1, num_nodes],-0.05,0.05), name='candidate_value_b')\n",
    "\n",
    "    # Output gate\n",
    "    encoder_output_gate_x = tf.compat.v1.get_variable('output_gate_x', shape=[input_size, num_nodes], initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "    encoder_output_gate_m = tf.compat.v1.get_variable('output_gate_m', shape=[num_nodes, num_nodes], initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "    encoder_output_gate_b = tf.Variable(tf.random.uniform([1, num_nodes],-0.05,0.05), name='output_gate_b')\n",
    "\n",
    "    # Variáveis para salvar o resultado\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False, name='train_output')\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False, name = 'train_cell')\n",
    "\n",
    "    saved_test_output = tf.Variable(tf.zeros([batch_size, num_nodes]),trainable=False, name='test_output')\n",
    "    saved_test_state = tf.Variable(tf.zeros([batch_size, num_nodes]),trainable=False, name='test_cell')\n",
    "\n",
    "print('Encoder Model defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09954b00",
   "metadata": {},
   "source": [
    "### Defining the Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed93b5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder Model defined\n"
     ]
    }
   ],
   "source": [
    "with tf.compat.v1.variable_scope('Decoder'):\n",
    "\n",
    "    # Input gate\n",
    "    decoder_input_gate_x = tf.compat.v1.get_variable('input_gate_x',shape=[input_size, num_nodes], initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "    decoder_input_gate_m = tf.compat.v1.get_variable('input_gate_m',shape=[num_nodes, num_nodes], initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "    decoder_input_gate_b = tf.Variable(tf.random.uniform([1, num_nodes],-0.05, 0.05), name='input_gate_b')\n",
    "\n",
    "    # Forget gate\n",
    "    decoder_forget_gate_x = tf.compat.v1.get_variable('forget_gate_x', shape=[input_size, num_nodes], initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "    decoder_forget_gate_m = tf.compat.v1.get_variable('forget_gate_m', shape=[num_nodes, num_nodes], initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "    decoder_forget_gate_b = tf.Variable(tf.random.uniform([1, num_nodes],-0.05, 0.05), name='forget_gate_b')\n",
    "\n",
    "    # Candidate value (c~_t)\n",
    "    decoder_candidate_value_x = tf.compat.v1.get_variable('candidate_value_x', shape=[input_size, num_nodes], initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "    decoder_candidate_value_m = tf.compat.v1.get_variable('candidate_value_m', shape=[num_nodes, num_nodes], initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "    decoder_candidate_value_b = tf.Variable(tf.random.uniform([1, num_nodes],-0.05,0.05), name='candidate_value_b')\n",
    "\n",
    "    # Output gate\n",
    "    decoder_output_gate_x = tf.compat.v1.get_variable('output_gate_x',shape=[input_size, num_nodes], initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "    decoder_output_gate_m = tf.compat.v1.get_variable('output_gate_m',shape=[num_nodes, num_nodes], initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "    decoder_output_gate_b = tf.Variable(tf.random.uniform([1, num_nodes],-0.05,0.05),name='output_gate_b')\n",
    "\n",
    "    # Softmax Classifier\n",
    "    w = tf.compat.v1.get_variable('softmax_weights',shape=[num_nodes, vocabulary_size], initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "    b = tf.Variable(tf.random.uniform([vocabulary_size],-0.05,-0.05),name='softmax_bias')\n",
    "    \n",
    "print('Decoder Model defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272d1241",
   "metadata": {},
   "source": [
    "### Defining LSTM cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81ab79a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder LSTM cell\n",
    "def encoder_lstm_cell(_input, _output, _state):\n",
    "    input_gate = tf.sigmoid(tf.matmul(_input, encoder_input_gate_x) + tf.matmul(_output, encoder_input_gate_m) + encoder_input_gate_b)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(_input, encoder_forget_gate_x) + tf.matmul(_output, encoder_forget_gate_m) + encoder_forget_gate_b)\n",
    "    update = tf.matmul(_input, encoder_candidate_value_x) + tf.matmul(_output, encoder_candidate_value_m) + encoder_candidate_value_b\n",
    "    _state = forget_gate * _state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(_input, encoder_output_gate_x) + tf.matmul(_output, encoder_output_gate_m) + encoder_output_gate_b)\n",
    "    return output_gate * tf.tanh(_state), _state\n",
    "\n",
    "# Decoder LSTM cell\n",
    "def decoder_lstm_cell(_input, _output, _state):\n",
    "    input_gate = tf.sigmoid(tf.matmul(_input, decoder_input_gate_x) + tf.matmul(_output, decoder_input_gate_m) + decoder_input_gate_b)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(_input, decoder_forget_gate_x) + tf.matmul(_output, decoder_forget_gate_m) + decoder_forget_gate_b)\n",
    "    update = tf.matmul(_input, decoder_candidate_value_x) + tf.matmul(_output, decoder_candidate_value_m) + decoder_candidate_value_b\n",
    "    _state = forget_gate * _state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(_input, decoder_output_gate_x) + tf.matmul(_output, decoder_output_gate_m) + decoder_output_gate_b)\n",
    "    return output_gate * tf.tanh(_state), _state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0792a93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#=========================== TRAIN =================================\n",
    "\n",
    "outputs = list()\n",
    "output = saved_output\n",
    "state = saved_state\n",
    "\n",
    "# Calculate the output and state of the encoder\n",
    "for _input in encoder_train_inputs:\n",
    "    output, state = encoder_lstm_cell(_input, output, state)\n",
    "\n",
    "# Calculate the output and state of the decoder\n",
    "with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "    for _input in decoder_train_inputs:\n",
    "        output, state = decoder_lstm_cell(_input, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "# Calculate the decoder logits for all unrolled steps\n",
    "logits = tf.matmul(tf.concat(axis=0, values=outputs), w) + b\n",
    "\n",
    "# Decoder predictions\n",
    "train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "\n",
    "#=========================== TEST =================================\n",
    "\n",
    "test_output  = saved_test_output\n",
    "test_state = saved_test_state\n",
    "test_predictions = []\n",
    "\n",
    "for _input in encoder_test_input:\n",
    "    test_output, test_state = encoder_lstm_cell(_input, test_output,test_state)\n",
    "\n",
    "# Calculate the decoder output\n",
    "with tf.control_dependencies([saved_test_output.assign(test_output), saved_test_state.assign(test_state)]):\n",
    "    for i in range(decoder_num_unrollings):\n",
    "\n",
    "        test_output, test_state = decoder_lstm_cell(decoder_test_input, test_output, test_state)\n",
    "        test_prediction = tf.nn.softmax(tf.compat.v1.nn.xw_plus_b(test_output, w, b))\n",
    "        decoder_test_input = tf.nn.embedding_lookup(params=word_embeddings,ids=tf.argmax(input=test_prediction,axis=1))\n",
    "        test_predictions.append(tf.argmax(input=test_prediction,axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da1f597",
   "metadata": {},
   "source": [
    "### Calculating the Loss\n",
    "\n",
    "The loss is calculated by summing all losses obtained along the time axis and the average of the lot axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b56d6329",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_batch = tf.concat(axis=0,values=decoder_train_masks) * tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf.concat(axis=0, values=decoder_train_labels))\n",
    "loss = tf.reduce_mean(input_tensor=loss_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00034e96",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "There are two optimizers used here: Adam and SGD. \n",
    "Using Adam just causes the model to exhibit some undesirable behavior in the long run. \n",
    "So Adam is used to get a good initial guess for the SGD and use the SGD from that point on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d16822bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are used to slow down the learning rate over time\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "inc_gstep = tf.compat.v1.assign(global_step,global_step + 1)\n",
    "\n",
    "# Using two optimizers, when optimizer changes we reset global step\n",
    "reset_gstep = tf.compat.v1.assign(global_step,0)\n",
    "\n",
    "# Calculated decaying learning rate\n",
    "learning_rate = tf.maximum(\n",
    "    tf.compat.v1.train.exponential_decay(0.005, global_step, decay_steps=1, decay_rate=0.95, staircase=True), 0.00001)\n",
    "\n",
    "sgd_learning_rate = tf.maximum(\n",
    "    tf.compat.v1.train.exponential_decay(0.005, global_step, decay_steps=1, decay_rate=0.95, staircase=True), 0.00001)\n",
    "\n",
    "with tf.compat.v1.variable_scope('Adam'):\n",
    "    optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate)\n",
    "with tf.compat.v1.variable_scope('SGD'):\n",
    "    sgd_optimizer = tf.compat.v1.train.GradientDescentOptimizer(sgd_learning_rate)\n",
    "\n",
    "gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "optimize = optimizer.apply_gradients(zip(gradients, v))\n",
    "\n",
    "sgd_gradients, v = zip(*sgd_optimizer.compute_gradients(loss))\n",
    "sgd_gradients, _ = tf.clip_by_global_norm(sgd_gradients, 5.0)\n",
    "sgd_optimize = optimizer.apply_gradients(zip(sgd_gradients, v))\n",
    "\n",
    "# Making sure there are fluid gradients from decoder to encoder\n",
    "for (g_i,v_i) in zip(gradients,v):\n",
    "    assert g_i is not None, 'Gradient none for %s'%(v_i.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acb775f",
   "metadata": {},
   "source": [
    "### Resetting the Training and Testing States\n",
    "\n",
    "Define the state reset functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dda6d04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset training state\n",
    "reset_train_state = tf.group(tf.compat.v1.assign(saved_output, tf.zeros([batch_size, num_nodes])),\n",
    "                             tf.compat.v1.assign(saved_state, tf.zeros([batch_size, num_nodes])))\n",
    "\n",
    "reset_test_state = tf.group(\n",
    "    saved_test_output.assign(tf.zeros([batch_size, num_nodes])),\n",
    "    saved_test_state.assign(tf.zeros([batch_size, num_nodes])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480476d4",
   "metadata": {},
   "source": [
    "## Running the Neural Network\n",
    "\n",
    "With all the TensorFlow operations defined, now to define various functions related to running the model, as well as running the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd0e86e",
   "metadata": {},
   "source": [
    "### Evaluate and Print Results\n",
    "\n",
    "it is defined two functions to print and save the prediction results for training data as well as test data, and finally, define a function to get candidate and reference data to calculate the BLEU Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "919c629c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_and_save_train_predictions(decoder_unrolled_labels, pred_unrolled, random_index, train_prediction_text_fname):\n",
    "\n",
    "    print_str = 'Actual: ' \n",
    "    for w in np.argmax(np.concatenate(decoder_unrolled_labels,axis=0)[random_index::batch_size],axis=1).tolist():\n",
    "        print_str += reverse_dictionary[w] + ' '\n",
    "        if reverse_dictionary[w] == '</s>':\n",
    "            break\n",
    "    print(print_str)\n",
    "    with open(os.path.join(log_dir, train_prediction_text_fname),'a',encoding='utf-8') as fa:                \n",
    "        fa.write(print_str)  \n",
    "\n",
    "    print()\n",
    "    print_str = 'Predicted: '\n",
    "    for w in np.argmax(pred_unrolled[random_index::batch_size],axis=1).tolist():\n",
    "        print_str += reverse_dictionary[w] + ' '\n",
    "        if reverse_dictionary[w] == '</s>':\n",
    "            break\n",
    "    print(print_str)\n",
    "    with open(os.path.join(log_dir, train_prediction_text_fname),'a',encoding='utf-8') as fa:                \n",
    "        fa.write(print_str+'\\n')\n",
    "\n",
    "\n",
    "def create_bleu_ref_candidate_lists(all_preds, all_labels):\n",
    "\n",
    "    bleu_labels, bleu_preds = [],[]\n",
    "    ref_list, cand_list = [],[]\n",
    "    for b_i in range(batch_size):\n",
    "        tmp_lbl = all_labels[b_i::batch_size]\n",
    "        tmp_lbl = tmp_lbl[np.where(tmp_lbl != dictionary['</s>'])]\n",
    "        ref_str = ' '.join([reverse_dictionary[lbl] for lbl in tmp_lbl])\n",
    "        ref_list.append([ref_str])\n",
    "        tmp_pred = all_preds[b_i::batch_size]\n",
    "        tmp_pred = tmp_pred[np.where(tmp_pred != dictionary['</s>'])]\n",
    "        cand_str = ' '.join([reverse_dictionary[pre] for pre in tmp_pred])\n",
    "        cand_list.append(cand_str)\n",
    "\n",
    "    return cand_list, ref_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b960071b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_step(unrolled_encoder_data, unrolled_decoder_data, unrolled_decoder_labels):\n",
    "\n",
    "    feed_dict = {}\n",
    "    for ui, dat in enumerate(unrolled_encoder_data):\n",
    "        feed_dict[encoder_train_inputs[ui]] = dat\n",
    "\n",
    "    for ui,(dat,lbl) in enumerate(zip(unrolled_decoder_data,unrolled_decoder_labels)):\n",
    "        feed_dict[decoder_train_inputs[ui]] = dat\n",
    "        feed_dict[decoder_train_labels[ui]] = lbl\n",
    "        d_msk = (np.logical_not(np.argmax(lbl,axis=1)==dictionary['</s>'])).astype(np.int32).reshape(-1,1)\n",
    "        feed_dict[decoder_train_masks[ui]] = d_msk\n",
    "\n",
    "    # ======================= OTIMIZAÇÃO ==========================\n",
    "    if (step+1) < 20000:\n",
    "        _,l,tr_pred = sess.run([optimize,loss,train_prediction], feed_dict=feed_dict)\n",
    "    else:\n",
    "        _,l,tr_pred = sess.run([sgd_optimize,loss,train_prediction], feed_dict=feed_dict)\n",
    "\n",
    "    # ======================= SAVING ==========================\n",
    "    if (step+1)%1000==0:\n",
    "        checkpoint = tf.train.Checkpoint(optimizer=sgd_optimize, loss=loss, train_prediction=train_prediction)\n",
    "        \n",
    "    return l, tr_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb502b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-30 15:54:25.434896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-30 15:54:25.498165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-30 15:54:25.794318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-30 15:54:25.795045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-30 15:54:26.789518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-30 15:54:26.789873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-30 15:54:26.790149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-30 15:54:26.790353: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3098 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "2023-03-30 15:54:26.920979: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'logs'\n",
    "if not os.path.exists(log_dir):\n",
    "    os.mkdir(log_dir)\n",
    "\n",
    "train_prediction_text_fname = 'train_predictions.txt'\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.allow_soft_placement=True\n",
    "sess = tf.compat.v1.InteractiveSession(config=config)\n",
    "\n",
    "tf.compat.v1.global_variables_initializer().run()\n",
    "word_embeddings = np.load('embeddings.npy')\n",
    "\n",
    "def define_data_generators(batch_size, encoder_num_unrollings, decoder_num_unrollings):\n",
    "\n",
    "    encoder_data_generator = DataGenerator(batch_size=batch_size,num_unroll=encoder_num_unrollings,is_input=True, is_train=True)\n",
    "    decoder_data_generator = DataGenerator(batch_size=batch_size,num_unroll=decoder_num_unrollings,is_input=False, is_train=True)\n",
    "\n",
    "    test_encoder_data_generator = DataGenerator(batch_size=batch_size,num_unroll=encoder_num_unrollings,is_input=True, is_train=False)\n",
    "    test_decoder_data_generator = DataGenerator(batch_size=batch_size,num_unroll=decoder_num_unrollings,is_input=False, is_train=False)\n",
    "\n",
    "    return encoder_data_generator,decoder_data_generator,test_encoder_data_generator,test_decoder_data_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdf529b",
   "metadata": {},
   "source": [
    "### Restore checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b02c12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "status = checkpoint.restore(tf.train.latest_checkpoint(checkpoint_directory))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3348bde",
   "metadata": {},
   "source": [
    "### Running Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c428430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      ".....Step  500\n",
      "Actual: the secret ingredient is crime . </s> \n",
      "\n",
      "Predicted: i <unk> . . a . </s> \n",
      "(Train) BLEU (440 elements):  0.024440227855029428\n",
      ".....Step  1000\n",
      "Actual: please do not mutilate your child's genitals . that is all . </s> \n",
      "\n",
      "Predicted: i i a a . . . . </s> \n",
      "(Train) BLEU (580 elements):  0.09712813972500225\n",
      ".....Step  1500\n",
      "Actual: i used to deposit checks from my roommate for rent and he always said something stupid in the the memo line . he started writing things in the memo line \n",
      "\n",
      "Predicted: i was to be a and a mom . a in i was was to . . the time time . . i was a to . the time . \n",
      "(Train) BLEU (400 elements):  0.12480847946887484\n",
      ".....Step  2000\n",
      "Actual: i was once at work and i work i . a deli so i was cutting the plastic off this roast beef and instead of cutting the plastic wrapping i \n",
      "\n",
      "Predicted: i have a a a in i was in was i girl of i was a . first of a . . . i of the . <unk> of . \n",
      "(Train) BLEU (550 elements):  0.12866980304824177\n",
      ".....Step  2500\n",
      "Actual: the <unk> gargle blaster - the experience of drinking it described as being smashed in the face with a lemon peel wrapped around a large gold brick </s> \n",
      "\n",
      "Predicted: i first of . . <unk> <unk> of the . . . a . . the world . a few . . . the <unk> . . . </s> \n",
      "(Train) BLEU (520 elements):  0.1466004712979857\n",
      ".....Step  3000\n",
      "Actual: staying somewhere or with someone because it's safe or the easy route even though you're unhappy . sometimes it worth the risk to pack up and move on or out \n",
      "\n",
      "Predicted: i of . <unk> a . i a to . <unk> . . </s> \n",
      "(Train) BLEU (560 elements):  0.15016520078585505\n",
      ".....Step  3500\n",
      "Actual: tying to make us jealous . sometimes a girl will go on and on about some other dude , like it's going to somehow motivates us into trying harder . \n",
      "\n",
      "Predicted: i . the a . to </s> \n",
      "(Train) BLEU (500 elements):  0.15635415182947054\n",
      ".....Step  4000\n",
      "Actual: nothing . i wake up in the afternoon . </s> \n",
      "\n",
      "Predicted: i . </s> \n",
      "(Train) BLEU (360 elements):  0.1632906370123255\n",
      ".....Step  4500\n",
      "Actual: honestly not worried cause it's out of my control if i get it or not , and even if i do get it i'll just go to the doctor and \n",
      "\n",
      "Predicted: i i a . . a of the life . you was a . something . but i to i was it it . be be to the <unk> . \n",
      "(Train) BLEU (450 elements):  0.16279490951882433\n",
      ".....Step  5000\n",
      "Actual: my neighbor totoro </s> \n",
      "\n",
      "Predicted: i dad was was </s> \n",
      "(Train) BLEU (440 elements):  0.16696302794002318\n",
      ".....Step  5500\n",
      "Actual: the coronavirus is making headlines . that's what's happening in <unk> now </s> \n",
      "\n",
      "Predicted: i <unk> is the . . </s> \n",
      "(Train) BLEU (380 elements):  0.16435722414759418\n",
      ".....Step  6000\n",
      "Actual: chilli dog tacos you take pita bread and put in a hot dog and top that with chili and cheese . roll the top together and secure with a tooth \n",
      "\n",
      "Predicted: i . . . </s> \n",
      "(Train) BLEU (490 elements):  0.17785645234923975\n",
      ".....Step  6500\n",
      "Actual: put on your tuxedo . the poor guy is self conscious because hes <unk> </s> \n",
      "\n",
      "Predicted: i a the mouth . </s> \n",
      "(Train) BLEU (510 elements):  0.1730456900114986\n",
      ".....Step  7000\n",
      "Actual: we are specifically limited to one large animal . it fails to state what the animal can be . i'm thinking whale , or perhaps african elephant . </s> \n",
      "\n",
      "Predicted: i are a , . the of . . </s> \n",
      "(Train) BLEU (640 elements):  0.1724019416376958\n",
      ".....Step  7500\n",
      "Actual: send her home towel my balls off lights out </s> \n",
      "\n",
      "Predicted: i a own . . own . . . . </s> \n",
      "(Train) BLEU (490 elements):  0.17781620568665948\n",
      ".....Step  8000\n",
      "Actual: she was super serious about astrology . so did what every horoscope told her , and it was mind numbing . </s> \n",
      "\n",
      "Predicted: i was a <unk> and the . i i i i time time i . i i was a . . </s> \n",
      "(Train) BLEU (590 elements):  0.17444187199617722\n",
      ".....Step  8500\n",
      "Actual: hannibal , would be sick playing as the detective and playing through the tv show series and movies . </s> \n",
      "\n",
      "Predicted: i , <unk> be a of . a <unk> . the the the <unk> . . . the . </s> \n",
      "(Train) BLEU (480 elements):  0.17242142903458388\n",
      ".....Step  9000\n",
      "Actual: mcdonalds would lobby politicians to make vegetarianism illegal </s> \n",
      "\n",
      "Predicted: i . be </s> \n",
      "(Train) BLEU (420 elements):  0.17033964432881296\n",
      ".....Step  9500\n",
      "Actual: <unk> from how to train your dragon 2 . not so much the way he died , but the funeral scene , the music and the recital by <unk> . \n",
      "\n",
      "Predicted: the . the to do of <unk> . . </s> \n",
      "(Train) BLEU (420 elements):  0.1762731219634474\n",
      ".....Step  10000\n",
      "Actual: close your eyes </s> \n",
      "\n",
      "Predicted: i , <unk> . </s> \n",
      "(Train) BLEU (650 elements):  0.17011627571854437\n",
      ".....Step  10500\n",
      "Actual: if i had the ability to alter people's eyebrows at my will <unk> , i would be truly <unk> . </s> \n",
      "\n",
      "Predicted: i you was a same to get the , , the time . . but was be a a . </s> \n",
      "(Train) BLEU (500 elements):  0.17638526923589717\n",
      ".....Step  11000\n",
      "Actual: last april my wife's car was stolen from our driveway . the car was recovered a few weeks later by some friends of ours that are police officers that recognized \n",
      "\n",
      "Predicted: i year , dad . was a . the house . </s> \n",
      "(Train) BLEU (530 elements):  0.18304231408785127\n",
      ".....Step  11500\n",
      "Actual: turn the volume all the way up </s> \n",
      "\n",
      "Predicted: i to <unk> of of time to to </s> \n",
      "(Train) BLEU (450 elements):  0.18299114800070268\n",
      ".....Step  12000\n",
      "Actual: command block from minecraft </s> \n",
      "\n",
      "Predicted: the of </s> \n",
      "(Train) BLEU (560 elements):  0.17449352834309922\n",
      ".....Step  12500\n",
      "Actual: if the roles were switched and the woman put a man through college , she would get half his earnings until the day he dies . get paid and buy \n",
      "\n",
      "Predicted: i you world is in , i <unk> who a lot on a , and was be a . wife . he <unk> . was . </s> \n",
      "(Train) BLEU (370 elements):  0.1802798242710769\n",
      ".....Step  13000\n",
      "Actual: i was hanging out with friends at a bar but then going to head to my friend's to stay the night . we drove separately though . it was in \n",
      "\n",
      "Predicted: i was in in of a . a bar and i i to a in a parents . a in <unk> . i were to . . i was a \n",
      "(Train) BLEU (620 elements):  0.18144632757401652\n",
      ".....Step  13500\n",
      "Actual: enjoy making food or baking </s> \n",
      "\n",
      "Predicted: i the a . something . </s> \n",
      "(Train) BLEU (660 elements):  0.18358359587938913\n",
      ".....Step  14000\n",
      "Actual: not an olympian , but there is a reason why olympians have access to a lot of free condoms . </s> \n",
      "\n",
      "Predicted: i a adult , but i was a guy that i is been to the <unk> of people . . i </s> \n",
      "(Train) BLEU (520 elements):  0.1815662621776851\n",
      ".....Step  14500\n",
      "Actual: wait , you guys are getting coins ? </s> \n",
      "\n",
      "Predicted: i . i can are a a . </s> \n",
      "(Train) BLEU (390 elements):  0.1864608888028448\n",
      ".....Step  15000\n",
      "Actual: i think many yellow flags are really pink flags . we know they're red flags but try to convince ourselves they're not that bad . edit: clarity </s> \n",
      "\n",
      "Predicted: i have that people is . the good . . </s> \n",
      "(Train) BLEU (540 elements):  0.18278599766857645\n",
      ".....Step  15500\n",
      "Actual: being trapped in my body for a surgery or coma . </s> \n",
      "\n",
      "Predicted: i a in the head . a few . something . </s> \n",
      "(Train) BLEU (460 elements):  0.18226888936155453\n",
      ".....Step  16000\n",
      "Actual: probally daily bathing but same clothes </s> \n",
      "\n",
      "Predicted: i . . . i the . </s> \n",
      "(Train) BLEU (430 elements):  0.17639029979245055\n",
      ".....Step  16500\n",
      "Actual: canada isn't really socialist . . . . we're just farther left than america . </s> \n",
      "\n",
      "Predicted: i , to like . </s> \n",
      "(Train) BLEU (480 elements):  0.18268204572558144\n",
      ".....Step  17000\n",
      "Actual: back to the future . </s> \n",
      "\n",
      "Predicted: the in the movie . </s> \n",
      "(Train) BLEU (450 elements):  0.18275553524391008\n",
      ".....Step  17500\n",
      "Actual: very sensitive magnet . </s> \n",
      "\n",
      "Predicted: i <unk> . . </s> \n",
      "(Train) BLEU (360 elements):  0.18204193756627432\n",
      ".....Step  18000\n",
      "Actual: forget writing my name in snow , i'm carving it into a rocky hillside </s> \n",
      "\n",
      "Predicted: i to to life . the . but not . . the lot . . </s> \n",
      "(Train) BLEU (550 elements):  0.18776003021511634\n",
      ".....Step  18500\n",
      "Actual: the fuck is a water bear hero gonna do </s> \n",
      "\n",
      "Predicted: i <unk> of the <unk> . . . be it </s> \n",
      "(Train) BLEU (500 elements):  0.18628525039873556\n",
      ".....Step  19000\n",
      "Actual: ability to produce crabs . don't ask how </s> \n",
      "\n",
      "Predicted: i to <unk> the . </s> \n",
      "(Train) BLEU (460 elements):  0.18924181331220957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....Step  19500\n",
      "Actual: batman & robin had a really solid soundtrack for being as cheesy as it was . and opening it with smashing pumpkins , and finishing with an alternate take on \n",
      "\n",
      "Predicted: the . <unk> . a good good game . a a a . a was a </s> \n",
      "(Train) BLEU (390 elements):  0.18625420427999664\n",
      ".....Step  20000\n",
      "Actual: $180 mongoose mountain bike . a lot of money for a 14 year old , but it was worth it . . . still use it , too . </s> \n",
      "\n",
      "Predicted: i . . . . </s> \n",
      "(Train) BLEU (450 elements):  0.18620236387294706\n",
      ".....Step  20500\n",
      "Actual: eat bananas , drink plenty of water and stretch every morning . reason of spasms could be low potassium , dehydration , lack of exercise or poorly executed . </s> \n",
      "\n",
      "Predicted: i a to and , of the . then . day . </s> \n",
      "(Train) BLEU (520 elements):  0.18640694976549757\n",
      ".....Step  21000\n",
      "Actual: it spices things up and it makes the food less boring imo </s> \n",
      "\n",
      "Predicted: i is . . . i is me most . . . . </s> \n",
      "(Train) BLEU (630 elements):  0.1884307196915248\n",
      ".....Step  21500\n",
      "Actual: i worked at an airport and all i do all day was watch people as they go by . one particular good looking woman came and was carrying some flowers \n",
      "\n",
      "Predicted: i have at a airport and a the have not of . a a . well were to the </s> \n",
      "(Train) BLEU (460 elements):  0.18292601641957915\n",
      ".....Step  22000\n",
      "Actual: a couple days ago i had mine . i stepped outside the shelter to smoke a cigarette and met a 31 year old guy doing the same thing . i \n",
      "\n",
      "Predicted: i <unk> of ago , was a to i was out of <unk> . the . few of i it few . old . . the <unk> thing . i \n",
      "(Train) BLEU (620 elements):  0.18338179017311187\n",
      ".....Step  22500\n",
      "Actual: it's not the viscosity , it's simply a matter of air flow . plastic you can squeeze and send flying , glass you cannot haha </s> \n",
      "\n",
      "Predicted: i a a same . but a the lot . the . . </s> \n",
      "(Train) BLEU (560 elements):  0.18387868223025994\n",
      ".....Step  23000\n",
      "Actual: he showed up 2 months late for our first date . i still have no idea why i was completely happy to go out with him . we went to \n",
      "\n",
      "Predicted: i was a with years ago . a family job . i was had a idea how . was in . . me to . my . i were to \n",
      "(Train) BLEU (560 elements):  0.19031634126992855\n",
      ".....Step  23500\n",
      "Actual: marry someone who makes a good living , you can aimlessly follow your whims and have a so who slowly grows to resent your existence . or you could not \n",
      "\n",
      "Predicted: i to else is me lot job . but can get get that own . you a good of is you the the the own . </s> \n",
      "(Train) BLEU (510 elements):  0.19342628804597956\n",
      ".....Step  24000\n",
      "Actual: my 350 pound mother streaked in front of my boyfriend and my friends for 100 bucks . i was only 16 , and that image still haunts me . </s> \n",
      "\n",
      "Predicted: i mom was is was i the of the family . i mom were a years . i was a a and and i i was . me . i \n",
      "(Train) BLEU (580 elements):  0.19031046370725238\n",
      ".....Step  24500\n",
      "Actual: for those doing online schooling right now: seriously don't procrastinate : </s> \n",
      "\n",
      "Predicted: i the who the , , now . . have a </s> \n",
      "(Train) BLEU (490 elements):  0.19129895655133244\n",
      ".....Step  25000\n",
      "Actual: wiley coyote , that poor guy has had it rough . </s> \n",
      "\n",
      "Predicted: the . a and was . . a a . . </s> \n",
      "(Train) BLEU (460 elements):  0.19122015637440698\n",
      ".....Step  25500\n",
      "Actual: when i couldn't be bothered to eat despite being hungry . </s> \n",
      "\n",
      "Predicted: i i was sleep a , my the , a , i </s> \n",
      "(Train) BLEU (520 elements):  0.18853657043483077\n",
      ".....Step  26000\n",
      "Actual: 25 dollars isn't the price range for nice liquor =/ i would say splurge on some grey goose if you are willing to throw down another 15 . </s> \n",
      "\n",
      "Predicted: i years . a best of . a . . . have have that a the sort . . you have going to be it the . . </s> \n",
      "(Train) BLEU (470 elements):  0.19221512926471956\n",
      ".....Step  26500\n",
      "Actual: the episode of malcolm in the middle where the boys are stealing from the church and selling for profit . and francis gets to alaska and realizes he's in for \n",
      "\n",
      "Predicted: the <unk> of the . the <unk> of the <unk> are . . the <unk> . the . the . </s> \n",
      "(Train) BLEU (550 elements):  0.18537133067984338\n",
      ".....Step  27000\n",
      "Actual: misty from pokemon . </s> \n",
      "\n",
      "Predicted: i . the . </s> \n",
      "(Train) BLEU (340 elements):  0.1940532076493975\n",
      ".....Step  27500\n",
      "Actual: the complete and utter breakdown of society as we know it . gay nazis would ride on the back of dinosaurs . </s> \n",
      "\n",
      "Predicted: i <unk> of the . . the . a are . is </s> \n",
      "(Train) BLEU (560 elements):  0.19844144210472686\n",
      ".....Step  28000\n",
      "Actual: a trillion dollars ? jesus . probably invest in a lawyer and an accountant . that would put me in the kind of tax bracket that i have no idea \n",
      "\n",
      "Predicted: i <unk> of of </s> \n",
      "(Train) BLEU (610 elements):  0.1882184993295973\n",
      ".....Step  28500\n",
      "Actual: from this site: http://www . <unk> . <unk> . <unk> on the end of many caskets there is a plastic vial that screws into the body . it makes a \n",
      "\n",
      "Predicted: i the thread , . <unk> . <unk> . <unk> . the <unk> of the . . are a <unk> <unk> . is is the <unk> . </s> \n",
      "(Train) BLEU (600 elements):  0.1971580279132\n",
      ".....Step  29000\n",
      "Actual: somebody would get 7 <unk> dollars . </s> \n",
      "\n",
      "Predicted: i is be a . . to </s> \n",
      "(Train) BLEU (660 elements):  0.19346104963220445\n",
      ".....Step  29500\n",
      "Actual: it would throw the audience off , probably . if it's a dramatic scene it would be adversely amusing . it's gonna look funny even though it's accurate . </s> \n",
      "\n",
      "Predicted: i was be me same . the but a </s> \n",
      "(Train) BLEU (450 elements):  0.1896593881074002\n",
      ".....Step  30000\n",
      "Actual: the iron throne , have it converted into a working toilet and invite people to use my throne room . </s> \n",
      "\n",
      "Predicted: i <unk> . </s> \n",
      "(Train) BLEU (580 elements):  0.19407754495940846\n",
      ".....Step  30500\n",
      "Actual: talk to your co-workers about it in the break room , where no customers can hear you . take it as a learning experience being able to deal with dickhead \n",
      "\n",
      "Predicted: i to the friend , a . the morning . . and you one is be you . </s> \n",
      "(Train) BLEU (510 elements):  0.19436519261531435\n",
      ".....Step  31000\n",
      "Actual: johnny english vs mr bean </s> \n",
      "\n",
      "Predicted: i cash . . . . </s> \n",
      "(Train) BLEU (570 elements):  0.19541520098104692\n",
      ".....Step  31500\n",
      "Actual: psychedelic hallucination gas </s> \n",
      "\n",
      "Predicted: i <unk> <unk> . </s> \n",
      "(Train) BLEU (590 elements):  0.1912585177791407\n",
      ".....Step  32000\n",
      "Actual: windows at night time . i hate looking at dark windows . i always expect a face to pop up or a random hand to smack the window from somewhere \n",
      "\n",
      "Predicted: i . the . . </s> \n",
      "(Train) BLEU (420 elements):  0.20091139905047442\n",
      ".....Step  32500\n",
      "Actual: a dad came to school with rage in his eyes because his son was suspended for 3 days . . . this conversation took place while the son is in \n",
      "\n",
      "Predicted: i friend in to a with a and the room . he mom was a . a years . he . . was was me to he time was a \n",
      "(Train) BLEU (540 elements):  0.19530590495462377\n",
      ".....Step  33000\n",
      "Actual: don't mean to sound like a douche but what are the races and types of ppl involved , approx ages , city , general area ? is she a rich \n",
      "\n",
      "Predicted: i worry if be . a <unk> . i is you most ? are of people ? ? and ? ? and ? and , , </s> \n",
      "(Train) BLEU (410 elements):  0.19297119833299783\n",
      ".....Step  33500\n",
      "Actual: pepsi was available in the ussr </s> \n",
      "\n",
      "Predicted: i . born . the middle . </s> \n",
      "(Train) BLEU (540 elements):  0.1892332337942613\n",
      ".....Step  34000\n",
      "Actual: got a couple: assumed he had the job during the interview so was very relaxed . leaned back in the chair . showed up late . texting the whole time \n",
      "\n",
      "Predicted: i a <unk> , . was a same of the day . he a well . </s> \n",
      "(Train) BLEU (610 elements):  0.1891334282186105\n",
      ".....Step  34500\n",
      "Actual: normal sized <unk> . fuck the new tiny ones and their 2 layers . </s> \n",
      "\n",
      "Predicted: i . . . </s> \n",
      "(Train) BLEU (410 elements):  0.19804873384726268\n",
      ".....Step  35000\n",
      "Actual: ahhh ahhh <unk> <unk> </s> \n",
      "\n",
      "Predicted: <unk> the the <unk> </s> \n",
      "(Train) BLEU (490 elements):  0.1981263305777112\n",
      ".....Step  35500\n",
      "Actual: green jelly - 3 little pigs https://youtu . <unk> </s> \n",
      "\n",
      "Predicted: the <unk> - <unk> </s> \n",
      "(Train) BLEU (520 elements):  0.19976657007992699\n",
      ".....Step  36000\n",
      "Actual: well does revenge as a child count ? when i was 11 there were two bullies who would mess with me all the time . they were a couple years \n",
      "\n",
      "Predicted: i , this . a kid . . </s> \n",
      "(Train) BLEU (530 elements):  0.19596497278251543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....Step  36500\n",
      "Actual: my brother and i are very close , but i didn't know his wife very well before the wedding . i was happy to be asked to be a bridesmaid \n",
      "\n",
      "Predicted: i dad is i were a close to and i was know how mom was well . he first . he was a to the a to be a little \n",
      "(Train) BLEU (580 elements):  0.20022646429219562\n",
      ".....Step  37000\n",
      "Actual: not red lobster but seafood counter at a grocery store , the manager would regularly take dead lobsters that were being eaten by other lobsters , cook them , and \n",
      "\n",
      "Predicted: i a a , i . . a bar store . but first was be a a . . i a a . a people . and , to and \n",
      "(Train) BLEU (470 elements):  0.198799128871467\n",
      ".....Step  37500\n",
      "Actual: am i the only one who just hates everything about this video ? it's like a giant brooklyn yuppie hipster stereotype . such a narcissistic asshole hoping these people will \n",
      "\n",
      "Predicted: i i was only one who was got to . the . game </s> \n",
      "(Train) BLEU (400 elements):  0.19914697744926377\n",
      ".....Step  38000\n",
      "Actual: wait , am i the only one that finds it strange that you recorded her phone conversations ? </s> \n",
      "\n",
      "Predicted: i , i i was only one who is in is is you can like to . . </s> \n",
      "(Train) BLEU (480 elements):  0.19259782440206036\n",
      ".....Step  38500\n",
      "Actual: born 1967 , rural area , philippines much of the rural areas had no electricity . i remember the day sometime in 1970 when we got the utility installed . \n",
      "\n",
      "Predicted: i , a and , , and , , the <unk> , , a matter to </s> \n",
      "(Train) BLEU (570 elements):  0.20238729225588914\n",
      ".....Step  39000\n",
      "Actual: borders bookstore </s> \n",
      "\n",
      "Predicted: i . . </s> \n",
      "(Train) BLEU (390 elements):  0.19507237512087677\n",
      ".....Step  39500\n",
      "Actual: those happen once in a while , usually when i'm eating like shit , it's a hell of an adventure </s> \n",
      "\n",
      "Predicted: i of to in a <unk> . and , i going a a . and not lot to a entire <unk> </s> \n",
      "(Train) BLEU (610 elements):  0.1952748381400306\n",
      ".....Step  40000\n",
      "Actual: just another page in the history book i won't give it a second thought . </s> \n",
      "\n",
      "Predicted: i go one . the world of . was be you to little . . </s> \n",
      "(Train) BLEU (530 elements):  0.19726286760604142\n",
      ".....Step  40500\n",
      "Actual: when i started working , and i could buy things i wanted , an extra thing near the end of december wasn't a huge deal . shortly after that , \n",
      "\n",
      "Predicted: i i was a in i i was have a in was to i hour <unk> i . other of the . a good fan . i to i i \n",
      "(Train) BLEU (500 elements):  0.19356329670980427\n",
      ".....Step  41000\n",
      "Actual: the strokes - is this it 14 years later and i still listen in awe </s> \n",
      "\n",
      "Predicted: the <unk> . the a . is . ago . i was have to the . </s> \n",
      "(Train) BLEU (540 elements):  0.19507538522126938\n",
      ".....Step  41500\n",
      "Actual: rainbow in the dark </s> \n",
      "\n",
      "Predicted: the - the cradle - </s> \n",
      "(Train) BLEU (530 elements):  0.20270480107298156\n",
      ".....Step  42000\n",
      "Actual: i proposed on april 1st . i kept joking that it was a prank . i seriously considered saying april fool's at the alter . my now wife understands my \n",
      "\n",
      "Predicted: i was to a , , i was a . i was a good . i was have a that . to the time of i mom , was in \n",
      "(Train) BLEU (460 elements):  0.19009390155552383\n",
      ".....Step  42500\n",
      "Actual: warning: interacting with may cause an increase in your knowledge of dinosaurs . </s> \n",
      "\n",
      "Predicted: i , . a the the <unk> . the life . the . </s> \n",
      "(Train) BLEU (470 elements):  0.19976968561622044\n",
      ".....Step  43000\n",
      "Actual: getting everything done on schedule </s> \n",
      "\n",
      "Predicted: the a . . the . </s> \n",
      "(Train) BLEU (530 elements):  0.1942724282774912\n",
      ".....Step  43500\n",
      "Actual: i'm from estonia though currently living in london which isn't as tiny as andorra or liechtenstein but still relatively small . although there are other factors as well , one \n",
      "\n",
      "Predicted: i not the , , . in the . is a a . a . <unk> . i the . . </s> \n",
      "(Train) BLEU (450 elements):  0.20050853949403036\n",
      ".....Step  44000\n",
      "Actual: bisexual male here . when dating a girl , lots of things are already laid out for me not to be anti-feminist , several of the following have been reversed \n",
      "\n",
      "Predicted: i people , . </s> \n",
      "(Train) BLEU (640 elements):  0.2052925368736977\n",
      ".....Step  44500\n",
      "Actual: i live 20 minutes from canyon lake . it's rare that <unk> hits home quite like this , but i will do what i can to get my ass out \n",
      "\n",
      "Predicted: i was in years ago the . . i not , i . me . . a . but i don't never it you do do do . life . \n",
      "(Train) BLEU (490 elements):  0.20370835584205174\n",
      ".....Step  45000\n",
      "Actual: i've used an array of juices and sodas , along with water . water: 2/10 only works when you're eating frosted flakes . <unk> 0/10 jesus fucking christ orange <unk> \n",
      "\n",
      "Predicted: i been to <unk> to the . i the and with my . i , , have . i in . . . </s> \n",
      "(Train) BLEU (480 elements):  0.2014454137566767\n",
      ".....Step  45500\n",
      "Actual: i have a very difficult time saying 'no' . </s> \n",
      "\n",
      "Predicted: i have a friend vivid time . . . </s> \n",
      "(Train) BLEU (490 elements):  0.19661905291619675\n",
      ".....Step  46000\n",
      "Actual: who the fuck decided we eat and breathe with the same tube ? that asshole needs to be fired . </s> \n",
      "\n",
      "Predicted: i would fuck up to would the the . the same thing . </s> \n",
      "(Train) BLEU (430 elements):  0.19755442733128142\n",
      ".....Step  46500\n",
      "Actual: how many guests are you expecting that the wedding ? multiply by 2 to account for <unk> . call this number g . i assume you are treating your guests \n",
      "\n",
      "Predicted: i do people you you doing ? you <unk> ? </s> \n",
      "(Train) BLEU (480 elements):  0.20554487952232478\n",
      ".....Step  47000\n",
      "Actual: my greatest feeling is also my saddest and most painful . my cousin was diagnosed with bone cancer in his femur at age 15 , he got through it and \n",
      "\n",
      "Predicted: i dad friend was a a favorite friend i of i've i dad is a with a . . the life . the . . and was a the . \n",
      "(Train) BLEU (540 elements):  0.19917053099814042\n",
      ".....Step  47500\n",
      "Actual: back in 2000 there was this game called ants on microsoft gaming zone . it was a 4 player online multiplayer where you had these ants that had to go \n",
      "\n",
      "Predicted: i in the , is a kid that <unk> of the . . . </s> \n",
      "(Train) BLEU (420 elements):  0.19746027147282985\n",
      ".....Step  48000\n",
      "Actual: take whatever is on the front page and do a fixed of it . </s> \n",
      "\n",
      "Predicted: i a to a the couch page </s> \n",
      "(Train) BLEU (540 elements):  0.1980294676935963\n",
      ".....Step  48500\n",
      "Actual: i'm not racist but . . . . </s> \n",
      "\n",
      "Predicted: i not sure a not . . . . </s> \n",
      "(Train) BLEU (490 elements):  0.19380524740440208\n",
      ".....Step  49000\n",
      "Actual: i have had some great encounters , from free coffee to someone buying me a meal because i had left my wallet at home . i always carry those experiences \n",
      "\n",
      "Predicted: i was a a friends stories . but a . . me . a . lot . i was a my friends . a . i was thought a things \n",
      "(Train) BLEU (390 elements):  0.1946478115667356\n",
      ".....Step  49500\n",
      "Actual: what made me realize that i needed help was that i went into the doctor's today . i've had a bad cough for the past few weeks . they put \n",
      "\n",
      "Predicted: i do the think that i am to . a i am to a bathroom . . i been a lot habit that me first . years . i are \n",
      "(Train) BLEU (500 elements):  0.20042566227872038\n",
      ".....Step  50000\n",
      "Actual: frank <unk> dune </s> \n",
      "\n",
      "Predicted: the zappa . . </s> \n",
      "(Train) BLEU (470 elements):  0.19878276998477706\n",
      ".....Step  50500\n",
      "Actual: i was surprised that strangers came up to me and talked to me . </s> \n",
      "\n",
      "Predicted: i don't in to i i to with the . i to a . i </s> \n",
      "(Train) BLEU (510 elements):  0.19556340743352288\n",
      ".....Step  51000\n",
      "Actual: pets are family </s> \n",
      "\n",
      "Predicted: i , you . </s> \n",
      "(Train) BLEU (490 elements):  0.20447626316694248\n",
      ".....Step  51500\n",
      "Actual: be the one to start the conversation . please . </s> \n",
      "\n",
      "Predicted: i a only that be . <unk> . </s> \n",
      "(Train) BLEU (420 elements):  0.2060228550022879\n",
      ".....Step  52000\n",
      "Actual: joe , he seems like the most confident member of the group and has the ability to add so much more to what he's told to do </s> \n",
      "\n",
      "Predicted: the rogan the was to a first beautiful movie . the world of the a best to be a much . . the he a me be . </s> \n",
      "(Train) BLEU (530 elements):  0.19996344317950415\n",
      ".....Step  52500\n",
      "Actual: i don't have a mobile phone . the effect on my life is that i experience life when i get up from my desk . </s> \n",
      "\n",
      "Predicted: i have know a job but but i only is the family is a i can in . i was a . the own . </s> \n",
      "(Train) BLEU (540 elements):  0.2023118465201213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....Step  53000\n",
      "Actual: call it death with dignity , because that's what it is . </s> \n",
      "\n",
      "Predicted: i a to . a . and they why you is . </s> \n",
      "(Train) BLEU (480 elements):  0.1957275868118364\n",
      ".....Step  53500\n",
      "Actual: this one is good when you're feeling low . </s> \n",
      "\n",
      "Predicted: i is is a . i in . . </s> \n",
      "(Train) BLEU (540 elements):  0.20611113537715012\n",
      ".....Step  54000\n",
      "Actual: not a teacher , but i was an it technician at my high school about 6 years after i graduated . the staff at the school all share stories about \n",
      "\n",
      "Predicted: i a teacher , but i was a older teacher teacher a school school . a years ago a was in i first were a time was day the . \n",
      "(Train) BLEU (510 elements):  0.18943730700790004\n",
      ".....Step  54500\n",
      "Actual: <unk> , when rickrolling was hot , that shit was great . i also always enjoyed <unk> isn't a meme </s> \n",
      "\n",
      "Predicted: <unk> . <unk> i <unk> a , i was was a . </s> \n",
      "(Train) BLEU (490 elements):  0.20583684883798167\n",
      ".....Step  55000\n",
      "Actual: i work in it and almost every person that i work with that is indian is at least outwardly very positive and pleasant socially . also rather willing to share \n",
      "\n",
      "Predicted: the would in a . i a day in i was in a is a . a the . . <unk> . the . . </s> \n",
      "(Train) BLEU (370 elements):  0.20032219060127618\n",
      ".....Step  55500\n",
      "Actual: playing cod4 , like a <unk> . on whatever map it is where it's raining , and there is the farm like house with the 2 silos near it . \n",
      "\n",
      "Predicted: i the . and a <unk> , </s> \n",
      "(Train) BLEU (500 elements):  0.20058183280687208\n",
      ".....Step  56000\n",
      "Actual: wake up from the dream </s> \n",
      "\n",
      "Predicted: i up with the <unk> . </s> \n",
      "(Train) BLEU (380 elements):  0.1987311815209929\n",
      ".....Step  56500\n",
      "Actual: all vegetables . turns out you shouldn't dump the contents of canned vegetables into a pot and boil it all until it turns grey . who woulda thunk , amirite \n",
      "\n",
      "Predicted: i the . i out that have be to same of the . . the <unk> of you your to the you . out . </s> \n",
      "(Train) BLEU (470 elements):  0.19986243313225832\n",
      ".....Step  57000\n",
      "Actual: <unk> is cave johnson and <unk> daughter </s> \n",
      "\n",
      "Predicted: the . a . . the . . </s> \n",
      "(Train) BLEU (460 elements):  0.20121251960978445\n",
      ".....Step  57500\n",
      "Actual: rebel wilson and melissa mccarthy </s> \n",
      "\n",
      "Predicted: the the . the . . </s> \n",
      "(Train) BLEU (630 elements):  0.20017090274832128\n",
      ".....Step  58000\n",
      "Actual: not at all when my brother tried to make a pass at my then fiance he was dead to me he did other things but that was the final straw \n",
      "\n",
      "Predicted: i a the , i parents was to go me shit . the house . . was a . me . was . . . i he a same of \n",
      "(Train) BLEU (450 elements):  0.20219518517864632\n",
      ".....Step  58500\n",
      "Actual: i thoroughly enjoyed robert <unk> <unk> . easy read , hard to put down </s> \n",
      "\n",
      "Predicted: the think a at <unk> . . i to a but to be a a </s> \n",
      "(Train) BLEU (530 elements):  0.2036839137782998\n",
      ".....Step  59000\n",
      "Actual: inception . there were so many subtleties and nuances that i had to </s> \n",
      "\n",
      "Predicted: the . . was a many times in the . was was a be </s> \n",
      "(Train) BLEU (580 elements):  0.20242164062912177\n",
      ".....Step  59500\n",
      "Actual: monkey island 2: <unk> revenge </s> \n",
      "\n",
      "Predicted: the . . </s> \n",
      "(Train) BLEU (600 elements):  0.19617261370371428\n",
      ".....Step  60000\n",
      "Actual: where the sun beats by blue sky black death the moon is down - explosions in the sky </s> \n",
      "\n",
      "Predicted: the the hell is you <unk> <unk> </s> \n",
      "(Train) BLEU (480 elements):  0.19983470826945593\n",
      ".....Step  60500\n",
      "Actual: i made a couple of blind jokes about myself . oh yeah , did i mention i'm legally blind ? </s> \n",
      "\n",
      "Predicted: i was a <unk> of my friends in a . i , , i you have a a ? ? </s> \n",
      "(Train) BLEU (430 elements):  0.2063117365642356\n",
      ".....Step  61000\n",
      "Actual: a movie remake of season 8 of got that is actually decent </s> \n",
      "\n",
      "Predicted: the <unk> of of the . . the a i a a . </s> \n",
      "(Train) BLEU (450 elements):  0.20607310691901295\n",
      ".....Step  61500\n",
      "Actual: remake pulp fiction into a <unk> like the rocky horror picture show , with songs , cross-dressing and props and everything or just make any other movie into that style \n",
      "\n",
      "Predicted: the of . . the <unk> . the <unk> . . . . and the , and , <unk> . <unk> . the a it <unk> . . the . \n",
      "(Train) BLEU (550 elements):  0.19852507334327366\n",
      ".....Step  62000\n",
      "Actual: my definition of personal space . edit: holy crap i haven't logged into <unk> since i made this comment and it blew the heck up . thanks for the awards \n",
      "\n",
      "Predicted: the wife is people is is </s> \n",
      "(Train) BLEU (410 elements):  0.19626106006534869\n",
      ".....Step  62500\n",
      "Actual: i've proposed somebody in first grade . i've shat my pants in school . i've called my teacher - mom . </s> \n",
      "\n",
      "Predicted: i been to to the grade . </s> \n",
      "(Train) BLEU (510 elements):  0.20332441460262285\n",
      ".....Step  63000\n",
      "Actual: it's full of <unk> mechanics . </s> \n",
      "\n",
      "Predicted: i a of people . . </s> \n",
      "(Train) BLEU (450 elements):  0.19680768771253282\n",
      ".....Step  63500\n",
      "Actual: i once tried to blow open a trash bag </s> \n",
      "\n",
      "Predicted: i was had to be up and few . of </s> \n",
      "(Train) BLEU (480 elements):  0.2031394964084469\n",
      ".....Step  64000\n",
      "Actual: i'm in the camp that believes that there are actual cures for some of the incurable diseases that exist . but there's no money in the cure . . . \n",
      "\n",
      "Predicted: i not a uk , i the i is a people . the reason the most . . are to </s> \n",
      "(Train) BLEU (500 elements):  0.20518664654129531\n",
      ".....Step  64500\n",
      "Actual: i'm literally not joking but i can write with both hands and can solve the rubik's cube in less than 30 seconds </s> \n",
      "\n",
      "Predicted: i a a a . i have tell a my sides of <unk> be the same . . the than a years . </s> \n",
      "(Train) BLEU (410 elements):  0.20890089000952725\n",
      ".....Step  65000\n",
      "Actual: daredevil . it had a great intro . </s> \n",
      "\n",
      "Predicted: i . . is a lot idea to </s> \n",
      "(Train) BLEU (560 elements):  0.202530966357238\n",
      ".....Step  65500\n",
      "Actual: i'm just gonna get a t-rex and watch it wreak havoc for a bit before they take it away . fuck long term satisfaction i want immediate carnage </s> \n",
      "\n",
      "Predicted: i a a say a lot . i it . . . the while . i have a . . </s> \n",
      "(Train) BLEU (470 elements):  0.19641548314448828\n",
      ".....Step  66000\n",
      "Actual: i have several brothers and sisters . once i <unk> my younger brother and knocked him out cold after watching too much wrestling . i left him there and told \n",
      "\n",
      "Predicted: i was a friends . i . i i was my wife , , i up to of , a the much , . </s> \n",
      "(Train) BLEU (480 elements):  0.20544719848069634\n",
      ".....Step  66500\n",
      "Actual: lotta smart hr employees on <unk> . </s> \n",
      "\n",
      "Predicted: i , , . . the . i </s> \n",
      "(Train) BLEU (550 elements):  0.2024743806754185\n",
      ".....Step  67000\n",
      "Actual: my parents got tickets to be on the show my mom works in post production , probably how they got them . it was a lot of sitting around and \n",
      "\n",
      "Predicted: i dad had a to the a the <unk> . mom . . the . . and a to were to . i was a lot of people in the \n",
      "(Train) BLEU (580 elements):  0.20019420365777738\n",
      ".....Step  67500\n",
      "Actual: the smartest and wealthiest people in the state have incredible reading comprehension issues . they will study the menu for ten minutes or more and then when they order , \n",
      "\n",
      "Predicted: i <unk> thing <unk> , who the world of a . in . . </s> \n",
      "(Train) BLEU (650 elements):  0.204984936451932\n",
      ".....Step  68000\n",
      "Actual: i think it should be a last case option . where sex education and pregnancy <unk> are used first . </s> \n",
      "\n",
      "Predicted: i think it's is be a good year of . </s> \n",
      "(Train) BLEU (500 elements):  0.20666554296746759\n",
      ".....Step  68500\n",
      "Actual: the <unk> guide to the galaxy </s> \n",
      "\n",
      "Predicted: the <unk> of . the galaxy . </s> \n",
      "(Train) BLEU (640 elements):  0.20504020532397677\n",
      ".....Step  69000\n",
      "Actual: how come he don't want me , man ? - fresh prince </s> \n",
      "\n",
      "Predicted: the to to is know to to but . </s> \n",
      "(Train) BLEU (520 elements):  0.19241157453615776\n",
      ".....Step  69500\n",
      "Actual: doing math without a calculator because they were banned . reason being: you won't have a calculator in your pocket all the time when you grow up . . . \n",
      "\n",
      "Predicted: i a in a <unk> , i were a . i , . . be a lot . the life . the time . you get . . </s> \n",
      "(Train) BLEU (490 elements):  0.2052274696077896\n",
      ".....Step  70000\n",
      "Actual: me having a girlfriend </s> \n",
      "\n",
      "Predicted: i . a job in </s> \n",
      "(Train) BLEU (410 elements):  0.20958734700562442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....Step  70500\n",
      "Actual: if they only kidnap one human to study all of humanity , they've already failed . </s> \n",
      "\n",
      "Predicted: i you are do them , , the , the the , then will . to </s> \n",
      "(Train) BLEU (480 elements):  0.20290593012976782\n",
      ".....Step  71000\n",
      "Actual: our engineers and scientists are too smart to get tied up in our political system . </s> \n",
      "\n",
      "Predicted: i <unk> is <unk> are not lazy . be a to . the relationship . . </s> \n",
      "(Train) BLEU (420 elements):  0.20366204647273134\n",
      ".....Step  71500\n",
      "Actual: walmart i've seen that shit fuck the economies of small towns bad man . </s> \n",
      "\n",
      "Predicted: <unk> . been a i . . fuck . the . . . . </s> \n",
      "(Train) BLEU (480 elements):  0.20517034218930685\n",
      ".....Step  72000\n",
      "Actual: i am in foster care now i'm 17 and i entered in temporary care at the age of 14 . i've always said the most important thing for temporary foster \n",
      "\n",
      "Predicted: i have a a of . . not . i am a a . . the end of the . i been been that best recent thing i've me and \n",
      "(Train) BLEU (490 elements):  0.19708464344286442\n",
      ".....Step  72500\n",
      "Actual: build <unk> , duh </s> \n",
      "\n",
      "Predicted: i a . and , </s> \n",
      "(Train) BLEU (500 elements):  0.20576846703590243\n",
      ".....Step  73000\n",
      "Actual: people crying in sadness vs crying out in anger , i know there's some gray area in between where they can be used interchangeably , it's hard to get you \n",
      "\n",
      "Predicted: i who to the . . . of the . and was that a of . . the the i are . a to . but not to be a \n",
      "(Train) BLEU (420 elements):  0.20665447182549432\n",
      ".....Step  73500\n",
      "Actual: defunding arts in schools . </s> \n",
      "\n",
      "Predicted: i , . the . </s> \n",
      "(Train) BLEU (520 elements):  0.202859267917874\n",
      ".....Step  74000\n",
      "Actual: real estate in whichever city becomes the next big tech hub . </s> \n",
      "\n",
      "Predicted: i estate . the . . a best thing of of . </s> \n",
      "(Train) BLEU (460 elements):  0.20947687389798728\n",
      ".....Step  74500\n",
      "Actual: put our cat down because we couldn't afford to help him with surgery for his renal failure . it was like accepting an inadequacy that resulted in a dear friends \n",
      "\n",
      "Predicted: i it phone in and they don't get you the . . the . a son . . </s> \n",
      "(Train) BLEU (480 elements):  0.2014405238340256\n",
      ".....Step  75000\n",
      "Actual: nah , i think i'd drown before i got much food in my mouth . </s> \n",
      "\n",
      "Predicted: i . i would you be . you get a more . my life . </s> \n",
      "(Train) BLEU (390 elements):  0.20667749136387314\n",
      ".....Step  75500\n",
      "Actual: instead of spock ears , they'll wear fake ears with gauges molded in . also clip on nose & lip rips and fake tattoo arm sleeves . also beards of \n",
      "\n",
      "Predicted: i of a , , i be a , . a . . the </s> \n",
      "(Train) BLEU (520 elements):  0.19983854374269558\n",
      ".....Step  76000\n",
      "Actual: i think many rockstar characters are very well made , but their best has to be john marston . </s> \n",
      "\n",
      "Predicted: the have it's of as are the good . . but i <unk> friends been be a . . </s> \n",
      "(Train) BLEU (440 elements):  0.20156788306156492\n",
      ".....Step  76500\n",
      "Actual: due to <unk> , i cant <unk> dies </s> \n",
      "\n",
      "Predicted: i to the , i have get . . </s> \n",
      "(Train) BLEU (420 elements):  0.20701503227545848\n",
      ".....Step  77000\n",
      "Actual: district 9 , anyone ? </s> \n",
      "\n",
      "Predicted: the . . <unk> who </s> \n",
      "(Train) BLEU (380 elements):  0.19505968487187822\n",
      ".....Step  77500\n",
      "Actual: our world is out perception of our surroundings in our 3 dimensional life . it's possible that we are living amongst beings , things and events that we cannot experience \n",
      "\n",
      "Predicted: the <unk> is a of . the country . the country . . . </s> \n",
      "(Train) BLEU (470 elements):  0.2007885568821017\n",
      ".....Step  78000\n",
      "Actual: going to a children's dentist . mine kicked me out when i got wisdom teeth . adult dentists don't talk like donald duck or give you toys , they just \n",
      "\n",
      "Predicted: i to the bar <unk> . </s> \n",
      "(Train) BLEU (600 elements):  0.2062569905274634\n",
      ".....Step  78500\n",
      "Actual: the turning point in a book where , up until then , you thought it was unbearably awful , and now realize the novel is a masterpiece and you fall \n",
      "\n",
      "Predicted: i smell is of the car . i i , i i and can it was a . . and i i that <unk> is a good . i can \n",
      "(Train) BLEU (440 elements):  0.2041016971194741\n",
      ".....Step  79000\n",
      "Actual: for all you canadian motherfuckers out there . uh-oh </s> \n",
      "\n",
      "Predicted: the the the know . , of is </s> \n",
      "(Train) BLEU (580 elements):  0.20377918102972242\n",
      ".....Step  79500\n",
      "Actual: fix things that break </s> \n",
      "\n",
      "Predicted: the a like are up </s> \n",
      "(Train) BLEU (500 elements):  0.20580917602621912\n",
      ".....Step  80000\n",
      "Actual: colin <unk> ? biography , i was like 8 or so . </s> \n",
      "\n",
      "Predicted: the . <unk> </s> \n",
      "(Train) BLEU (650 elements):  0.20200527121402378\n",
      ".....Step  80500\n",
      "Actual: she <unk> installing a urinal in the basement bathroom . </s> \n",
      "\n",
      "Predicted: i was a to <unk> . the middle . . </s> \n",
      "(Train) BLEU (500 elements):  0.20454939165759659\n",
      ".....Step  81000\n",
      "Actual: i can't stand ben stiller . he's just the worst . i just can't imagine anyone else playing white goodman in dodgeball . </s> \n",
      "\n",
      "Predicted: <unk> don't remember the . </s> \n",
      "(Train) BLEU (450 elements):  0.20872683117969804\n",
      ".....Step  81500\n",
      "Actual: i'll have to say 2020 . i had a promotion and started working as a manager . i got a blue belt on <unk> . and i think have grown \n",
      "\n",
      "Predicted: i be a be that . </s> \n",
      "(Train) BLEU (580 elements):  0.20600615715541898\n",
      ".....Step  82000\n",
      "Actual: i had a large camel spider run over my face at about 3 am . that was not a pleasant sensation . </s> \n",
      "\n",
      "Predicted: i was a <unk> <unk> in . in a house . a 5 months . i was a a great thing . </s> \n",
      "(Train) BLEU (500 elements):  0.20607078435338452\n",
      ".....Step  82500\n",
      "Actual: i didn't have a problem with most subjects , but if i have to choose , p . e . why do we need to learn gymnastics , acrobatics , \n",
      "\n",
      "Predicted: i was know a job , a of , but i i was a be a i . . . g are you have to be to ? and <unk> \n",
      "(Train) BLEU (430 elements):  0.21573529710727987\n",
      ".....Step  83000\n",
      "Actual: i have nothing to add other than i love these videos . thanks for doing what you do . </s> \n",
      "\n",
      "Predicted: i don't a to do with than the have it people . </s> \n",
      "(Train) BLEU (490 elements):  0.2063254805240396\n",
      ".....Step  83500\n",
      "Actual: i always point to the <unk> lines as signs of alien life , they were made between 500 bce - 500 ce and are absolutely massive formations in the ground \n",
      "\n",
      "Predicted: i don't thought the the <unk> . . a of the . . but are a up the . . the . . the the a . . the same \n",
      "(Train) BLEU (570 elements):  0.20858666923718752\n",
      ".....Step  84000\n",
      "Actual: being in complete control and pleasing my partner . </s> \n",
      "\n",
      "Predicted: i a a with . i . own . </s> \n",
      "(Train) BLEU (500 elements):  0.19713742537819703\n",
      ".....Step  84500\n",
      "Actual: <unk> and rubber </s> \n",
      "\n",
      "Predicted: i <unk> the . </s> \n",
      "(Train) BLEU (530 elements):  0.2122094502952756\n",
      ".....Step  85000\n",
      "Actual: i'd say comfortably numb by pink floyd . i pretend that i am the frontman in an enormous and trippy concert . </s> \n",
      "\n",
      "Predicted: the say the numb . <unk> floyd </s> \n",
      "(Train) BLEU (590 elements):  0.20293390403427233\n",
      ".....Step  85500\n",
      "Actual: something super cheap with the price tag still on it </s> \n",
      "\n",
      "Predicted: i like nice . a same of . . the . </s> \n",
      "(Train) BLEU (550 elements):  0.2099020118549591\n",
      ".....Step  86000\n",
      "Actual: why do most fish live in salt water ? because pepper makes them sneeze . </s> \n",
      "\n",
      "Predicted: why is you people of in the ? ? </s> \n",
      "(Train) BLEU (530 elements):  0.20565149667606097\n",
      ".....Step  86500\n",
      "Actual: not exactly on point , but a woman i work with couldn't come to an agreement with her husband when they married , so they both took a new last \n",
      "\n",
      "Predicted: i a what the , but i friend . was in , do out me old . a . . i were me and i were have her lot job \n",
      "(Train) BLEU (500 elements):  0.20290999927175962\n",
      ".....Step  87000\n",
      "Actual: because i am in a different timezone , it is <unk> here . </s> \n",
      "\n",
      "Predicted: i i don't a the small school . and is a . . </s> \n",
      "(Train) BLEU (350 elements):  0.19962140966181685\n",
      ".....Step  87500\n",
      "Actual: why were you up so late ? i just didn't go to bed . you miss the best part of the day sure , when everyone is out and it's \n",
      "\n",
      "Predicted: i do you really with i . </s> \n",
      "(Train) BLEU (550 elements):  0.21480414263407607\n",
      ".....Step  88000\n",
      "Actual: what a long crazy trip it's been </s> \n",
      "\n",
      "Predicted: i is fuck term is is like a </s> \n",
      "(Train) BLEU (360 elements):  0.20683895238353533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....Step  88500\n",
      "Actual: the two <unk> what can men do against such reckless hate ? ride out with me . clip </s> \n",
      "\n",
      "Predicted: the <unk> of . the you </s> \n",
      "(Train) BLEU (630 elements):  0.20029313636265278\n",
      ".....Step  89000\n",
      "Actual: a headache that woke me up at 4am one day . never felt pain like it - dental <unk> i've had . broken bones i've had . i've been mildly \n",
      "\n",
      "Predicted: i <unk> . is up up . the . day . </s> \n",
      "(Train) BLEU (460 elements):  0.20886468320735352\n",
      ".....Step  89500\n",
      "Actual: my family runs a concessions trailer at fairs . lots of great stories but this one always sticks out . we have a giant <unk> steam table directly in front \n",
      "\n",
      "Predicted: i friend was in lot and on a . i of people things . i is of makes to . </s> \n",
      "(Train) BLEU (480 elements):  0.21038580313332506\n",
      ".....Step  90000\n",
      "Actual: roller coaster tycoon </s> \n",
      "\n",
      "Predicted: the coaster . . </s> \n",
      "(Train) BLEU (450 elements):  0.20288541909718583\n",
      ".....Step  90500\n",
      "Actual: beer steak grill sun cats </s> \n",
      "\n",
      "Predicted: i . . . . </s> \n",
      "(Train) BLEU (510 elements):  0.20421277636260102\n",
      ".....Step  91000\n",
      "Actual: i leave various weapons next to a tree and leave a sign for the bears saying if you want it it's yours . </s> \n",
      "\n",
      "Predicted: i would my <unk> . to me <unk> . i it new of the rest . . i were to to a of </s> \n",
      "(Train) BLEU (350 elements):  0.2110998511590819\n",
      ".....Step  91500\n",
      "Actual: just because we spend a lot of time on technology doesn't mean we're dumb stupid idiots who can't understand anything besides free wi-fi </s> \n",
      "\n",
      "Predicted: i a you have a lot of time to the . have you not . . . are be what . . . . </s> \n",
      "(Train) BLEU (430 elements):  0.20204609735081347\n",
      ".....Step  92000\n",
      "Actual: just the tip </s> \n",
      "\n",
      "Predicted: i a <unk> of </s> \n",
      "(Train) BLEU (490 elements):  0.20480653402962054\n",
      ".....Step  92500\n",
      "Actual: first , i take the pill . that is to say , i accept the pill , but i don't swallow it right then and there . instead , i \n",
      "\n",
      "Predicted: i time i would a <unk> to </s> \n",
      "(Train) BLEU (520 elements):  0.20198493070063464\n",
      ".....Step  93000\n",
      "Actual: just finished the umbrella academy . i'm not sure about my chances </s> \n",
      "\n",
      "Predicted: the the a first . of </s> \n",
      "(Train) BLEU (370 elements):  0.2091330341821086\n",
      ".....Step  93500\n",
      "Actual: ask him why antibiotics start becoming obsolete . </s> \n",
      "\n",
      "Predicted: i if if he , . . . </s> \n",
      "(Train) BLEU (620 elements):  0.19812886950462896\n",
      ".....Step  94000\n",
      "Actual: good job . wanna really make someone insecure ? wait till they've done something really really well and then walk up to them and say wow good job and nothing \n",
      "\n",
      "Predicted: i luck . </s> \n",
      "(Train) BLEU (500 elements):  0.21116684929046325\n",
      ".....Step  94500\n",
      "Actual: my sister told me this gem . her male friend said he hated being a guy because he couldnt use an umbrella . apparently it's gay to keep yourself dry \n",
      "\n",
      "Predicted: i dad was me to story . i husband is was i was me a <unk> . he was . to <unk> to </s> \n",
      "(Train) BLEU (550 elements):  0.20407308510179276\n",
      ".....Step  95000\n",
      "Actual: don't put something in an email unless you'd be happy with everyone in the office seeing it . if someone wants you to do something that you aren't sure is \n",
      "\n",
      "Predicted: <unk> worry a on the argument . you be a . the . the world . . is </s> \n",
      "(Train) BLEU (370 elements):  0.2152258413147808\n",
      ".....Step  95500\n",
      "Actual: time - pink floyd </s> \n",
      "\n",
      "Predicted: <unk> - <unk> floyd - </s> \n",
      "(Train) BLEU (670 elements):  0.2109123060470796\n",
      ".....Step  96000\n",
      "Actual: i fucking hate the polar express . like if someone starts playing it , it will literally boil my blood and i have to leave . </s> \n",
      "\n",
      "Predicted: the think hate the movie . . </s> \n",
      "(Train) BLEU (580 elements):  0.2029806226370729\n",
      ".....Step  96500\n",
      "Actual: some people have did things that are so horrible that they don't deserve to live . </s> \n",
      "\n",
      "Predicted: i people are a a like i the much . i are have to be in </s> \n",
      "(Train) BLEU (530 elements):  0.20949137442160568\n",
      ".....Step  97000\n",
      "Actual: travel around and learn about other places . learn to drive , and how to take a plane , and a cab , and the subway , and a ferry \n",
      "\n",
      "Predicted: i to the you to the people . </s> \n",
      "(Train) BLEU (430 elements):  0.21159244938878732\n",
      ".....Step  97500\n",
      "Actual: bar server here . please stop asking if you can have one for free . idgaf it's your birthday . it's everyone in here's birthday . </s> \n",
      "\n",
      "Predicted: i , . . i , <unk> for you have get a of a . </s> \n",
      "(Train) BLEU (440 elements):  0.1999716864614223\n",
      ".....Step  98000\n",
      "Actual: it's a substitute for a turn signal and/or a ged . </s> \n",
      "\n",
      "Predicted: i a good . a few to . . lot . </s> \n",
      "(Train) BLEU (520 elements):  0.2044314841079151\n",
      ".....Step  98500\n",
      "Actual: because vietnam was viewed as an <unk> and unfair war by much of the american public . that's invariably going to translate in hatred for the soldiers in the war \n",
      "\n",
      "Predicted: i the . a the a american . the . . the time the <unk> . . </s> \n",
      "(Train) BLEU (470 elements):  0.2003220459572208\n",
      ".....Step  99000\n",
      "Actual: antivaxxers and overt racism . who even goes up to someone and called them the big ol n word ? i didn't realize we were in the 50s you shit \n",
      "\n",
      "Predicted: the and <unk> . . </s> \n",
      "(Train) BLEU (540 elements):  0.20597738966356713\n",
      ".....Step  99500\n",
      "Actual: how to yell and lie </s> \n",
      "\n",
      "Predicted: i many do in <unk> . </s> \n",
      "(Train) BLEU (490 elements):  0.20223368278630852\n",
      ".....Step  100000\n",
      "Actual: i went on a first date with a girl that i had been talking online to for awhile . we decided to go to a movie together one weekend so \n",
      "\n",
      "Predicted: i was to a date date with a friend who was was a dating about to me a . i were to get to a party and and day . \n",
      "(Train) BLEU (370 elements):  0.20464307340547733\n"
     ]
    }
   ],
   "source": [
    "train_bleu_scores_over_time,test_bleu_scores_over_time = [],[]\n",
    "loss_over_time = []\n",
    "\n",
    "train_bleu_refs, train_bleu_cands = [],[]\n",
    "test_bleu_refs, test_bleu_cands = [],[]\n",
    "\n",
    "num_steps = 100001\n",
    "avg_loss = 0\n",
    "\n",
    "encoder_data_generator, decoder_data_generator, \\\n",
    "test_encoder_data_generator, test_decoder_data_generator = \\\n",
    "define_data_generators(batch_size, encoder_num_unrollings, decoder_num_unrollings)\n",
    "\n",
    "print('Starting training')\n",
    "\n",
    "for step in range(num_steps):\n",
    "\n",
    "    if (step+1)%100==0:\n",
    "        print('.',end='')\n",
    "\n",
    "    sent_ids = np.random.randint(low=0,high=train_inputs.shape[0],size=(batch_size))\n",
    "\n",
    "    # Getting an unrolled set of data batches for the encoder\n",
    "    unrolled_encoder_data, _, _ = encoder_data_generator.unroll_batches(sent_ids=sent_ids)\n",
    "    \n",
    "    # Getting an unrolled set of data batches for the decoder\n",
    "    unrolled_decoder_data, unrolled_decoder_labels, _ = decoder_data_generator.unroll_batches(sent_ids=sent_ids)\n",
    "\n",
    "    # Train for single step\n",
    "    l, tr_pred = train_single_step(unrolled_encoder_data, unrolled_decoder_data, unrolled_decoder_labels)\n",
    "\n",
    "    # Calculate BLEU scores\n",
    "    if np.random.random() < 0.1:\n",
    "\n",
    "        all_labels = np.argmax(np.concatenate(unrolled_decoder_labels,axis=0),axis=1)\n",
    "        all_preds = np.argmax(tr_pred,axis=1)\n",
    "\n",
    "        batch_cands, batch_refs = create_bleu_ref_candidate_lists(all_preds, all_labels)\n",
    "\n",
    "        train_bleu_refs.extend(batch_refs)\n",
    "        train_bleu_cands.extend(batch_cands)\n",
    "\n",
    "    if (step+1)%500==0:\n",
    "\n",
    "        print('Step ',step+1)\n",
    "        with open(os.path.join(log_dir, train_prediction_text_fname),'a') as fa:\n",
    "            fa.write('============= Step ' +  str(step+1) + ' =============\\n')\n",
    "\n",
    "        random_index = np.random.randint(low=1,high=batch_size)\n",
    "        print_and_save_train_predictions(unrolled_decoder_labels, tr_pred, random_index, train_prediction_text_fname)\n",
    "\n",
    "        # Calculating the BLEU score for the accumulated candidates\n",
    "        bscore = 0.0\n",
    "        bscore = corpus_bleu(train_bleu_refs,train_bleu_cands,smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method4)\n",
    "        train_bleu_scores_over_time.append(bscore)\n",
    "        print('(Train) BLEU (%d elements): '%(len(train_bleu_refs)),bscore)\n",
    "\n",
    "        train_bleu_refs, train_bleu_cands = [],[]\n",
    "        with open(log_dir + os.sep +'blue_scores.txt','a') as fa_bleu:\n",
    "            fa_bleu.write(str(step+1) +','+str(bscore)+'\\n')\n",
    "\n",
    "        with open(os.path.join(log_dir, train_prediction_text_fname),'a') as fa:\n",
    "            fa.write('(Train) BLEU: %.5f\\n'%bscore)\n",
    "    \n",
    "    # Update average loss\n",
    "    avg_loss += l\n",
    "\n",
    "    # Resetting hidden state for each batch\n",
    "    sess.run(reset_train_state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c462bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
