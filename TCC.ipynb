{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3bcd51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.10.6\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print('Python version:', python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86547416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Mar 13 17:15:44 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 520.61.05    Driver Version: 520.61.05    CUDA Version: 11.8     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0  On |                  N/A |\n",
      "| 30%   31C    P8    N/A /  75W |    134MiB /  4096MiB |     18%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1556      G   /usr/lib/xorg/Xorg                 74MiB |\n",
      "|    0   N/A  N/A      1713      G   /usr/bin/gnome-shell               20MiB |\n",
      "|    0   N/A  N/A      3099      G   ...176270494694352513,131072       36MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa4b4a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-13 17:15:45.791969: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-13 17:15:46.530478: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-03-13 17:15:47.886612: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-13 17:15:47.886720: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-13 17:15:47.886728: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import zipfile\n",
    "import math\n",
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from matplotlib import pylab\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "import matplotlib\n",
    "import matplotlib.gridspec as gridspec\n",
    "import json\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a0dd06",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "[Dowload](https://nlp.stanford.edu/projects/nmt/):\n",
    "\n",
    "* English vocabulary: [`vocab.50K.en`](https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/vocab.50K.en)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb7635e",
   "metadata": {},
   "source": [
    "### Loading the Datasets and Building the Vocabulary\n",
    "\n",
    "First, we build the vocabulary dictionaries for the source and target (English) language. \n",
    "The vocabularies are found in the file `vocab.50K.en`(English)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18b2e809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary: [('<unk>', 0), ('<s>', 1), ('</s>', 2), ('the', 3), (',', 4), ('.', 5), ('of', 6), ('and', 7), ('to', 8), ('in', 9)]\n",
      "Reverse dictionary: [(0, '<unk>'), (1, '<s>'), (2, '</s>'), (3, 'the'), (4, ','), (5, '.'), (6, 'of'), (7, 'and'), (8, 'to'), (9, 'in')]\n",
      "Vocabulary size:  50000\n"
     ]
    }
   ],
   "source": [
    "# Word string -> ID mapping\n",
    "dictionary = dict()\n",
    "\n",
    "vocabulary_size = len(dictionary)\n",
    "with open('data/vocab.50K.en', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        # disregard the new line aka `\\n`\n",
    "        dictionary[line[:-1]] = len(dictionary)\n",
    "        \n",
    "vocabulary_size = len(dictionary)\n",
    "reverse_dictionary = dict(zip(dictionary.values(),dictionary.keys()))\n",
    "\n",
    "print('Dictionary:', list(dictionary.items())[:10], end = '\\n')\n",
    "print('Reverse dictionary:', list(reverse_dictionary.items())[:10], end = '\\n')\n",
    "print('Vocabulary size: ', vocabulary_size, end = '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1623674",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "Here we load the data from the dataset.csv file (generated in the other script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58d59dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('data/dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ecca3e",
   "metadata": {},
   "source": [
    "### Data pre-processing\n",
    "Transform to lower, remove the new line and the punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42052bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9509/3017365748.py:6: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  .str.replace('.',' .')   \\\n",
      "/tmp/ipykernel_9509/3017365748.py:7: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  .str.replace('?',' ?')   \\\n"
     ]
    }
   ],
   "source": [
    "wt = nltk.tokenize.WhitespaceTokenizer()\n",
    "\n",
    "for column in dataset.columns:\n",
    "    dataset[column] = dataset[column].str.lower() \n",
    "    dataset[column] = dataset[column].str.replace(',', ' ,')  \\\n",
    "                                     .str.replace('.',' .')   \\\n",
    "                                     .str.replace('?',' ?')   \\\n",
    "                                     .str.replace('\\n',' ')\n",
    "    dataset[column] = dataset[column].apply(wt.tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6db800f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[what's, the, purpose, of, life, ?]</td>\n",
       "      <td>[idk, ., try, to, figure, out, how, to, be, ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[what's, the, purpose, of, life, ?]</td>\n",
       "      <td>[the, purpose, of, life, is, to, produce, comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[what's, the, purpose, of, life, ?]</td>\n",
       "      <td>[the, same, purpose, that, the, first, replica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[what's, the, purpose, of, life, ?]</td>\n",
       "      <td>[have, fun, and, help, others, ,, change, the,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[i've, tried, to, quit, smoking, ,, this, is, ...</td>\n",
       "      <td>[\"i, smoked, for, 12y, then, stopped, ., it, a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0                [what's, the, purpose, of, life, ?]   \n",
       "1                [what's, the, purpose, of, life, ?]   \n",
       "2                [what's, the, purpose, of, life, ?]   \n",
       "3                [what's, the, purpose, of, life, ?]   \n",
       "4  [i've, tried, to, quit, smoking, ,, this, is, ...   \n",
       "\n",
       "                                              answer  \n",
       "0  [idk, ., try, to, figure, out, how, to, be, ha...  \n",
       "1  [the, purpose, of, life, is, to, produce, comp...  \n",
       "2  [the, same, purpose, that, the, first, replica...  \n",
       "3  [have, fun, and, help, others, ,, change, the,...  \n",
       "4  [\"i, smoked, for, 12y, then, stopped, ., it, a...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada21a8a",
   "metadata": {},
   "source": [
    "### Data analysis\n",
    "Mean sentence length and standard deviation of sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f68588d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Questions) Average sentence length:  17.103324600337082\n",
      "(Questions) Standard deviation of sentence length:  9.124359472017858\n",
      "(Answers) Average sentence length:  54.53903092455196\n",
      "(Answers) Standard deviation of sentence length:  843.2193542807091\n"
     ]
    }
   ],
   "source": [
    "print('(Questions) Average sentence length: ', dataset['question'].str.len().mean())\n",
    "print('(Questions) Standard deviation of sentence length: ', dataset['question'].str.len().std())\n",
    "\n",
    "print('(Answers) Average sentence length: ', dataset['answer'].str.len().mean())\n",
    "print('(Answers) Standard deviation of sentence length: ', dataset['answer'].str.len().std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbe4985",
   "metadata": {},
   "source": [
    "### Update the sentences to fixed length\n",
    "Update all sentences with a fixed size, to process the sentences as batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02d6bef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum sentence length\n",
    "max_sent_length = {'question' : 30, 'answer': 70}\n",
    "\n",
    "for column in dataset.columns:\n",
    "    for tokens in dataset[column]: \n",
    "        \n",
    "        # adding the start token\n",
    "        tokens.insert(0, '<s>')\n",
    "        \n",
    "        if len(tokens) >= max_sent_length[column]:\n",
    "            tokens = tokens[:max_sent_length[column] - 1]\n",
    "            tokens.append('</s>')\n",
    "            \n",
    "        if len(tokens) < max_sent_length[column]:\n",
    "            tokens.extend(['</s>' for _ in range(max_sent_length[column] - len(tokens))])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
