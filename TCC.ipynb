{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86547416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Mar 27 16:32:51 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 520.61.05    Driver Version: 520.61.05    CUDA Version: 11.8     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0  On |                  N/A |\n",
      "| 30%   26C    P8    N/A /  75W |    308MiB /  4096MiB |     11%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1557      G   /usr/lib/xorg/Xorg                102MiB |\n",
      "|    0   N/A  N/A      1723      G   /usr/bin/gnome-shell              120MiB |\n",
      "|    0   N/A  N/A      3912      G   ...166383460551712268,131072       83MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa4b4a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "from matplotlib import pylab\n",
    "import matplotlib\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib inline\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from sklearn.utils import shuffle\n",
    "import word2vec\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a4ff641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.python.util.deprecation as deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a0dd06",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "[Dowload](https://nlp.stanford.edu/projects/nmt/):\n",
    "\n",
    "* English vocabulary: [`vocab.50K.en`](https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/vocab.50K.en)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb7635e",
   "metadata": {},
   "source": [
    "### Loading the Datasets and Building the Vocabulary\n",
    "\n",
    "First, we build the vocabulary dictionaries for the source and target (English) language. \n",
    "The vocabularies are found in the file `vocab.50K.en`(English)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18b2e809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary: [('<unk>', 0), ('<s>', 1), ('</s>', 2), ('the', 3), (',', 4), ('.', 5), ('of', 6), ('and', 7), ('to', 8), ('in', 9)]\n",
      "Reverse dictionary: [(0, '<unk>'), (1, '<s>'), (2, '</s>'), (3, 'the'), (4, ','), (5, '.'), (6, 'of'), (7, 'and'), (8, 'to'), (9, 'in')]\n",
      "Vocabulary size:  50000\n"
     ]
    }
   ],
   "source": [
    "# Word string -> ID mapping\n",
    "dictionary = dict()\n",
    "\n",
    "vocabulary_size = len(dictionary)\n",
    "with open('data/vocab.50K.en', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        # disregard the new line aka `\\n`\n",
    "        dictionary[line[:-1]] = len(dictionary)\n",
    "        \n",
    "vocabulary_size = len(dictionary)\n",
    "reverse_dictionary = dict(zip(dictionary.values(),dictionary.keys()))\n",
    "\n",
    "print('Dictionary:', list(dictionary.items())[:10], end = '\\n')\n",
    "print('Reverse dictionary:', list(reverse_dictionary.items())[:10], end = '\\n')\n",
    "print('Vocabulary size: ', vocabulary_size, end = '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1623674",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "Here we load the data from the dataset.csv file (generated in the other script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58d59dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('data/dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ecca3e",
   "metadata": {},
   "source": [
    "### Data pre-processing\n",
    "Transform to lower, remove the new line and the punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42052bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wt = nltk.tokenize.WhitespaceTokenizer()\n",
    "\n",
    "for column in dataset.columns:\n",
    "    dataset[column] = dataset[column].str.lower() \n",
    "    dataset[column] = dataset[column].str.replace(',', ' ,')  \\\n",
    "                                     .str.replace('.',' .', regex=False)   \\\n",
    "                                     .str.replace('?',' ?', regex=False)   \\\n",
    "                                     .str.replace(')','', regex=False)   \\\n",
    "                                     .str.replace('(','', regex=False)   \\\n",
    "                                     .str.replace('\"','')   \\\n",
    "                                     .str.replace('\\n',' ')\n",
    "    dataset[column] = dataset[column].apply(wt.tokenize)\n",
    "dataset = shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6db800f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>966889</th>\n",
       "      <td>[reddit, ,, what, things, do, you, enjoy, that...</td>\n",
       "      <td>[i, enjoyed, spider-man, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993139</th>\n",
       "      <td>[american, redditors:, if, you, own, one, ,, w...</td>\n",
       "      <td>[america, is, a, great, country, but, it, has,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371397</th>\n",
       "      <td>[whitout, saying, ,, where, are, you, from, ?]</td>\n",
       "      <td>[america, hates, us, and, our, government, is,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028614</th>\n",
       "      <td>[reddit, ,, what, do, you, look, like, ?]</td>\n",
       "      <td>[here's, me, at, my, college's, cafeteria, ., ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096571</th>\n",
       "      <td>[men, of, reddit, ,, what, female, celebrity's...</td>\n",
       "      <td>[the, entire, big, butt, phenomena, it's, abou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  question  \\\n",
       "966889   [reddit, ,, what, things, do, you, enjoy, that...   \n",
       "993139   [american, redditors:, if, you, own, one, ,, w...   \n",
       "371397      [whitout, saying, ,, where, are, you, from, ?]   \n",
       "1028614          [reddit, ,, what, do, you, look, like, ?]   \n",
       "1096571  [men, of, reddit, ,, what, female, celebrity's...   \n",
       "\n",
       "                                                    answer  \n",
       "966889                         [i, enjoyed, spider-man, 3]  \n",
       "993139   [america, is, a, great, country, but, it, has,...  \n",
       "371397   [america, hates, us, and, our, government, is,...  \n",
       "1028614  [here's, me, at, my, college's, cafeteria, ., ...  \n",
       "1096571  [the, entire, big, butt, phenomena, it's, abou...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada21a8a",
   "metadata": {},
   "source": [
    "### Data analysis\n",
    "Mean sentence length and standard deviation of sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f68588d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Questions) Average sentence length:  17.101486059545056\n",
      "(Questions) Standard deviation of sentence length:  9.122891352194078\n",
      "(Answers) Average sentence length:  54.367627238247216\n",
      "(Answers) Standard deviation of sentence length:  843.0636308326159\n"
     ]
    }
   ],
   "source": [
    "print('(Questions) Average sentence length: ', dataset['question'].str.len().mean())\n",
    "print('(Questions) Standard deviation of sentence length: ', dataset['question'].str.len().std())\n",
    "\n",
    "print('(Answers) Average sentence length: ', dataset['answer'].str.len().mean())\n",
    "print('(Answers) Standard deviation of sentence length: ', dataset['answer'].str.len().std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbe4985",
   "metadata": {},
   "source": [
    "### Update the sentences to fixed length\n",
    "Update all sentences with a fixed size, to process the sentences as batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44f77928",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sent_length = {'question' : 30, 'answer': 70}\n",
    "\n",
    "def padding_sent(source):\n",
    "    padded = []\n",
    "    for tokens in dataset[source]: \n",
    "        # adding the start token\n",
    "        tokens.insert(0, '<s>')  \n",
    "\n",
    "        if len(tokens) >= max_sent_length[source]:\n",
    "            tokens = tokens[:(max_sent_length[source] - 1)]\n",
    "            tokens.append('</s>')\n",
    "\n",
    "        if len(tokens) < max_sent_length[source]:\n",
    "            tokens.extend(['</s>' for _ in range(max_sent_length[source] - len(tokens))])  \n",
    "\n",
    "        padded.append(tokens)\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cc61866",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = padding_sent('question')\n",
    "answers = padding_sent('answer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0924d3d3",
   "metadata": {},
   "source": [
    "### Create the reverse dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7ad51f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reverse_dataset(source):\n",
    "    reverse_tokens = []\n",
    "    reverse_dataset = []\n",
    "    for tokens in source: \n",
    "        for token in tokens: \n",
    "            if token not in dictionary.keys():\n",
    "                reverse_tokens.append(dictionary['<unk>'])\n",
    "            else:\n",
    "                reverse_tokens.append(dictionary[token])\n",
    "        reverse_dataset.append(reverse_tokens)\n",
    "        reverse_tokens = []\n",
    "    return reverse_dataset\n",
    "\n",
    "train_inputs =  np.array(create_reverse_dataset(questions), dtype=np.int32)\n",
    "train_outputs =  np.array(create_reverse_dataset(answers), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467bf5ec",
   "metadata": {},
   "source": [
    "### Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c1c2d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_cursors = [0 for _ in range(train_inputs.shape[0])]\n",
    "batch_size = 32\n",
    "embedding_size = 64\n",
    "steps = 80000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00e9857b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "with window_size = 2:\n",
      "    batch: [['<s>', 'former', 'of', '<unk>'], ['<s>', '<unk>', 'favourite', 'light'], ['<s>', 'postal', 'of', '<unk>'], ['<s>', 'as', 'kid', ','], ['<s>', 'you', 'throw', '10000'], ['<s>', 'how', 'you', 'feel'], ['<s>', 'where', 'you', 'put'], ['<s>', 'when', 'the', 'absolute']]\n",
      "    labels: ['smokers', 'your', 'workers', 'a', 'can', 'would', 'do', 'was']\n",
      "Defining 4 embedding lookups representing each word in the context\n",
      "Stacked embedding size: [32, 64, 4]\n",
      "Reduced mean embedding size: [32, 64]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-20 14:31:47.258978: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-20 14:31:47.306573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 14:31:47.369285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 14:31:47.369851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 14:31:47.802758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 14:31:47.802977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 14:31:47.803119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 14:31:47.803228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2606 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "2023-03-20 14:31:47.809391: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 2000: 2.779788\n",
      "Average loss at step 4000: 1.444825\n",
      "Average loss at step 6000: 1.281724\n",
      "Average loss at step 8000: 1.185413\n",
      "Average loss at step 10000: 1.129574\n",
      "Nearest to should: did, constituent, would, husbands, Derrida, does, well, greetings,\n",
      "Nearest to -: ,, women, instead, fellow, cooled, cops, housed, Bible,\n",
      "Nearest to also: parents, dopo, deceiving, CAM, Elche, S.p.A., walked, leases,\n",
      "Nearest to The: Alejandro, presses, pioneered, Absolute, Berlaymont, devoid, Excellent, multiple,\n",
      "Nearest to these: 1821, sparking, satisfying, MacDonald, www.avaaz.org, Jo, collateral, Rhapsody,\n",
      "Nearest to us: <unk>, worst, for, indemnify, longest, apple, alright, ridiculous,\n",
      "Nearest to out: sleep, indelible, prayer, fell, live, all, vs, meu,\n",
      "Nearest to or: Poos, biocidal, parody, Multitude, object, decides, Immanuel, Link,\n",
      "Nearest to by: Advocates, GfK, Sealed, ERC, 3.7, jour, 6.30, tabling,\n",
      "Nearest to as: video, movie, person, Procchio, girl, scale, song, weird,\n",
      "Nearest to latest: Scouting, Vigilio, worst, comeback, architect, insult, undulating, Büro,\n",
      "Nearest to brought: how, bullied, lavatory, reverse, legislatures, Regiment, Angaben, Characteristic,\n",
      "Nearest to size: Terry, Retoucher, action, bronchitis, mythical, contemplated, Jakob, EFSF,\n",
      "Nearest to sound: untenable, Localization, Millions, scales, acknowledgment, sank, safest, painful,\n",
      "Nearest to Central: cabbage, dado, Synergy, Assault, attenuation, ideale, Actions, referral,\n",
      "Nearest to supported: metropolitan, nomads, jpeg, Hosting, Browsing, immune, said, OTHER,\n",
      "Nearest to independent: Probe, sectoral, Agora, ICEcat, Wegener, dodge, 24h, Inquiry,\n",
      "Nearest to land: SOUND, refinements, exchanger, supplanted, eutrophication, 307, endorse, went,\n",
      "Nearest to search: mainframe, chute, Ironforge, Lakeside, anger, sunburn, eagerness, relegated,\n",
      "Nearest to rates: Whether, aeronautics, magazin, Watts, dominate, Chess, skates, Suliban,\n",
      "Average loss at step 12000: 1.093283\n",
      "Average loss at step 14000: 1.067507\n",
      "Average loss at step 16000: 1.049263\n",
      "Average loss at step 18000: 1.047472\n",
      "Average loss at step 20000: 1.040354\n",
      "Nearest to should: would, did, can, realised, mysql, Espace, constituent, warming,\n",
      "Nearest to -: fellow, mall, ap, un, Bible, chefs, ,, walks,\n",
      "Nearest to also: walked, granted, parents, dopo, deceiving, hit, given, Of,\n",
      "Nearest to The: Alejandro, presses, pioneered, Absolute, Berlaymont, devoid, Excellent, emits,\n",
      "Nearest to these: 1821, collateral, satisfying, Rhapsody, MacDonald, Holstein, Jamaica, doorway,\n",
      "Nearest to us: <unk>, indemnify, longest, actual, appropriate, Coriantumr, overnight, winding,\n",
      "Nearest to out: fell, sleep, live, indelible, all, lived, prayer, meu,\n",
      "Nearest to or: Poos, decides, Multitude, object, adjournment, biocidal, inapplicable, parody,\n",
      "Nearest to by: Advocates, GfK, Sealed, sides, daran, moments, analyzed, 6.30,\n",
      "Nearest to as: Procchio, finished, CHILD, huge, Orion, girl, CDA, picking,\n",
      "Nearest to latest: worst, insult, Scouting, Vigilio, comeback, biggest, town, 2050,\n",
      "Nearest to brought: bullied, 17, tonight, want, how, lavatory, Characteristic, legislatures,\n",
      "Nearest to size: Retoucher, Terry, alerting, action, bronchitis, mythical, Na, rife,\n",
      "Nearest to sound: untenable, Localization, Worms, painful, safest, upmarket, harsh, grab,\n",
      "Nearest to Central: dado, Synergy, cabbage, Assault, attenuation, ideale, Actions, referral,\n",
      "Nearest to supported: metropolitan, nomads, jpeg, immune, Browsing, Mato, Hosting, Laotian,\n",
      "Nearest to independent: Probe, sectoral, Agora, 24h, biker, ICEcat, dodge, Stream,\n",
      "Nearest to land: SOUND, refinements, bored, endorse, eutrophication, custodians, went, very,\n",
      "Nearest to search: mainframe, Lakeside, Ironforge, linkages, eagerness, chute, lingua, instructors,\n",
      "Nearest to rates: aeronautics, Whether, magazin, renminbi, Chess, Watts, dominate, Suliban,\n",
      "Average loss at step 22000: 1.029904\n",
      "Average loss at step 24000: 1.034981\n",
      "Average loss at step 26000: 1.030983\n",
      "Average loss at step 28000: 1.020353\n",
      "Average loss at step 30000: 1.044761\n",
      "Nearest to should: would, did, can, will, mysql, could, warming, realised,\n",
      "Nearest to -: fellow, ,, Maritim, mall, ap, chemists, bosses, :,\n",
      "Nearest to also: walked, moved, granted, tasked, thoughts, fiber, dopo, deceiving,\n",
      "Nearest to The: presses, Alejandro, pioneered, Absolute, Berlaymont, devoid, Excellent, emits,\n",
      "Nearest to these: 1821, doorway, Holstein, routinely, Along, Rhapsody, F1, slowly,\n",
      "Nearest to us: indemnify, Coriantumr, <unk>, handicraft, emotional, equitably, tasting, usa,\n",
      "Nearest to out: fell, indelible, traveled, lived, 8, all, 6th, Benghazi,\n",
      "Nearest to or: Poos, Multitude, DVI, Payments, adjournment, gestellt, inapplicable, Nueva,\n",
      "Nearest to by: Advocates, GfK, on, realistic, Sealed, daran, boards, summoned,\n",
      "Nearest to as: CHILD, Orion, Procchio, certain, finished, Lopar, manner, child,\n",
      "Nearest to latest: worst, 2050, Scouting, darkest, Vigilio, onion, equivalent, comeback,\n",
      "Nearest to brought: 17, honestly, bullied, tonight, come, legislatures, want, tried,\n",
      "Nearest to size: Retoucher, alerting, Terry, college, Teatro, mythical, action, bronchitis,\n",
      "Nearest to sound: untenable, Localization, Worms, opposite, upmarket, heinous, event, painful,\n",
      "Nearest to Central: dado, Synergy, Assault, attenuation, ideale, Actions, referral, boarded,\n",
      "Nearest to supported: metropolitan, nomads, Mato, jpeg, Laotian, immune, Hosting, longtime,\n",
      "Nearest to independent: Probe, sectoral, Agora, Stream, periodically, 24h, biker, dodge,\n",
      "Nearest to land: bored, SOUND, refinements, went, endorse, homosexual, custodians, walking,\n",
      "Nearest to search: Lakeside, lingua, mainframe, Ironforge, relegated, awake, linkages, gravel,\n",
      "Nearest to rates: aeronautics, Whether, Suliban, renminbi, magazin, dominate, testimonials, Chess,\n",
      "Average loss at step 32000: 1.023770\n",
      "Average loss at step 34000: 1.041398\n",
      "Average loss at step 36000: 1.042854\n",
      "Average loss at step 38000: 1.027680\n",
      "Average loss at step 40000: 1.037344\n",
      "Nearest to should: would, can, did, will, could, mysql, detailled, Espace,\n",
      "Nearest to -: ,, ap, :, coaches, ethics, Maritim, 80th, bosses,\n",
      "Nearest to also: walked, involved, moved, arising, fiber, sirens, abducted, thoughts,\n",
      "Nearest to The: presses, Alejandro, pioneered, Absolute, Berlaymont, devoid, Excellent, colonized,\n",
      "Nearest to these: Doctor, F1, Along, routinely, contravention, 1821, 0044, www.avaaz.org,\n",
      "Nearest to us: indemnify, usa, emotional, handicraft, Coriantumr, spills, equitably, Friesland,\n",
      "Nearest to out: fell, indelible, 8, dipping, Benghazi, realised, Documentation, Bregenzerwald,\n",
      "Nearest to or: Poos, Multitude, Payments, Nueva, indemnity, gestellt, impatience, inapplicable,\n",
      "Nearest to by: Advocates, GfK, on, realistic, boards, dismayed, robbed, predefined,\n",
      "Nearest to as: Orion, CHILD, Teheran, Brähler, rebound, bind, Eugen, Lopar,\n",
      "Nearest to latest: onion, 2050, town, smallest, darkest, removing, Scouting, temperature,\n",
      "Nearest to brought: tried, honestly, 17, come, bullied, Sad, contrast, Characteristic,\n",
      "Nearest to size: alerting, Retoucher, mythical, Terry, Teatro, bronchitis, grinding, action,\n",
      "Nearest to sound: untenable, Localization, Worms, upmarket, heinous, opposite, event, Lisboa,\n",
      "Nearest to Central: dado, Synergy, Assault, attenuation, ideale, Actions, antioxidants, referral,\n",
      "Nearest to supported: metropolitan, nomads, Mato, Laotian, jpeg, immune, soundtracks, Browsing,\n",
      "Nearest to independent: Probe, 24h, Agora, periodically, 1923, sectoral, Stream, biker,\n",
      "Nearest to land: SOUND, bored, utmost, refinements, doorway, homosexual, went, custodians,\n",
      "Nearest to search: Lakeside, lingua, Ironforge, eagerness, relegated, shouting, linkages, gravel,\n",
      "Nearest to rates: aeronautics, Whether, renminbi, magazin, Or, Suliban, dominate, Chess,\n",
      "Average loss at step 42000: 1.040096\n",
      "Average loss at step 44000: 1.037316\n",
      "Average loss at step 46000: 1.070902\n",
      "Average loss at step 48000: 1.063058\n",
      "Average loss at step 50000: 1.059402\n",
      "Nearest to should: can, would, did, could, will, mysql, detailled, does,\n",
      "Nearest to -: ,, :, finances, ethics, Maritim, hillside, nightclub, coaches,\n",
      "Nearest to also: sirens, involved, walked, Pond, fiber, commentaries, Comparative, arising,\n",
      "Nearest to The: presses, Alejandro, pioneered, Absolute, colonized, devoid, Berlaymont, Excellent,\n",
      "Nearest to these: Doctor, Along, F1, 0044, Linguistic, Uniform, 50, contravention,\n",
      "Nearest to us: usa, emotional, insbesondere, handicraft, indemnify, spirit, <unk>, Coriantumr,\n",
      "Nearest to out: fell, Documentation, 8, indelible, soir, dipping, tempted, Benghazi,\n",
      "Nearest to or: Poos, Multitude, DVI, Payments, impatience, Valencia, Nueva, indemnity,\n",
      "Nearest to by: Advocates, GfK, on, dismayed, predefined, realistic, data, daran,\n",
      "Nearest to as: Orion, CHILD, rebound, Teheran, bind, Eugen, Brähler, Preliminary,\n",
      "Nearest to latest: onion, temperature, town, smallest, rarest, outcome, removing, meaning,\n",
      "Nearest to brought: tried, greatly, done, honestly, gotten, terrified, arguing, according,\n",
      "Nearest to size: Terry, alerting, Retoucher, bronchitis, Teatro, mythical, Plain, whet,\n",
      "Nearest to sound: untenable, upmarket, opposite, Localization, Worms, heat, flawed, heinous,\n",
      "Nearest to Central: dado, Synergy, Assault, attenuation, ideale, Actions, antioxidants, referral,\n",
      "Nearest to supported: metropolitan, nomads, Mato, soundtracks, Laotian, jpeg, Hosting, longtime,\n",
      "Nearest to independent: Probe, 1923, Agora, periodically, Stream, 24h, imprisoned, sectoral,\n",
      "Nearest to land: bored, SOUND, refinements, doorway, utmost, walking, homosexual, went,\n",
      "Nearest to search: Lakeside, relegated, eagerness, recent, shouting, monsoon, Ironforge, lingua,\n",
      "Nearest to rates: aeronautics, Whether, Or, magazin, Chess, partner, renminbi, dominate,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 52000: 1.066826\n",
      "Average loss at step 54000: 1.067815\n",
      "Average loss at step 56000: 1.074104\n",
      "Average loss at step 58000: 1.076034\n",
      "Average loss at step 60000: 1.071978\n",
      "Nearest to should: can, would, did, could, will, does, detailled, Vejer,\n",
      "Nearest to -: ,, :, finances, nightclub, coaches, Maritim, hillside, 80th,\n",
      "Nearest to also: sirens, oral, involved, arising, commentaries, clic, visited, dopo,\n",
      "Nearest to The: Alejandro, presses, pioneered, colonized, Absolute, devoid, Berlaymont, Excellent,\n",
      "Nearest to these: Doctor, F1, Along, 0044, contravention, 50, Ideally, Linguistic,\n",
      "Nearest to us: usa, insbesondere, handicraft, emotional, jealousy, tumours, Friesland, indemnify,\n",
      "Nearest to out: Documentation, 8, soir, Exif, Byzantine, indelible, fell, tempted,\n",
      "Nearest to or: Poos, Valencia, DVI, impatience, Multitude, Payments, Nueva, zero,\n",
      "Nearest to by: Advocates, on, GfK, dismayed, predefined, data, realistic, daran,\n",
      "Nearest to as: Orion, Eugen, rebound, Teheran, CHILD, bind, Lopar, Preliminary,\n",
      "Nearest to latest: onion, rarest, town, simplest, removing, outcome, temperature, ultimate,\n",
      "Nearest to brought: tried, terrified, according, aspire, taught, fill, access, collect,\n",
      "Nearest to size: Terry, alerting, Teatro, bronchitis, grinding, Plain, nett, música,\n",
      "Nearest to sound: untenable, upmarket, flawed, heat, opposite, Worms, Localization, heinous,\n",
      "Nearest to Central: dado, Synergy, Assault, antioxidants, attenuation, ideale, Actions, referral,\n",
      "Nearest to supported: metropolitan, nomads, Mato, soundtracks, Laotian, jpeg, immune, longtime,\n",
      "Nearest to independent: Probe, 1923, Agora, periodically, Stream, imprisoned, sectoral, 24h,\n",
      "Nearest to land: bored, SOUND, doorway, homosexual, Gaudi, went, utmost, refinements,\n",
      "Nearest to search: Lakeside, person, eagerness, monsoon, thanks, recent, opportune, relegated,\n",
      "Nearest to rates: aeronautics, Or, partner, Whether, magazin, Chess, riders, renminbi,\n",
      "Average loss at step 62000: 1.067349\n",
      "Average loss at step 64000: 1.083989\n",
      "Average loss at step 66000: 1.085574\n",
      "Average loss at step 68000: 1.089473\n",
      "Average loss at step 70000: 1.085248\n",
      "Nearest to should: can, would, did, could, might, will, does, detailled,\n",
      "Nearest to -: ,, :, nightclub, finances, 80th, enthralled, coaches, NYC,\n",
      "Nearest to also: sirens, oral, Pond, commentaries, involved, arising, deepens, reviews,\n",
      "Nearest to The: colonized, Alejandro, pioneered, presses, Absolute, devoid, Berlaymont, Excellent,\n",
      "Nearest to these: F1, Doctor, Along, 50, contravention, Ideally, water-, loop,\n",
      "Nearest to us: usa, insbesondere, tumours, handicraft, jealousy, alterations, ze, bassist,\n",
      "Nearest to out: 8, Documentation, soir, Exif, ANSI, Byzantine, into, Kouchner,\n",
      "Nearest to or: Valencia, impatience, DVI, Multitude, Payments, zero, Poos, Belle,\n",
      "Nearest to by: Advocates, GfK, on, dismayed, predefined, data, Bowling, identically,\n",
      "Nearest to as: Orion, Eugen, rebound, Teheran, Preliminary, Lopar, bind, CHILD,\n",
      "Nearest to latest: onion, rarest, removing, outcome, earliest, shortest, simplest, newest,\n",
      "Nearest to brought: tried, aspire, collect, according, arguing, taught, done, terrified,\n",
      "Nearest to size: Teatro, grinding, música, alerting, Plain, nett, moroccan, audience,\n",
      "Nearest to sound: untenable, upmarket, flawed, opposite, Worms, regulars, Localization, convenient,\n",
      "Nearest to Central: dado, Synergy, Assault, antioxidants, attenuation, ideale, Actions, referral,\n",
      "Nearest to supported: metropolitan, Mato, attempted, soundtracks, hacked, proposed, nomads, bathrooms,\n",
      "Nearest to independent: Probe, hey, imprisoned, 1923, Agora, periodically, apparently, ΝΑΤΟ,\n",
      "Nearest to land: bored, doorway, SOUND, see, Gaudi, went, 7,000, Toward,\n",
      "Nearest to search: opportune, monsoon, thanks, eagerness, memorable, relegated, Lakeside, anger,\n",
      "Nearest to rates: aeronautics, Whether, Or, magazin, partner, Chess, riders, runaway,\n",
      "Average loss at step 72000: 1.090024\n",
      "Average loss at step 74000: 1.094442\n",
      "Average loss at step 76000: 1.099556\n",
      "Average loss at step 78000: 1.096344\n",
      "Average loss at step 80000: 1.113771\n",
      "Nearest to should: can, would, might, did, could, cant, Vejer, will,\n",
      "Nearest to -: ,, :, nightclub, finances, enthralled, NYC, Algeria, hillside,\n",
      "Nearest to also: sirens, oral, worry, arising, deepens, involved, craft, reviews,\n",
      "Nearest to The: colonized, Alejandro, presses, pioneered, Absolute, devoid, earn, Berlaymont,\n",
      "Nearest to these: F1, Along, Ideally, Doctor, 50, loop, contravention, water-,\n",
      "Nearest to us: usa, insbesondere, jealousy, bassist, tumours, ravaged, handicraft, united,\n",
      "Nearest to out: soir, Documentation, Exif, Apostle, Byzantine, 8, ANSI, into,\n",
      "Nearest to or: Valencia, DVI, Belle, impatience, leniency, Party, toch, Payments,\n",
      "Nearest to by: Advocates, GfK, on, predefined, dismayed, narrow, data, crossroad,\n",
      "Nearest to as: Eugen, Preliminary, Lopar, Teheran, Orion, quantitatively, rebound, Peaks,\n",
      "Nearest to latest: onion, rarest, temperature, earliest, outcome, removing, shortest, simplest,\n",
      "Nearest to brought: tried, taught, access, according, aspire, arguing, collect, extent,\n",
      "Nearest to size: audience, grinding, dangers, música, Teatro, euphemism, nett, downfall,\n",
      "Nearest to sound: untenable, flawed, upmarket, copied, normalise, regulars, Localization, Worms,\n",
      "Nearest to Central: dado, Synergy, antioxidants, Assault, attenuation, ideale, Actions, referral,\n",
      "Nearest to supported: Mato, proposed, attempted, nomads, Hosting, metropolitan, longtime, immune,\n",
      "Nearest to independent: Probe, 1923, Agora, Nespresso, ΝΑΤΟ, periodically, mattresses, hey,\n",
      "Nearest to land: bored, doorway, SOUND, 7,000, see, Toward, rebel, five,\n",
      "Nearest to search: opportune, anger, monsoon, memorable, buddies, relegated, thanks, shouting,\n",
      "Nearest to rates: aeronautics, Whether, magazin, Or, renminbi, runaway, Chess, Penken,\n",
      "Average loss at step 82000: 1.113116\n",
      "Average loss at step 84000: 1.120602\n",
      "Average loss at step 86000: 1.113074\n",
      "Average loss at step 88000: 1.122525\n",
      "Average loss at step 90000: 1.124206\n",
      "Nearest to should: can, might, would, could, did, does, Vejer, will,\n",
      "Nearest to -: ,, :, finances, nightclub, enthralled, NYC, satirical, Maritim,\n",
      "Nearest to also: sirens, oral, deepens, offended, arising, craft, Estimates, involved,\n",
      "Nearest to The: colonized, Alejandro, presses, Absolute, pioneered, earn, devoid, Berlaymont,\n",
      "Nearest to these: F1, Doctor, Along, 50, Ideally, contravention, 195, loop,\n",
      "Nearest to us: usa, insbesondere, united, ravaged, bassist, tumours, jealousy, ze,\n",
      "Nearest to out: soir, Documentation, ANSI, Byzantine, Exif, into, Kouchner, pickpockets,\n",
      "Nearest to or: Valencia, DVI, Belle, leniency, Party, impatience, toch, Payments,\n",
      "Nearest to by: Advocates, GfK, predefined, narrow, on, data, crossroad, dismayed,\n",
      "Nearest to as: Eugen, Preliminary, Teheran, quantitatively, archaeologist, Liebe, Orion, Peaks,\n",
      "Nearest to latest: rarest, onion, whole, earliest, temperature, newest, outcome, shortest,\n",
      "Nearest to brought: tried, taught, access, aspire, collect, according, purchased, motivated,\n",
      "Nearest to size: audience, dangers, spirit, downfall, location, Teatro, euphemism, música,\n",
      "Nearest to sound: untenable, upmarket, flawed, normalise, regulars, heinous, elephant, opposite,\n",
      "Nearest to Central: dado, antioxidants, Synergy, Assault, attenuation, ideale, Actions, referral,\n",
      "Nearest to supported: Mato, proposed, attempted, 80th, metropolitan, endowed, Hosting, longtime,\n",
      "Nearest to independent: Probe, 1923, 24h, Agora, imprisoned, hey, periodically, Nespresso,\n",
      "Nearest to land: bored, doorway, 7,000, rebel, SOUND, split, heat, Toward,\n",
      "Nearest to search: opportune, anger, monsoon, memorable, shouting, relegated, buddies, Lithuanian,\n",
      "Nearest to rates: Whether, aeronautics, magazin, Penken, renminbi, Or, runaway, Chess,\n",
      "Average loss at step 92000: 1.131230\n",
      "Average loss at step 94000: 1.129633\n",
      "Average loss at step 96000: 1.124089\n",
      "Average loss at step 98000: 1.144291\n",
      "Average loss at step 100000: 1.134075\n",
      "Nearest to should: can, might, would, could, does, did, will, detailled,\n",
      "Nearest to -: ,, :, nightclub, enthralled, --, finances, satirical, NYC,\n",
      "Nearest to also: sirens, deepens, oral, Clooney, Brett, Tamils, stitchers, Pond,\n",
      "Nearest to The: colonized, Alejandro, presses, Absolute, devoid, earn, Berlaymont, pioneered,\n",
      "Nearest to these: F1, 50, Ideally, Along, 195, contravention, Doctor, water-,\n",
      "Nearest to us: usa, insbesondere, united, bassist, ravaged, Plan, tumours, ze,\n",
      "Nearest to out: soir, Documentation, into, ANSI, Kouchner, Exif, Byzantine, Ones,\n",
      "Nearest to or: Valencia, DVI, Belle, leniency, Party, impatience, toch, restlessness,\n",
      "Nearest to by: Advocates, GfK, narrow, crossroad, predefined, on, tortured, dismayed,\n",
      "Nearest to as: Eugen, Preliminary, Teheran, archaeologist, quantitatively, Liebe, rebound, Orion,\n",
      "Nearest to latest: newest, earliest, rarest, temperature, ultimate, clown, longest, whole,\n",
      "Nearest to brought: tried, taught, collect, motivated, ruined, access, according, Fourteen,\n",
      "Nearest to size: audience, dangers, spirit, opposite, música, downfall, pinnacle, location,\n",
      "Nearest to sound: normalise, untenable, upmarket, heinous, flawed, elephant, Fathers, brew,\n",
      "Nearest to Central: dado, antioxidants, Synergy, Assault, attenuation, ideale, Actions, referral,\n",
      "Nearest to supported: Mato, attempted, proposed, 80th, Hosting, endowed, nomads, hacked,\n",
      "Nearest to independent: imprisoned, Probe, 1923, hey, 22, Agora, k, apparently,\n",
      "Nearest to land: bored, 7,000, doorway, split, heat, rebel, benefits, end,\n",
      "Nearest to search: opportune, anger, shouting, relegated, buddies, monsoon, resolution, memorable,\n",
      "Nearest to rates: aeronautics, Whether, renminbi, magazin, Penken, riders, Or, Suliban,\n"
     ]
    }
   ],
   "source": [
    "word2vec.define_data_and_hyperparameters(\n",
    "        train_inputs.shape[0], \n",
    "        max_sent_length['question'], \n",
    "        max_sent_length['answer'], \n",
    "        dictionary, \n",
    "        reverse_dictionary,  \n",
    "        train_inputs, \n",
    "        train_outputs, \n",
    "        embedding_size,\n",
    "        vocabulary_size)\n",
    "\n",
    "word2vec.print_some_batches()\n",
    "word2vec.define_word2vec_tensorflow(batch_size)\n",
    "word2vec.run_word2vec(batch_size, steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bc51f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data\n",
      "['<unk>', 'american', '<unk>', '<unk>', 'men']\n",
      "[',', '<unk>', 'saying', ',', 'of']\n",
      "['what', 'if', ',', 'what', '<unk>']\n",
      "['things', 'you', 'where', 'do', ',']\n",
      "['do', 'own', 'are', 'you', 'what']\n",
      "['you', 'one', 'you', 'look', 'female']\n",
      "['enjoy', ',', 'from', 'like', '<unk>']\n",
      "['that', 'why', '?', '?', 'appearance']\n",
      "['you', 'did', '</s>', '</s>', 'is']\n",
      "['believe', 'you', '</s>', '</s>', 'highly']\n",
      "['<unk>', 'buy', '</s>', '</s>', 'exaggerated']\n",
      "['would', 'a', '</s>', '</s>', '?']\n",
      "['judge', 'gun', '</s>', '</s>', '</s>']\n",
      "['you', '?', '</s>', '</s>', '</s>']\n",
      "['for', '</s>', '</s>', '</s>', '</s>']\n",
      "['?', '</s>', '</s>', '</s>', '</s>']\n",
      "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "\n",
      "Output data batch\n",
      "['i', '<unk>', '<unk>', '<unk>', 'the']\n",
      "['enjoyed', 'is', 'hates', 'me', 'entire']\n",
      "['<unk>', 'a', 'us', 'at', 'big']\n",
      "['3', 'great', 'and', 'my', 'butt']\n",
      "['</s>', 'country', 'our', '<unk>', 'phenomena']\n",
      "['</s>', 'but', 'government', 'cafeteria', '<unk>']\n",
      "['</s>', 'it', 'is', '.', 'about']\n",
      "['</s>', 'has', 'filled', 'i', 'proportions']\n",
      "['</s>', 'many', 'with', '<unk>', 'people']\n",
      "['</s>', 'enemies', '<unk>', 'really', '.']\n",
      "['</s>', '.', 'idiots', 'that', '</s>']\n",
      "['</s>', 'the', 'its', 'happy', '</s>']\n",
      "['</s>', 'british', 'not', '.', '</s>']\n",
      "['</s>', 'forces', 'fucking', '</s>', '</s>']\n",
      "['</s>', 'stationed', '<unk>', '</s>', '</s>']\n",
      "['</s>', 'in', '</s>', '</s>', '</s>']\n",
      "['</s>', '<unk>', '</s>', '</s>', '</s>']\n",
      "['</s>', 'stand', '</s>', '</s>', '</s>']\n",
      "['</s>', 'poised', '</s>', '</s>', '</s>']\n",
      "['</s>', ',', '</s>', '</s>', '</s>']\n",
      "['</s>', 'watching', '</s>', '</s>', '</s>']\n",
      "['</s>', 'us', '</s>', '</s>', '</s>']\n",
      "['</s>', 'for', '</s>', '</s>', '</s>']\n",
      "['</s>', 'any', '</s>', '</s>', '</s>']\n",
      "['</s>', 'sign', '</s>', '</s>', '</s>']\n",
      "['</s>', 'of', '</s>', '</s>', '</s>']\n",
      "['</s>', 'weakness', '</s>', '</s>', '</s>']\n",
      "['</s>', 'so', '</s>', '</s>', '</s>']\n",
      "['</s>', 'that', '</s>', '</s>', '</s>']\n",
      "['</s>', 'they', '</s>', '</s>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "class DataGenerator(object):\n",
    "\n",
    "    def __init__(self, batch_size, num_unroll, is_input, is_train):\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unroll = num_unroll\n",
    "        self._cursor = [0 for offset in range(self._batch_size)]\n",
    "        self._word_embeddings = np.load('embeddings.npy')\n",
    "        self._sent_ids = None\n",
    "        self._is_input = is_input\n",
    "        self._is_train = is_train\n",
    "\n",
    "    def next_batch(self, sent_ids):\n",
    "\n",
    "        sent_length = max_sent_length['question'] if self._is_input else max_sent_length['answer']\n",
    "\n",
    "        batch_data = np.zeros((self._batch_size, embedding_size), dtype=np.float32)\n",
    "        batch_labels = np.zeros((self._batch_size, vocabulary_size), dtype=np.float32)\n",
    "\n",
    "        for batch in range(self._batch_size):\n",
    "            sent_id = sent_ids[batch]\n",
    "            \n",
    "            if self._is_input:\n",
    "                sent_text = train_inputs[sent_id] if self._is_input else test_inputs[sent_id]\n",
    "            else:\n",
    "                sent_text = train_outputs[sent_id] if self._is_input else train_outputs[sent_id]\n",
    "            \n",
    "            batch_data[batch] = self._word_embeddings[sent_text[self._cursor[batch]],:]\n",
    "            batch_labels[batch] = np.zeros((vocabulary_size), dtype=np.float32)\n",
    "            batch_labels[batch, sent_text[self._cursor[batch] + 1]] = 1.0\n",
    "\n",
    "            self._cursor[batch] = (self._cursor[batch] + 1) % (sent_length - 1)\n",
    "\n",
    "        return batch_data,batch_labels\n",
    "\n",
    "    def unroll_batches(self,sent_ids):\n",
    "\n",
    "        if sent_ids is not None:\n",
    "            self._sent_ids = sent_ids\n",
    "            self._cursor = [0 for _ in range(self._batch_size)]\n",
    "        unroll_data, unroll_labels = [],[]\n",
    "\n",
    "        for unroll_ids in range(self._num_unroll):\n",
    "            data, labels = self.next_batch(self._sent_ids)\n",
    "            unroll_data.append(data)\n",
    "            unroll_labels.append(labels)\n",
    "        return unroll_data, unroll_labels, self._sent_ids\n",
    "\n",
    "    def reset_indices(self):\n",
    "        self._cursor = [0 for offset in range(self._batch_size)]\n",
    "\n",
    "dg = DataGenerator(batch_size=5, num_unroll=20, is_input=True, is_train=True)\n",
    "u_data, u_labels, _ = dg.unroll_batches([0,1,2,3,4])\n",
    "\n",
    "print('Input data')\n",
    "for _, lbl in zip(u_data,u_labels):\n",
    "    print([reverse_dictionary[w] for w in np.argmax(lbl,axis=1).tolist()])\n",
    "\n",
    "dg = DataGenerator(batch_size=5, num_unroll=30, is_input=False, is_train=True)\n",
    "u_data, u_labels, _ = dg.unroll_batches([0,1,2,3,4])\n",
    "\n",
    "print('\\nOutput data batch')\n",
    "for d_i,(_, lbl) in enumerate(zip(u_data,u_labels)):\n",
    "    print([reverse_dictionary[w] for w in np.argmax(lbl,axis=1).tolist()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d461925",
   "metadata": {},
   "source": [
    "## Building the Model with TensorFlow\n",
    "\n",
    "Define the hyperparameters, the input/output placeholders, the LSTM/Output layer parameters, the LSTM/output calculations, and finally the optimization steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cca169",
   "metadata": {},
   "source": [
    "### Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b60e521e",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_mat = np.load('embeddings.npy')\n",
    "input_size = emb_mat.shape[1]\n",
    "\n",
    "num_nodes = 128\n",
    "batch_size = 10\n",
    "\n",
    "encoder_num_unrollings = 20\n",
    "decoder_num_unrollings = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdf386c",
   "metadata": {},
   "source": [
    "### Input / Output Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15751651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining Encoder Data Placeholders\n",
      "Defining Decoder Data Placeholders\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "word_embeddings = tf.convert_to_tensor(value=emb_mat,name='embeddings')\n",
    "\n",
    "print('Defining Encoder Data Placeholders')\n",
    "encoder_train_inputs = []\n",
    "\n",
    "for ui in range(encoder_num_unrollings):\n",
    "    encoder_train_inputs.append(tf.compat.v1.placeholder(tf.float32, shape=[batch_size,input_size],name='train_inputs_%d'%ui))\n",
    "\n",
    "print('Defining Decoder Data Placeholders')\n",
    "\n",
    "decoder_train_inputs, decoder_train_labels, decoder_train_masks = [],[],[]\n",
    "\n",
    "for ui in range(decoder_num_unrollings):\n",
    "    decoder_train_inputs.append(tf.compat.v1.placeholder(tf.float32, shape=[batch_size,input_size],name='decoder_train_inputs_%d'%ui))\n",
    "    decoder_train_labels.append(tf.compat.v1.placeholder(tf.float32, shape=[batch_size,vocabulary_size], name = 'decoder_train_labels_%d'%ui))\n",
    "    decoder_train_masks.append(tf.compat.v1.placeholder(tf.float32, shape=[batch_size,1],name='decoder_train_masks_%d'%ui))\n",
    "\n",
    "\n",
    "encoder_test_input = [tf.compat.v1.placeholder(tf.float32, shape=[batch_size,input_size], name='test_input_%d'%ui) for ui in range(encoder_num_unrollings)]\n",
    "decoder_test_input = tf.nn.embedding_lookup(params=word_embeddings,ids=[dictionary['<s>']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52470c6",
   "metadata": {},
   "source": [
    "### Defining the Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a10af22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Model defined\n"
     ]
    }
   ],
   "source": [
    "with tf.compat.v1.variable_scope('Encoder'):\n",
    "\n",
    "    # Input gate\n",
    "    encoder_input_gate_x = tf.compat.v1.get_variable('input_gate_x', shape=[input_size, num_nodes], initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "    encoder_input_gate_m = tf.compat.v1.get_variable('input_gate_m', shape=[num_nodes, num_nodes], initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "    encoder_input_gate_b = tf.Variable(tf.random.uniform([1, num_nodes],-0.05, 0.05), name='input_gate_b')\n",
    "\n",
    "    # Forget gate\n",
    "    encoder_forget_gate_x = tf.compat.v1.get_variable('forget_gate_x', shape=[input_size, num_nodes], initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "    encoder_forget_gate_m = tf.compat.v1.get_variable('forget_gate_m', shape=[num_nodes, num_nodes], initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "    encoder_forget_gate_b = tf.Variable(tf.random.uniform([1, num_nodes],-0.05, 0.05), name='forget_gate_b')\n",
    "\n",
    "    # Candidate value (c~_t)\n",
    "    encoder_candidate_value_x = tf.compat.v1.get_variable('candidate_value_x', shape=[input_size, num_nodes], initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "    encoder_candidate_value_m = tf.compat.v1.get_variable('candidate_value_m', shape=[num_nodes, num_nodes], initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "    encoder_candidate_value_b = tf.Variable(tf.random.uniform([1, num_nodes],-0.05,0.05), name='candidate_value_b')\n",
    "\n",
    "    # Output gate\n",
    "    encoder_output_gate_x = tf.compat.v1.get_variable('output_gate_x', shape=[input_size, num_nodes], initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "    encoder_output_gate_m = tf.compat.v1.get_variable('output_gate_m', shape=[num_nodes, num_nodes], initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "    encoder_output_gate_b = tf.Variable(tf.random.uniform([1, num_nodes],-0.05,0.05), name='output_gate_b')\n",
    "\n",
    "    # Variáveis para salvar o resultado\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False, name='train_output')\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False, name = 'train_cell')\n",
    "\n",
    "    saved_test_output = tf.Variable(tf.zeros([batch_size, num_nodes]),trainable=False, name='test_output')\n",
    "    saved_test_state = tf.Variable(tf.zeros([batch_size, num_nodes]),trainable=False, name='test_cell')\n",
    "\n",
    "print('Encoder Model defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09954b00",
   "metadata": {},
   "source": [
    "### Defining the Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed93b5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder Model defined\n"
     ]
    }
   ],
   "source": [
    "with tf.compat.v1.variable_scope('Decoder'):\n",
    "\n",
    "    # Input gate\n",
    "    decoder_input_gate_x = tf.compat.v1.get_variable('input_gate_x',shape=[input_size, num_nodes], initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "    decoder_input_gate_m = tf.compat.v1.get_variable('input_gate_m',shape=[num_nodes, num_nodes], initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "    decoder_input_gate_b = tf.Variable(tf.random.uniform([1, num_nodes],-0.05, 0.05), name='input_gate_b')\n",
    "\n",
    "    # Forget gate\n",
    "    decoder_forget_gate_x = tf.compat.v1.get_variable('forget_gate_x', shape=[input_size, num_nodes], initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "    decoder_forget_gate_m = tf.compat.v1.get_variable('forget_gate_m', shape=[num_nodes, num_nodes], initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "    decoder_forget_gate_b = tf.Variable(tf.random.uniform([1, num_nodes],-0.05, 0.05), name='forget_gate_b')\n",
    "\n",
    "    # Candidate value (c~_t)\n",
    "    decoder_candidate_value_x = tf.compat.v1.get_variable('candidate_value_x', shape=[input_size, num_nodes], initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "    decoder_candidate_value_m = tf.compat.v1.get_variable('candidate_value_m', shape=[num_nodes, num_nodes], initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "    decoder_candidate_value_b = tf.Variable(tf.random.uniform([1, num_nodes],-0.05,0.05), name='candidate_value_b')\n",
    "\n",
    "    # Output gate\n",
    "    decoder_output_gate_x = tf.compat.v1.get_variable('output_gate_x',shape=[input_size, num_nodes], initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "    decoder_output_gate_m = tf.compat.v1.get_variable('output_gate_m',shape=[num_nodes, num_nodes], initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "    decoder_output_gate_b = tf.Variable(tf.random.uniform([1, num_nodes],-0.05,0.05),name='output_gate_b')\n",
    "\n",
    "    # Softmax Classifier\n",
    "    w = tf.compat.v1.get_variable('softmax_weights',shape=[num_nodes, vocabulary_size], initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\"))\n",
    "    b = tf.Variable(tf.random.uniform([vocabulary_size],-0.05,-0.05),name='softmax_bias')\n",
    "    \n",
    "print('Decoder Model defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272d1241",
   "metadata": {},
   "source": [
    "### Defining LSTM cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81ab79a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder LSTM cell\n",
    "def encoder_lstm_cell(_input, _output, _state):\n",
    "    input_gate = tf.sigmoid(tf.matmul(_input, encoder_input_gate_x) + tf.matmul(_output, encoder_input_gate_m) + encoder_input_gate_b)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(_input, encoder_forget_gate_x) + tf.matmul(_output, encoder_forget_gate_m) + encoder_forget_gate_b)\n",
    "    update = tf.matmul(_input, encoder_candidate_value_x) + tf.matmul(_output, encoder_candidate_value_m) + encoder_candidate_value_b\n",
    "    _state = forget_gate * _state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(_input, encoder_output_gate_x) + tf.matmul(_output, encoder_output_gate_m) + encoder_output_gate_b)\n",
    "    return output_gate * tf.tanh(_state), _state\n",
    "\n",
    "# Decoder LSTM cell\n",
    "def decoder_lstm_cell(_input, _output, _state):\n",
    "    input_gate = tf.sigmoid(tf.matmul(_input, decoder_input_gate_x) + tf.matmul(_output, decoder_input_gate_m) + decoder_input_gate_b)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(_input, decoder_forget_gate_x) + tf.matmul(_output, decoder_forget_gate_m) + decoder_forget_gate_b)\n",
    "    update = tf.matmul(_input, decoder_candidate_value_x) + tf.matmul(_output, decoder_candidate_value_m) + decoder_candidate_value_b\n",
    "    _state = forget_gate * _state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(_input, decoder_output_gate_x) + tf.matmul(_output, decoder_output_gate_m) + decoder_output_gate_b)\n",
    "    return output_gate * tf.tanh(_state), _state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0792a93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#=========================== TRAIN =================================\n",
    "\n",
    "outputs = list()\n",
    "output = saved_output\n",
    "state = saved_state\n",
    "\n",
    "# Calculate the output and state of the encoder\n",
    "for _input in encoder_train_inputs:\n",
    "    output, state = encoder_lstm_cell(_input, output, state)\n",
    "\n",
    "# Calculate the output and state of the decoder\n",
    "with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "    for _input in decoder_train_inputs:\n",
    "        output, state = decoder_lstm_cell(_input, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "# Calculate the decoder logits for all unrolled steps\n",
    "logits = tf.matmul(tf.concat(axis=0, values=outputs), w) + b\n",
    "\n",
    "# Decoder predictions\n",
    "train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "\n",
    "#=========================== TEST =================================\n",
    "\n",
    "test_output  = saved_test_output\n",
    "test_state = saved_test_state\n",
    "test_predictions = []\n",
    "\n",
    "for _input in encoder_test_input:\n",
    "    test_output, test_state = encoder_lstm_cell(_input, test_output,test_state)\n",
    "\n",
    "# Calculate the decoder output\n",
    "with tf.control_dependencies([saved_test_output.assign(test_output), saved_test_state.assign(test_state)]):\n",
    "    for i in range(decoder_num_unrollings):\n",
    "\n",
    "        test_output, test_state = decoder_lstm_cell(decoder_test_input, test_output, test_state)\n",
    "        test_prediction = tf.nn.softmax(tf.compat.v1.nn.xw_plus_b(test_output, w, b))\n",
    "        decoder_test_input = tf.nn.embedding_lookup(params=word_embeddings,ids=tf.argmax(input=test_prediction,axis=1))\n",
    "        test_predictions.append(tf.argmax(input=test_prediction,axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da1f597",
   "metadata": {},
   "source": [
    "### Calculating the Loss\n",
    "\n",
    "The loss is calculated by summing all losses obtained along the time axis and the average of the lot axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b56d6329",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_batch = tf.concat(axis=0,values=decoder_train_masks) * tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf.concat(axis=0, values=decoder_train_labels))\n",
    "loss = tf.reduce_mean(input_tensor=loss_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00034e96",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "There are two optimizers used here: Adam and SGD. \n",
    "Using Adam just causes the model to exhibit some undesirable behavior in the long run. \n",
    "So Adam is used to get a good initial guess for the SGD and use the SGD from that point on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d16822bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are used to slow down the learning rate over time\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "inc_gstep = tf.compat.v1.assign(global_step,global_step + 1)\n",
    "\n",
    "# Using two optimizers, when optimizer changes we reset global step\n",
    "reset_gstep = tf.compat.v1.assign(global_step,0)\n",
    "\n",
    "# Calculated decaying learning rate\n",
    "learning_rate = tf.maximum(\n",
    "    tf.compat.v1.train.exponential_decay(0.005, global_step, decay_steps=1, decay_rate=0.95, staircase=True), 0.00001)\n",
    "\n",
    "sgd_learning_rate = tf.maximum(\n",
    "    tf.compat.v1.train.exponential_decay(0.005, global_step, decay_steps=1, decay_rate=0.95, staircase=True), 0.00001)\n",
    "\n",
    "with tf.compat.v1.variable_scope('Adam'):\n",
    "    optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate)\n",
    "with tf.compat.v1.variable_scope('SGD'):\n",
    "    sgd_optimizer = tf.compat.v1.train.GradientDescentOptimizer(sgd_learning_rate)\n",
    "\n",
    "gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "optimize = optimizer.apply_gradients(zip(gradients, v))\n",
    "\n",
    "sgd_gradients, v = zip(*sgd_optimizer.compute_gradients(loss))\n",
    "sgd_gradients, _ = tf.clip_by_global_norm(sgd_gradients, 5.0)\n",
    "sgd_optimize = optimizer.apply_gradients(zip(sgd_gradients, v))\n",
    "\n",
    "# Making sure there are fluid gradients from decoder to encoder\n",
    "for (g_i,v_i) in zip(gradients,v):\n",
    "    assert g_i is not None, 'Gradient none for %s'%(v_i.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acb775f",
   "metadata": {},
   "source": [
    "### Resetting the Training and Testing States\n",
    "\n",
    "Define the state reset functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dda6d04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset training state\n",
    "reset_train_state = tf.group(tf.compat.v1.assign(saved_output, tf.zeros([batch_size, num_nodes])),\n",
    "                             tf.compat.v1.assign(saved_state, tf.zeros([batch_size, num_nodes])))\n",
    "\n",
    "reset_test_state = tf.group(\n",
    "    saved_test_output.assign(tf.zeros([batch_size, num_nodes])),\n",
    "    saved_test_state.assign(tf.zeros([batch_size, num_nodes])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480476d4",
   "metadata": {},
   "source": [
    "## Running the Neural Network\n",
    "\n",
    "With all the TensorFlow operations defined, now to define various functions related to running the model, as well as running the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd0e86e",
   "metadata": {},
   "source": [
    "### Evaluate and Print Results\n",
    "\n",
    "it is defined two functions to print and save the prediction results for training data as well as test data, and finally, define a function to get candidate and reference data to calculate the BLEU Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "919c629c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_and_save_train_predictions(decoder_unrolled_labels, pred_unrolled, random_index, train_prediction_text_fname):\n",
    "\n",
    "    print_str = 'Actual: ' \n",
    "    for w in np.argmax(np.concatenate(decoder_unrolled_labels,axis=0)[random_index::batch_size],axis=1).tolist():\n",
    "        print_str += reverse_dictionary[w] + ' '\n",
    "        if reverse_dictionary[w] == '</s>':\n",
    "            break\n",
    "    print(print_str)\n",
    "    with open(os.path.join(log_dir, train_prediction_text_fname),'a',encoding='utf-8') as fa:                \n",
    "        fa.write(print_str+'\\n')  \n",
    "\n",
    "    print()\n",
    "    print_str = 'Predicted: '\n",
    "    for w in np.argmax(pred_unrolled[random_index::batch_size],axis=1).tolist():\n",
    "        print_str += reverse_dictionary[w] + ' '\n",
    "        if reverse_dictionary[w] == '</s>':\n",
    "            break\n",
    "    print(print_str)\n",
    "    with open(os.path.join(log_dir, train_prediction_text_fname),'a',encoding='utf-8') as fa:                \n",
    "        fa.write(print_str+'\\n')\n",
    "\n",
    "\n",
    "def create_bleu_ref_candidate_lists(all_preds, all_labels):\n",
    "\n",
    "    bleu_labels, bleu_preds = [],[]\n",
    "    ref_list, cand_list = [],[]\n",
    "    for b_i in range(batch_size):\n",
    "        tmp_lbl = all_labels[b_i::batch_size]\n",
    "        tmp_lbl = tmp_lbl[np.where(tmp_lbl != dictionary['</s>'])]\n",
    "        ref_str = ' '.join([reverse_dictionary[lbl] for lbl in tmp_lbl])\n",
    "        ref_list.append([ref_str])\n",
    "        tmp_pred = all_preds[b_i::batch_size]\n",
    "        tmp_pred = tmp_pred[np.where(tmp_pred != dictionary['</s>'])]\n",
    "        cand_str = ' '.join([reverse_dictionary[pre] for pre in tmp_pred])\n",
    "        cand_list.append(cand_str)\n",
    "\n",
    "    return cand_list, ref_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b960071b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_step(unrolled_encoder_data, unrolled_decoder_data, unrolled_decoder_labels):\n",
    "\n",
    "    feed_dict = {}\n",
    "    for ui, dat in enumerate(unrolled_encoder_data):\n",
    "        feed_dict[encoder_train_inputs[ui]] = dat\n",
    "\n",
    "    for ui,(dat,lbl) in enumerate(zip(unrolled_decoder_data,unrolled_decoder_labels)):\n",
    "        feed_dict[decoder_train_inputs[ui]] = dat\n",
    "        feed_dict[decoder_train_labels[ui]] = lbl\n",
    "        d_msk = (np.logical_not(np.argmax(lbl,axis=1)==dictionary['</s>'])).astype(np.int32).reshape(-1,1)\n",
    "        feed_dict[decoder_train_masks[ui]] = d_msk\n",
    "\n",
    "    # ======================= OTIMIZAÇÃO ==========================\n",
    "    if (step+1) < 20000:\n",
    "        _,l,tr_pred = sess.run([optimize,loss,train_prediction], feed_dict=feed_dict)\n",
    "    else:\n",
    "        _,l,tr_pred = sess.run([sgd_optimize,loss,train_prediction], feed_dict=feed_dict)\n",
    "\n",
    "    return l, tr_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bb502b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-27 17:37:34.352902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-27 17:37:34.353369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-27 17:37:34.353649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-27 17:37:34.353853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-27 17:37:34.354129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-27 17:37:34.354251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3129 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py:1769: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'logs'\n",
    "if not os.path.exists(log_dir):\n",
    "    os.mkdir(log_dir)\n",
    "\n",
    "train_prediction_text_fname = 'train_predictions.txt'\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.allow_soft_placement=True\n",
    "sess = tf.compat.v1.InteractiveSession(config=config)\n",
    "\n",
    "tf.compat.v1.global_variables_initializer().run()\n",
    "word_embeddings = np.load('embeddings.npy')\n",
    "\n",
    "def define_data_generators(batch_size, encoder_num_unrollings, decoder_num_unrollings):\n",
    "\n",
    "    encoder_data_generator = DataGenerator(batch_size=batch_size,num_unroll=encoder_num_unrollings,is_input=True, is_train=True)\n",
    "    decoder_data_generator = DataGenerator(batch_size=batch_size,num_unroll=decoder_num_unrollings,is_input=False, is_train=True)\n",
    "\n",
    "    test_encoder_data_generator = DataGenerator(batch_size=batch_size,num_unroll=encoder_num_unrollings,is_input=True, is_train=False)\n",
    "    test_decoder_data_generator = DataGenerator(batch_size=batch_size,num_unroll=decoder_num_unrollings,is_input=False, is_train=False)\n",
    "\n",
    "    return encoder_data_generator,decoder_data_generator,test_encoder_data_generator,test_decoder_data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c428430",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bleu_scores_over_time,test_bleu_scores_over_time = [],[]\n",
    "loss_over_time = []\n",
    "\n",
    "train_bleu_refs, train_bleu_cands = [],[]\n",
    "test_bleu_refs, test_bleu_cands = [],[]\n",
    "\n",
    "num_steps = 100001\n",
    "avg_loss = 0\n",
    "\n",
    "encoder_data_generator, decoder_data_generator, \\\n",
    "test_encoder_data_generator, test_decoder_data_generator = \\\n",
    "define_data_generators(batch_size, encoder_num_unrollings, decoder_num_unrollings)\n",
    "\n",
    "print('Starting training')\n",
    "\n",
    "for step in range(num_steps):\n",
    "\n",
    "    if (step+1)%100==0:\n",
    "        print('.',end='')\n",
    "\n",
    "    sent_ids = np.random.randint(low=0,high=train_inputs.shape[0],size=(batch_size))\n",
    "\n",
    "    # Getting an unrolled set of data batches for the encoder\n",
    "    unrolled_encoder_data, _, _ = encoder_data_generator.unroll_batches(sent_ids=sent_ids)\n",
    "    \n",
    "    # Getting an unrolled set of data batches for the decoder\n",
    "    unrolled_decoder_data, unrolled_decoder_labels, _ = decoder_data_generator.unroll_batches(sent_ids=sent_ids)\n",
    "\n",
    "    # Train for single step\n",
    "    l, tr_pred = train_single_step(unrolled_encoder_data, unrolled_decoder_data, unrolled_decoder_labels)\n",
    "\n",
    "    # Calculate BLEU scores\n",
    "    if np.random.random() < 0.1:\n",
    "\n",
    "        all_labels = np.argmax(np.concatenate(unrolled_decoder_labels,axis=0),axis=1)\n",
    "        all_preds = np.argmax(tr_pred,axis=1)\n",
    "\n",
    "        batch_cands, batch_refs = create_bleu_ref_candidate_lists(all_preds, all_labels)\n",
    "\n",
    "        train_bleu_refs.extend(batch_refs)\n",
    "        train_bleu_cands.extend(batch_cands)\n",
    "\n",
    "    if (step+1)%500==0:\n",
    "\n",
    "        print('Step ',step+1)\n",
    "        with open(os.path.join(log_dir, train_prediction_text_fname),'a') as fa:\n",
    "            fa.write('============= Step ' +  str(step+1) + ' =============\\n')\n",
    "\n",
    "        random_index = np.random.randint(low=1,high=batch_size)\n",
    "        print_and_save_train_predictions(unrolled_decoder_labels, tr_pred, random_index, train_prediction_text_fname)\n",
    "\n",
    "        # Calculating the BLEU score for the accumulated candidates\n",
    "        bscore = 0.0\n",
    "        bscore = corpus_bleu(train_bleu_refs,train_bleu_cands,smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method4)\n",
    "        train_bleu_scores_over_time.append(bscore)\n",
    "        print('(Train) BLEU (%d elements): '%(len(train_bleu_refs)),bscore)\n",
    "\n",
    "        train_bleu_refs, train_bleu_cands = [],[]\n",
    "        with open(log_dir + os.sep +'blue_scores.txt','a') as fa_bleu:\n",
    "            fa_bleu.write(str(step+1) +','+str(bscore)+'\\n')\n",
    "\n",
    "        with open(os.path.join(log_dir, train_prediction_text_fname),'a') as fa:\n",
    "            fa.write('(Train) BLEU: %.5f\\n'%bscore)\n",
    "    \n",
    "    # Update average loss\n",
    "    avg_loss += l\n",
    "\n",
    "    # Resetting hidden state for each batch\n",
    "    sess.run(reset_train_state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b274446",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
