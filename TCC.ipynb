{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3bcd51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.10.6\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print('Python version:', python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86547416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Mar 20 14:29:45 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 520.61.05    Driver Version: 520.61.05    CUDA Version: 11.8     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0  On |                  N/A |\n",
      "| 30%   30C    P0    N/A /  75W |    773MiB /  4096MiB |      1%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1542      G   /usr/lib/xorg/Xorg                133MiB |\n",
      "|    0   N/A  N/A      1701      G   /usr/bin/gnome-shell               42MiB |\n",
      "|    0   N/A  N/A      2766      G   ...925571007835274077,131072       81MiB |\n",
      "|    0   N/A  N/A      4336      C   /usr/bin/python3                  510MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa4b4a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from matplotlib import pylab\n",
    "import matplotlib\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib inline\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from sklearn.utils import shuffle\n",
    "import word2vec\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a4ff641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.python.util.deprecation as deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a0dd06",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "[Dowload](https://nlp.stanford.edu/projects/nmt/):\n",
    "\n",
    "* English vocabulary: [`vocab.50K.en`](https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/vocab.50K.en)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb7635e",
   "metadata": {},
   "source": [
    "### Loading the Datasets and Building the Vocabulary\n",
    "\n",
    "First, we build the vocabulary dictionaries for the source and target (English) language. \n",
    "The vocabularies are found in the file `vocab.50K.en`(English)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18b2e809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary: [('<unk>', 0), ('<s>', 1), ('</s>', 2), ('the', 3), (',', 4), ('.', 5), ('of', 6), ('and', 7), ('to', 8), ('in', 9)]\n",
      "Reverse dictionary: [(0, '<unk>'), (1, '<s>'), (2, '</s>'), (3, 'the'), (4, ','), (5, '.'), (6, 'of'), (7, 'and'), (8, 'to'), (9, 'in')]\n",
      "Vocabulary size:  50000\n"
     ]
    }
   ],
   "source": [
    "# Word string -> ID mapping\n",
    "dictionary = dict()\n",
    "\n",
    "vocabulary_size = len(dictionary)\n",
    "with open('data/vocab.50K.en', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        # disregard the new line aka `\\n`\n",
    "        dictionary[line[:-1]] = len(dictionary)\n",
    "        \n",
    "vocabulary_size = len(dictionary)\n",
    "reverse_dictionary = dict(zip(dictionary.values(),dictionary.keys()))\n",
    "\n",
    "print('Dictionary:', list(dictionary.items())[:10], end = '\\n')\n",
    "print('Reverse dictionary:', list(reverse_dictionary.items())[:10], end = '\\n')\n",
    "print('Vocabulary size: ', vocabulary_size, end = '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1623674",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "Here we load the data from the dataset.csv file (generated in the other script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58d59dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('data/dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ecca3e",
   "metadata": {},
   "source": [
    "### Data pre-processing\n",
    "Transform to lower, remove the new line and the punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42052bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wt = nltk.tokenize.WhitespaceTokenizer()\n",
    "\n",
    "for column in dataset.columns:\n",
    "    dataset[column] = dataset[column].str.lower() \n",
    "    dataset[column] = dataset[column].str.replace(',', ' ,')  \\\n",
    "                                     .str.replace('.',' .', regex=False)   \\\n",
    "                                     .str.replace('?',' ?', regex=False)   \\\n",
    "                                     .str.replace(')','', regex=False)   \\\n",
    "                                     .str.replace('(','', regex=False)   \\\n",
    "                                     .str.replace('\"','')   \\\n",
    "                                     .str.replace('\\n',' ')\n",
    "    dataset[column] = dataset[column].apply(wt.tokenize)\n",
    "dataset = shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6db800f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>454381</th>\n",
       "      <td>[you, can, send, a, 1gb, flash-drive, back, to...</td>\n",
       "      <td>[pictures, of, what, i, look, like, now, ., ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055609</th>\n",
       "      <td>[what's, been, your, biggest, flop, reddit, po...</td>\n",
       "      <td>[i, was, really, proud, of, my, last, tattoo, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50746</th>\n",
       "      <td>[there, are, 10, 000, unarmed, pissed, off, ch...</td>\n",
       "      <td>[they, can't, do, harm, to, me, physically, or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698841</th>\n",
       "      <td>[what, do, you, want, to, see, humanity, inven...</td>\n",
       "      <td>[long, distance, teleportation, as, in, ,, it,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825893</th>\n",
       "      <td>[if, rick, from, pawn, stars, called, you, for...</td>\n",
       "      <td>[guns, or, mental, illnesses]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  question  \\\n",
       "454381   [you, can, send, a, 1gb, flash-drive, back, to...   \n",
       "1055609  [what's, been, your, biggest, flop, reddit, po...   \n",
       "50746    [there, are, 10, 000, unarmed, pissed, off, ch...   \n",
       "698841   [what, do, you, want, to, see, humanity, inven...   \n",
       "825893   [if, rick, from, pawn, stars, called, you, for...   \n",
       "\n",
       "                                                    answer  \n",
       "454381   [pictures, of, what, i, look, like, now, ., ma...  \n",
       "1055609  [i, was, really, proud, of, my, last, tattoo, ...  \n",
       "50746    [they, can't, do, harm, to, me, physically, or...  \n",
       "698841   [long, distance, teleportation, as, in, ,, it,...  \n",
       "825893                       [guns, or, mental, illnesses]  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada21a8a",
   "metadata": {},
   "source": [
    "### Data analysis\n",
    "Mean sentence length and standard deviation of sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f68588d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Questions) Average sentence length:  17.101486059545056\n",
      "(Questions) Standard deviation of sentence length:  9.122891352194081\n",
      "(Answers) Average sentence length:  54.367627238247216\n",
      "(Answers) Standard deviation of sentence length:  843.0636308326157\n"
     ]
    }
   ],
   "source": [
    "print('(Questions) Average sentence length: ', dataset['question'].str.len().mean())\n",
    "print('(Questions) Standard deviation of sentence length: ', dataset['question'].str.len().std())\n",
    "\n",
    "print('(Answers) Average sentence length: ', dataset['answer'].str.len().mean())\n",
    "print('(Answers) Standard deviation of sentence length: ', dataset['answer'].str.len().std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbe4985",
   "metadata": {},
   "source": [
    "### Update the sentences to fixed length\n",
    "Update all sentences with a fixed size, to process the sentences as batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44f77928",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sent_length = {'question' : 30, 'answer': 70}\n",
    "\n",
    "def padding_sent(source):\n",
    "    padded = []\n",
    "    for tokens in dataset[source]: \n",
    "        # adding the start token\n",
    "        tokens.insert(0, '<s>')  \n",
    "\n",
    "        if len(tokens) >= max_sent_length[source]:\n",
    "            tokens = tokens[:(max_sent_length[source] - 1)]\n",
    "            tokens.append('</s>')\n",
    "\n",
    "        if len(tokens) < max_sent_length[source]:\n",
    "            tokens.extend(['</s>' for _ in range(max_sent_length[source] - len(tokens))])  \n",
    "\n",
    "        padded.append(tokens)\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cc61866",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = padding_sent('question')\n",
    "answers = padding_sent('answer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0924d3d3",
   "metadata": {},
   "source": [
    "### Create the reverse dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7ad51f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reverse_dataset(source):\n",
    "    reverse_tokens = []\n",
    "    reverse_dataset = []\n",
    "    for tokens in source: \n",
    "        for token in tokens: \n",
    "            if token not in dictionary.keys():\n",
    "                reverse_tokens.append(dictionary['<unk>'])\n",
    "            else:\n",
    "                reverse_tokens.append(dictionary[token])\n",
    "        reverse_dataset.append(reverse_tokens)\n",
    "        reverse_tokens = []\n",
    "    return reverse_dataset\n",
    "\n",
    "train_inputs =  np.array(create_reverse_dataset(questions), dtype=np.int32)\n",
    "train_outputs =  np.array(create_reverse_dataset(answers), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467bf5ec",
   "metadata": {},
   "source": [
    "### Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00e9857b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "with window_size = 2:\n",
      "    batch: [['<s>', 'former', 'of', '<unk>'], ['<s>', '<unk>', 'favourite', 'light'], ['<s>', 'postal', 'of', '<unk>'], ['<s>', 'as', 'kid', ','], ['<s>', 'you', 'throw', '10000'], ['<s>', 'how', 'you', 'feel'], ['<s>', 'where', 'you', 'put'], ['<s>', 'when', 'the', 'absolute']]\n",
      "    labels: ['smokers', 'your', 'workers', 'a', 'can', 'would', 'do', 'was']\n",
      "Defining 4 embedding lookups representing each word in the context\n",
      "Stacked embedding size: [32, 64, 4]\n",
      "Reduced mean embedding size: [32, 64]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-20 14:31:47.258978: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-20 14:31:47.306573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 14:31:47.369285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 14:31:47.369851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 14:31:47.802758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 14:31:47.802977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 14:31:47.803119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-20 14:31:47.803228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2606 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "2023-03-20 14:31:47.809391: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 2000: 2.779788\n",
      "Average loss at step 4000: 1.444825\n",
      "Average loss at step 6000: 1.281724\n",
      "Average loss at step 8000: 1.185413\n",
      "Average loss at step 10000: 1.129574\n",
      "Nearest to should: did, constituent, would, husbands, Derrida, does, well, greetings,\n",
      "Nearest to -: ,, women, instead, fellow, cooled, cops, housed, Bible,\n",
      "Nearest to also: parents, dopo, deceiving, CAM, Elche, S.p.A., walked, leases,\n",
      "Nearest to The: Alejandro, presses, pioneered, Absolute, Berlaymont, devoid, Excellent, multiple,\n",
      "Nearest to these: 1821, sparking, satisfying, MacDonald, www.avaaz.org, Jo, collateral, Rhapsody,\n",
      "Nearest to us: <unk>, worst, for, indemnify, longest, apple, alright, ridiculous,\n",
      "Nearest to out: sleep, indelible, prayer, fell, live, all, vs, meu,\n",
      "Nearest to or: Poos, biocidal, parody, Multitude, object, decides, Immanuel, Link,\n",
      "Nearest to by: Advocates, GfK, Sealed, ERC, 3.7, jour, 6.30, tabling,\n",
      "Nearest to as: video, movie, person, Procchio, girl, scale, song, weird,\n",
      "Nearest to latest: Scouting, Vigilio, worst, comeback, architect, insult, undulating, Büro,\n",
      "Nearest to brought: how, bullied, lavatory, reverse, legislatures, Regiment, Angaben, Characteristic,\n",
      "Nearest to size: Terry, Retoucher, action, bronchitis, mythical, contemplated, Jakob, EFSF,\n",
      "Nearest to sound: untenable, Localization, Millions, scales, acknowledgment, sank, safest, painful,\n",
      "Nearest to Central: cabbage, dado, Synergy, Assault, attenuation, ideale, Actions, referral,\n",
      "Nearest to supported: metropolitan, nomads, jpeg, Hosting, Browsing, immune, said, OTHER,\n",
      "Nearest to independent: Probe, sectoral, Agora, ICEcat, Wegener, dodge, 24h, Inquiry,\n",
      "Nearest to land: SOUND, refinements, exchanger, supplanted, eutrophication, 307, endorse, went,\n",
      "Nearest to search: mainframe, chute, Ironforge, Lakeside, anger, sunburn, eagerness, relegated,\n",
      "Nearest to rates: Whether, aeronautics, magazin, Watts, dominate, Chess, skates, Suliban,\n",
      "Average loss at step 12000: 1.093283\n",
      "Average loss at step 14000: 1.067507\n",
      "Average loss at step 16000: 1.049263\n",
      "Average loss at step 18000: 1.047472\n",
      "Average loss at step 20000: 1.040354\n",
      "Nearest to should: would, did, can, realised, mysql, Espace, constituent, warming,\n",
      "Nearest to -: fellow, mall, ap, un, Bible, chefs, ,, walks,\n",
      "Nearest to also: walked, granted, parents, dopo, deceiving, hit, given, Of,\n",
      "Nearest to The: Alejandro, presses, pioneered, Absolute, Berlaymont, devoid, Excellent, emits,\n",
      "Nearest to these: 1821, collateral, satisfying, Rhapsody, MacDonald, Holstein, Jamaica, doorway,\n",
      "Nearest to us: <unk>, indemnify, longest, actual, appropriate, Coriantumr, overnight, winding,\n",
      "Nearest to out: fell, sleep, live, indelible, all, lived, prayer, meu,\n",
      "Nearest to or: Poos, decides, Multitude, object, adjournment, biocidal, inapplicable, parody,\n",
      "Nearest to by: Advocates, GfK, Sealed, sides, daran, moments, analyzed, 6.30,\n",
      "Nearest to as: Procchio, finished, CHILD, huge, Orion, girl, CDA, picking,\n",
      "Nearest to latest: worst, insult, Scouting, Vigilio, comeback, biggest, town, 2050,\n",
      "Nearest to brought: bullied, 17, tonight, want, how, lavatory, Characteristic, legislatures,\n",
      "Nearest to size: Retoucher, Terry, alerting, action, bronchitis, mythical, Na, rife,\n",
      "Nearest to sound: untenable, Localization, Worms, painful, safest, upmarket, harsh, grab,\n",
      "Nearest to Central: dado, Synergy, cabbage, Assault, attenuation, ideale, Actions, referral,\n",
      "Nearest to supported: metropolitan, nomads, jpeg, immune, Browsing, Mato, Hosting, Laotian,\n",
      "Nearest to independent: Probe, sectoral, Agora, 24h, biker, ICEcat, dodge, Stream,\n",
      "Nearest to land: SOUND, refinements, bored, endorse, eutrophication, custodians, went, very,\n",
      "Nearest to search: mainframe, Lakeside, Ironforge, linkages, eagerness, chute, lingua, instructors,\n",
      "Nearest to rates: aeronautics, Whether, magazin, renminbi, Chess, Watts, dominate, Suliban,\n",
      "Average loss at step 22000: 1.029904\n",
      "Average loss at step 24000: 1.034981\n",
      "Average loss at step 26000: 1.030983\n",
      "Average loss at step 28000: 1.020353\n",
      "Average loss at step 30000: 1.044761\n",
      "Nearest to should: would, did, can, will, mysql, could, warming, realised,\n",
      "Nearest to -: fellow, ,, Maritim, mall, ap, chemists, bosses, :,\n",
      "Nearest to also: walked, moved, granted, tasked, thoughts, fiber, dopo, deceiving,\n",
      "Nearest to The: presses, Alejandro, pioneered, Absolute, Berlaymont, devoid, Excellent, emits,\n",
      "Nearest to these: 1821, doorway, Holstein, routinely, Along, Rhapsody, F1, slowly,\n",
      "Nearest to us: indemnify, Coriantumr, <unk>, handicraft, emotional, equitably, tasting, usa,\n",
      "Nearest to out: fell, indelible, traveled, lived, 8, all, 6th, Benghazi,\n",
      "Nearest to or: Poos, Multitude, DVI, Payments, adjournment, gestellt, inapplicable, Nueva,\n",
      "Nearest to by: Advocates, GfK, on, realistic, Sealed, daran, boards, summoned,\n",
      "Nearest to as: CHILD, Orion, Procchio, certain, finished, Lopar, manner, child,\n",
      "Nearest to latest: worst, 2050, Scouting, darkest, Vigilio, onion, equivalent, comeback,\n",
      "Nearest to brought: 17, honestly, bullied, tonight, come, legislatures, want, tried,\n",
      "Nearest to size: Retoucher, alerting, Terry, college, Teatro, mythical, action, bronchitis,\n",
      "Nearest to sound: untenable, Localization, Worms, opposite, upmarket, heinous, event, painful,\n",
      "Nearest to Central: dado, Synergy, Assault, attenuation, ideale, Actions, referral, boarded,\n",
      "Nearest to supported: metropolitan, nomads, Mato, jpeg, Laotian, immune, Hosting, longtime,\n",
      "Nearest to independent: Probe, sectoral, Agora, Stream, periodically, 24h, biker, dodge,\n",
      "Nearest to land: bored, SOUND, refinements, went, endorse, homosexual, custodians, walking,\n",
      "Nearest to search: Lakeside, lingua, mainframe, Ironforge, relegated, awake, linkages, gravel,\n",
      "Nearest to rates: aeronautics, Whether, Suliban, renminbi, magazin, dominate, testimonials, Chess,\n",
      "Average loss at step 32000: 1.023770\n",
      "Average loss at step 34000: 1.041398\n",
      "Average loss at step 36000: 1.042854\n",
      "Average loss at step 38000: 1.027680\n",
      "Average loss at step 40000: 1.037344\n",
      "Nearest to should: would, can, did, will, could, mysql, detailled, Espace,\n",
      "Nearest to -: ,, ap, :, coaches, ethics, Maritim, 80th, bosses,\n",
      "Nearest to also: walked, involved, moved, arising, fiber, sirens, abducted, thoughts,\n",
      "Nearest to The: presses, Alejandro, pioneered, Absolute, Berlaymont, devoid, Excellent, colonized,\n",
      "Nearest to these: Doctor, F1, Along, routinely, contravention, 1821, 0044, www.avaaz.org,\n",
      "Nearest to us: indemnify, usa, emotional, handicraft, Coriantumr, spills, equitably, Friesland,\n",
      "Nearest to out: fell, indelible, 8, dipping, Benghazi, realised, Documentation, Bregenzerwald,\n",
      "Nearest to or: Poos, Multitude, Payments, Nueva, indemnity, gestellt, impatience, inapplicable,\n",
      "Nearest to by: Advocates, GfK, on, realistic, boards, dismayed, robbed, predefined,\n",
      "Nearest to as: Orion, CHILD, Teheran, Brähler, rebound, bind, Eugen, Lopar,\n",
      "Nearest to latest: onion, 2050, town, smallest, darkest, removing, Scouting, temperature,\n",
      "Nearest to brought: tried, honestly, 17, come, bullied, Sad, contrast, Characteristic,\n",
      "Nearest to size: alerting, Retoucher, mythical, Terry, Teatro, bronchitis, grinding, action,\n",
      "Nearest to sound: untenable, Localization, Worms, upmarket, heinous, opposite, event, Lisboa,\n",
      "Nearest to Central: dado, Synergy, Assault, attenuation, ideale, Actions, antioxidants, referral,\n",
      "Nearest to supported: metropolitan, nomads, Mato, Laotian, jpeg, immune, soundtracks, Browsing,\n",
      "Nearest to independent: Probe, 24h, Agora, periodically, 1923, sectoral, Stream, biker,\n",
      "Nearest to land: SOUND, bored, utmost, refinements, doorway, homosexual, went, custodians,\n",
      "Nearest to search: Lakeside, lingua, Ironforge, eagerness, relegated, shouting, linkages, gravel,\n",
      "Nearest to rates: aeronautics, Whether, renminbi, magazin, Or, Suliban, dominate, Chess,\n",
      "Average loss at step 42000: 1.040096\n",
      "Average loss at step 44000: 1.037316\n",
      "Average loss at step 46000: 1.070902\n",
      "Average loss at step 48000: 1.063058\n",
      "Average loss at step 50000: 1.059402\n",
      "Nearest to should: can, would, did, could, will, mysql, detailled, does,\n",
      "Nearest to -: ,, :, finances, ethics, Maritim, hillside, nightclub, coaches,\n",
      "Nearest to also: sirens, involved, walked, Pond, fiber, commentaries, Comparative, arising,\n",
      "Nearest to The: presses, Alejandro, pioneered, Absolute, colonized, devoid, Berlaymont, Excellent,\n",
      "Nearest to these: Doctor, Along, F1, 0044, Linguistic, Uniform, 50, contravention,\n",
      "Nearest to us: usa, emotional, insbesondere, handicraft, indemnify, spirit, <unk>, Coriantumr,\n",
      "Nearest to out: fell, Documentation, 8, indelible, soir, dipping, tempted, Benghazi,\n",
      "Nearest to or: Poos, Multitude, DVI, Payments, impatience, Valencia, Nueva, indemnity,\n",
      "Nearest to by: Advocates, GfK, on, dismayed, predefined, realistic, data, daran,\n",
      "Nearest to as: Orion, CHILD, rebound, Teheran, bind, Eugen, Brähler, Preliminary,\n",
      "Nearest to latest: onion, temperature, town, smallest, rarest, outcome, removing, meaning,\n",
      "Nearest to brought: tried, greatly, done, honestly, gotten, terrified, arguing, according,\n",
      "Nearest to size: Terry, alerting, Retoucher, bronchitis, Teatro, mythical, Plain, whet,\n",
      "Nearest to sound: untenable, upmarket, opposite, Localization, Worms, heat, flawed, heinous,\n",
      "Nearest to Central: dado, Synergy, Assault, attenuation, ideale, Actions, antioxidants, referral,\n",
      "Nearest to supported: metropolitan, nomads, Mato, soundtracks, Laotian, jpeg, Hosting, longtime,\n",
      "Nearest to independent: Probe, 1923, Agora, periodically, Stream, 24h, imprisoned, sectoral,\n",
      "Nearest to land: bored, SOUND, refinements, doorway, utmost, walking, homosexual, went,\n",
      "Nearest to search: Lakeside, relegated, eagerness, recent, shouting, monsoon, Ironforge, lingua,\n",
      "Nearest to rates: aeronautics, Whether, Or, magazin, Chess, partner, renminbi, dominate,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 52000: 1.066826\n",
      "Average loss at step 54000: 1.067815\n",
      "Average loss at step 56000: 1.074104\n",
      "Average loss at step 58000: 1.076034\n",
      "Average loss at step 60000: 1.071978\n",
      "Nearest to should: can, would, did, could, will, does, detailled, Vejer,\n",
      "Nearest to -: ,, :, finances, nightclub, coaches, Maritim, hillside, 80th,\n",
      "Nearest to also: sirens, oral, involved, arising, commentaries, clic, visited, dopo,\n",
      "Nearest to The: Alejandro, presses, pioneered, colonized, Absolute, devoid, Berlaymont, Excellent,\n",
      "Nearest to these: Doctor, F1, Along, 0044, contravention, 50, Ideally, Linguistic,\n",
      "Nearest to us: usa, insbesondere, handicraft, emotional, jealousy, tumours, Friesland, indemnify,\n",
      "Nearest to out: Documentation, 8, soir, Exif, Byzantine, indelible, fell, tempted,\n",
      "Nearest to or: Poos, Valencia, DVI, impatience, Multitude, Payments, Nueva, zero,\n",
      "Nearest to by: Advocates, on, GfK, dismayed, predefined, data, realistic, daran,\n",
      "Nearest to as: Orion, Eugen, rebound, Teheran, CHILD, bind, Lopar, Preliminary,\n",
      "Nearest to latest: onion, rarest, town, simplest, removing, outcome, temperature, ultimate,\n",
      "Nearest to brought: tried, terrified, according, aspire, taught, fill, access, collect,\n",
      "Nearest to size: Terry, alerting, Teatro, bronchitis, grinding, Plain, nett, música,\n",
      "Nearest to sound: untenable, upmarket, flawed, heat, opposite, Worms, Localization, heinous,\n",
      "Nearest to Central: dado, Synergy, Assault, antioxidants, attenuation, ideale, Actions, referral,\n",
      "Nearest to supported: metropolitan, nomads, Mato, soundtracks, Laotian, jpeg, immune, longtime,\n",
      "Nearest to independent: Probe, 1923, Agora, periodically, Stream, imprisoned, sectoral, 24h,\n",
      "Nearest to land: bored, SOUND, doorway, homosexual, Gaudi, went, utmost, refinements,\n",
      "Nearest to search: Lakeside, person, eagerness, monsoon, thanks, recent, opportune, relegated,\n",
      "Nearest to rates: aeronautics, Or, partner, Whether, magazin, Chess, riders, renminbi,\n",
      "Average loss at step 62000: 1.067349\n",
      "Average loss at step 64000: 1.083989\n",
      "Average loss at step 66000: 1.085574\n",
      "Average loss at step 68000: 1.089473\n",
      "Average loss at step 70000: 1.085248\n",
      "Nearest to should: can, would, did, could, might, will, does, detailled,\n",
      "Nearest to -: ,, :, nightclub, finances, 80th, enthralled, coaches, NYC,\n",
      "Nearest to also: sirens, oral, Pond, commentaries, involved, arising, deepens, reviews,\n",
      "Nearest to The: colonized, Alejandro, pioneered, presses, Absolute, devoid, Berlaymont, Excellent,\n",
      "Nearest to these: F1, Doctor, Along, 50, contravention, Ideally, water-, loop,\n",
      "Nearest to us: usa, insbesondere, tumours, handicraft, jealousy, alterations, ze, bassist,\n",
      "Nearest to out: 8, Documentation, soir, Exif, ANSI, Byzantine, into, Kouchner,\n",
      "Nearest to or: Valencia, impatience, DVI, Multitude, Payments, zero, Poos, Belle,\n",
      "Nearest to by: Advocates, GfK, on, dismayed, predefined, data, Bowling, identically,\n",
      "Nearest to as: Orion, Eugen, rebound, Teheran, Preliminary, Lopar, bind, CHILD,\n",
      "Nearest to latest: onion, rarest, removing, outcome, earliest, shortest, simplest, newest,\n",
      "Nearest to brought: tried, aspire, collect, according, arguing, taught, done, terrified,\n",
      "Nearest to size: Teatro, grinding, música, alerting, Plain, nett, moroccan, audience,\n",
      "Nearest to sound: untenable, upmarket, flawed, opposite, Worms, regulars, Localization, convenient,\n",
      "Nearest to Central: dado, Synergy, Assault, antioxidants, attenuation, ideale, Actions, referral,\n",
      "Nearest to supported: metropolitan, Mato, attempted, soundtracks, hacked, proposed, nomads, bathrooms,\n",
      "Nearest to independent: Probe, hey, imprisoned, 1923, Agora, periodically, apparently, ΝΑΤΟ,\n",
      "Nearest to land: bored, doorway, SOUND, see, Gaudi, went, 7,000, Toward,\n",
      "Nearest to search: opportune, monsoon, thanks, eagerness, memorable, relegated, Lakeside, anger,\n",
      "Nearest to rates: aeronautics, Whether, Or, magazin, partner, Chess, riders, runaway,\n",
      "Average loss at step 72000: 1.090024\n",
      "Average loss at step 74000: 1.094442\n",
      "Average loss at step 76000: 1.099556\n",
      "Average loss at step 78000: 1.096344\n",
      "Average loss at step 80000: 1.113771\n",
      "Nearest to should: can, would, might, did, could, cant, Vejer, will,\n",
      "Nearest to -: ,, :, nightclub, finances, enthralled, NYC, Algeria, hillside,\n",
      "Nearest to also: sirens, oral, worry, arising, deepens, involved, craft, reviews,\n",
      "Nearest to The: colonized, Alejandro, presses, pioneered, Absolute, devoid, earn, Berlaymont,\n",
      "Nearest to these: F1, Along, Ideally, Doctor, 50, loop, contravention, water-,\n",
      "Nearest to us: usa, insbesondere, jealousy, bassist, tumours, ravaged, handicraft, united,\n",
      "Nearest to out: soir, Documentation, Exif, Apostle, Byzantine, 8, ANSI, into,\n",
      "Nearest to or: Valencia, DVI, Belle, impatience, leniency, Party, toch, Payments,\n",
      "Nearest to by: Advocates, GfK, on, predefined, dismayed, narrow, data, crossroad,\n",
      "Nearest to as: Eugen, Preliminary, Lopar, Teheran, Orion, quantitatively, rebound, Peaks,\n",
      "Nearest to latest: onion, rarest, temperature, earliest, outcome, removing, shortest, simplest,\n",
      "Nearest to brought: tried, taught, access, according, aspire, arguing, collect, extent,\n",
      "Nearest to size: audience, grinding, dangers, música, Teatro, euphemism, nett, downfall,\n",
      "Nearest to sound: untenable, flawed, upmarket, copied, normalise, regulars, Localization, Worms,\n",
      "Nearest to Central: dado, Synergy, antioxidants, Assault, attenuation, ideale, Actions, referral,\n",
      "Nearest to supported: Mato, proposed, attempted, nomads, Hosting, metropolitan, longtime, immune,\n",
      "Nearest to independent: Probe, 1923, Agora, Nespresso, ΝΑΤΟ, periodically, mattresses, hey,\n",
      "Nearest to land: bored, doorway, SOUND, 7,000, see, Toward, rebel, five,\n",
      "Nearest to search: opportune, anger, monsoon, memorable, buddies, relegated, thanks, shouting,\n",
      "Nearest to rates: aeronautics, Whether, magazin, Or, renminbi, runaway, Chess, Penken,\n",
      "Average loss at step 82000: 1.113116\n",
      "Average loss at step 84000: 1.120602\n",
      "Average loss at step 86000: 1.113074\n",
      "Average loss at step 88000: 1.122525\n",
      "Average loss at step 90000: 1.124206\n",
      "Nearest to should: can, might, would, could, did, does, Vejer, will,\n",
      "Nearest to -: ,, :, finances, nightclub, enthralled, NYC, satirical, Maritim,\n",
      "Nearest to also: sirens, oral, deepens, offended, arising, craft, Estimates, involved,\n",
      "Nearest to The: colonized, Alejandro, presses, Absolute, pioneered, earn, devoid, Berlaymont,\n",
      "Nearest to these: F1, Doctor, Along, 50, Ideally, contravention, 195, loop,\n",
      "Nearest to us: usa, insbesondere, united, ravaged, bassist, tumours, jealousy, ze,\n",
      "Nearest to out: soir, Documentation, ANSI, Byzantine, Exif, into, Kouchner, pickpockets,\n",
      "Nearest to or: Valencia, DVI, Belle, leniency, Party, impatience, toch, Payments,\n",
      "Nearest to by: Advocates, GfK, predefined, narrow, on, data, crossroad, dismayed,\n",
      "Nearest to as: Eugen, Preliminary, Teheran, quantitatively, archaeologist, Liebe, Orion, Peaks,\n",
      "Nearest to latest: rarest, onion, whole, earliest, temperature, newest, outcome, shortest,\n",
      "Nearest to brought: tried, taught, access, aspire, collect, according, purchased, motivated,\n",
      "Nearest to size: audience, dangers, spirit, downfall, location, Teatro, euphemism, música,\n",
      "Nearest to sound: untenable, upmarket, flawed, normalise, regulars, heinous, elephant, opposite,\n",
      "Nearest to Central: dado, antioxidants, Synergy, Assault, attenuation, ideale, Actions, referral,\n",
      "Nearest to supported: Mato, proposed, attempted, 80th, metropolitan, endowed, Hosting, longtime,\n",
      "Nearest to independent: Probe, 1923, 24h, Agora, imprisoned, hey, periodically, Nespresso,\n",
      "Nearest to land: bored, doorway, 7,000, rebel, SOUND, split, heat, Toward,\n",
      "Nearest to search: opportune, anger, monsoon, memorable, shouting, relegated, buddies, Lithuanian,\n",
      "Nearest to rates: Whether, aeronautics, magazin, Penken, renminbi, Or, runaway, Chess,\n",
      "Average loss at step 92000: 1.131230\n",
      "Average loss at step 94000: 1.129633\n",
      "Average loss at step 96000: 1.124089\n",
      "Average loss at step 98000: 1.144291\n",
      "Average loss at step 100000: 1.134075\n",
      "Nearest to should: can, might, would, could, does, did, will, detailled,\n",
      "Nearest to -: ,, :, nightclub, enthralled, --, finances, satirical, NYC,\n",
      "Nearest to also: sirens, deepens, oral, Clooney, Brett, Tamils, stitchers, Pond,\n",
      "Nearest to The: colonized, Alejandro, presses, Absolute, devoid, earn, Berlaymont, pioneered,\n",
      "Nearest to these: F1, 50, Ideally, Along, 195, contravention, Doctor, water-,\n",
      "Nearest to us: usa, insbesondere, united, bassist, ravaged, Plan, tumours, ze,\n",
      "Nearest to out: soir, Documentation, into, ANSI, Kouchner, Exif, Byzantine, Ones,\n",
      "Nearest to or: Valencia, DVI, Belle, leniency, Party, impatience, toch, restlessness,\n",
      "Nearest to by: Advocates, GfK, narrow, crossroad, predefined, on, tortured, dismayed,\n",
      "Nearest to as: Eugen, Preliminary, Teheran, archaeologist, quantitatively, Liebe, rebound, Orion,\n",
      "Nearest to latest: newest, earliest, rarest, temperature, ultimate, clown, longest, whole,\n",
      "Nearest to brought: tried, taught, collect, motivated, ruined, access, according, Fourteen,\n",
      "Nearest to size: audience, dangers, spirit, opposite, música, downfall, pinnacle, location,\n",
      "Nearest to sound: normalise, untenable, upmarket, heinous, flawed, elephant, Fathers, brew,\n",
      "Nearest to Central: dado, antioxidants, Synergy, Assault, attenuation, ideale, Actions, referral,\n",
      "Nearest to supported: Mato, attempted, proposed, 80th, Hosting, endowed, nomads, hacked,\n",
      "Nearest to independent: imprisoned, Probe, 1923, hey, 22, Agora, k, apparently,\n",
      "Nearest to land: bored, 7,000, doorway, split, heat, rebel, benefits, end,\n",
      "Nearest to search: opportune, anger, shouting, relegated, buddies, monsoon, resolution, memorable,\n",
      "Nearest to rates: aeronautics, Whether, renminbi, magazin, Penken, riders, Or, Suliban,\n"
     ]
    }
   ],
   "source": [
    "import word2vec\n",
    "\n",
    "sentence_cursors = [0 for _ in range(train_inputs.shape[0])]\n",
    "\n",
    "batch_size = 32\n",
    "embedding_size = 64\n",
    "steps = 80000\n",
    "\n",
    "word2vec.define_data_and_hyperparameters(\n",
    "        train_inputs.shape[0], \n",
    "        max_sent_length['question'], \n",
    "        max_sent_length['answer'], \n",
    "        dictionary, \n",
    "        reverse_dictionary,  \n",
    "        train_inputs, \n",
    "        train_outputs, \n",
    "        embedding_size,\n",
    "        vocabulary_size)\n",
    "\n",
    "word2vec.print_some_batches()\n",
    "word2vec.define_word2vec_tensorflow(batch_size)\n",
    "word2vec.run_word2vec(batch_size, steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5bc51f44",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataGenerator' object has no attribute 'is_input'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [20], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cursor \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m offset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_size)]\n\u001b[1;32m     51\u001b[0m dg \u001b[38;5;241m=\u001b[39m DataGenerator(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, num_unroll\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, is_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, is_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 52\u001b[0m u_data, u_labels, _ \u001b[38;5;241m=\u001b[39m \u001b[43mdg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munroll_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSource data\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, lbl \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(u_data,u_labels):\n",
      "Cell \u001b[0;32mIn [20], line 43\u001b[0m, in \u001b[0;36mDataGenerator.unroll_batches\u001b[0;34m(self, sent_ids)\u001b[0m\n\u001b[1;32m     40\u001b[0m unroll_data, unroll_labels \u001b[38;5;241m=\u001b[39m [],[]\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m unroll_ids \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_unroll):\n\u001b[0;32m---> 43\u001b[0m     data, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sent_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     unroll_data\u001b[38;5;241m.\u001b[39mappend(data)\n\u001b[1;32m     45\u001b[0m     unroll_labels\u001b[38;5;241m.\u001b[39mappend(labels)\n",
      "Cell \u001b[0;32mIn [20], line 14\u001b[0m, in \u001b[0;36mDataGenerator.next_batch\u001b[0;34m(self, sent_ids)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnext_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, sent_ids):\n\u001b[0;32m---> 14\u001b[0m     sent_length \u001b[38;5;241m=\u001b[39m max_sent_length[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_input\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m max_sent_length[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     16\u001b[0m     batch_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_size, embedding_size),dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     17\u001b[0m     batch_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_size,embedding_size),dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataGenerator' object has no attribute 'is_input'"
     ]
    }
   ],
   "source": [
    "class DataGenerator(object):\n",
    "\n",
    "    def __init__(self, batch_size, num_unroll, is_input, is_train):\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unroll = num_unroll\n",
    "        self._cursor = [0 for offset in range(self._batch_size)]\n",
    "        self._word_embeddings = np.load('embeddings.npy')\n",
    "        self._sent_ids = None\n",
    "        self._is_input = is_input\n",
    "        self._is_train = is_train\n",
    "\n",
    "    def next_batch(self, sent_ids):\n",
    "\n",
    "        sent_length = max_sent_length['question'] if self._is_input else max_sent_length['answer']\n",
    "\n",
    "        batch_data = np.zeros((self._batch_size, embedding_size),dtype=np.float32)\n",
    "        batch_labels = np.zeros((self._batch_size,embedding_size),dtype=np.float32)\n",
    "\n",
    "        for batch in range(self._batch_size):\n",
    "            sent_id = sent_ids[batch]\n",
    "            \n",
    "            if self._is_input:\n",
    "                sent_text = train_inputs[sent_id] if self._is_input else test_inputs[sent_id]\n",
    "            else:\n",
    "                sent_text = train_outputs[sent_id] if self._is_input else train_outputs[sent_id]\n",
    "            \n",
    "            batch_data[batch] = self._word_embeddings[sent_text[self._cursor[batch]],:]\n",
    "            batch_labels[batch] = np.zeros((vocabulary_size),dtype=np.float32)\n",
    "            batch_labels[batch,sent_text[self._cursor[batch]+1]] = 1.0\n",
    "\n",
    "            self._cursor[b] = (self._cursor[b]+1)%(max_sent_length-1)\n",
    "\n",
    "        return batch_data,batch_labels\n",
    "\n",
    "    def unroll_batches(self,sent_ids):\n",
    "\n",
    "        if sent_ids is not None:\n",
    "            self._sent_ids = sent_ids\n",
    "            self._cursor = [0 for _ in range(self._batch_size)]\n",
    "        unroll_data, unroll_labels = [],[]\n",
    "\n",
    "        for unroll_ids in range(self._num_unroll):\n",
    "            data, labels = self.next_batch(self._sent_ids)\n",
    "            unroll_data.append(data)\n",
    "            unroll_labels.append(labels)\n",
    "        return unroll_data, unroll_labels, self._sent_ids\n",
    "\n",
    "    def reset_indices(self):\n",
    "        self._cursor = [0 for offset in range(self._batch_size)]\n",
    "\n",
    "dg = DataGenerator(batch_size=5, num_unroll=20, is_input=True, is_train=True)\n",
    "u_data, u_labels, _ = dg.unroll_batches([0,1,2,3,4])\n",
    "\n",
    "print('Source data')\n",
    "for _, lbl in zip(u_data,u_labels):\n",
    "    print([reverse_dictionary[w] for w in np.argmax(lbl,axis=1).tolist()])\n",
    "\n",
    "dg = DataGeneratorMT(batch_size=5, num_unroll=30, is_input=False, is_train=True)\n",
    "u_data, u_labels, _ = dg.unroll_batches([0,1,2,3,4])\n",
    "print('\\nTarget data batch')\n",
    "for d_i,(_, lbl) in enumerate(zip(u_data,u_labels)):\n",
    "    print([tgt_reverse_dictionary[w] for w in np.argmax(lbl,axis=1).tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aeca77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
